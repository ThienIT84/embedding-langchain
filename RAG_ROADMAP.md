# ğŸš€ ROADMAP: Tá»« Embedding Pipeline Sang RAG System

## ğŸ“Œ TÃ¬nh HÃ¬nh Hiá»‡n Táº¡i

âœ… **Báº¡n Ä‘Ã£ cÃ³:**
1. Embedding pipeline (6 modules)
2. PDF Ä‘Æ°á»£c chia thÃ nh chunks
3. Embedding vectors lÆ°u á»Ÿ Supabase pgvector

âŒ **CÃ²n thiáº¿u Ä‘á»ƒ cÃ³ RAG:**
1. Similarity search (tÃ¬m chunks tÆ°Æ¡ng tá»±)
2. Prompt template (xÃ¢y dá»±ng question tá»« chunks)
3. LLM call (gá»i model AI Ä‘á»ƒ tráº£ lá»i)
4. Backend API endpoint (káº¿t ná»‘i frontend)

---

## ğŸ¯ 3 BÆ°á»›c Äá»ƒ CÃ³ RAG

### **BÆ¯á»šC 1: Similarity Search (Retriever)**

**Má»¥c Ä‘Ã­ch:** Khi user há»i cÃ¢u há»i â†’ tÃ¬m chunks liÃªn quan tá»« DB

**CÃ¡ch thá»±c hiá»‡n:**

1. Embed cÃ¢u há»i thÃ nh vector (768 chiá»u)
2. Query Supabase: tÃ¬m embeddings gáº§n nháº¥t (cosine similarity)
3. Return top-k chunks liÃªn quan

**Code:**

```python
# src/retriever.py (file má»›i)

import numpy as np
from .embedder import _get_model
from .supabase_client import get_supabase_client

def retrieve_similar_chunks(query: str, document_id: str, top_k: int = 5) -> list[dict]:
    """
    Embed cÃ¢u há»i â†’ tÃ¬m chunks tÆ°Æ¡ng tá»± tá»« Supabase
    """
    # BÆ°á»›c 1: Embed query
    model = _get_model()
    query_embedding = model.encode(query)  # [0.1, 0.2, ..., 0.3] (768 chiá»u)
    
    # BÆ°á»›c 2: Query Supabase (similarity search)
    client = get_supabase_client()
    
    # DÃ¹ng pgvector similarity operator (<->)
    # Similar documents cÃ³ distance nhá» hÆ¡n = more similar
    response = client.rpc(
        'search_embeddings',  # Function stored procedure
        {
            'query_embedding': query_embedding.tolist(),  # Convert to list
            'document_id': document_id,
            'similarity_threshold': 0.5,
            'limit': top_k
        }
    ).execute()
    
    return response.data  # List of chunks: [chunk1, chunk2, ...]
```

**SQL Function (táº¡o á»Ÿ Supabase):**

```sql
CREATE OR REPLACE FUNCTION search_embeddings(
    query_embedding vector(768),
    document_id text,
    similarity_threshold float DEFAULT 0.5,
    limit int DEFAULT 5
)
RETURNS TABLE (
    chunk_index int,
    content text,
    page_number int,
    similarity float
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        de.chunk_index,
        de.content,
        de.page_number,
        1 - (de.embedding <=> query_embedding) as similarity
    FROM document_embeddings de
    WHERE de.document_id = search_embeddings.document_id
    AND 1 - (de.embedding <=> query_embedding) > similarity_threshold
    ORDER BY de.embedding <=> query_embedding
    LIMIT limit;
END;
$$ LANGUAGE plpgsql;
```

**Giáº£i thÃ­ch:**
- `<=>` operator: pgvector distance (L2 norm)
- `1 - distance` = similarity (0-1)
- `ORDER BY distance`: sáº¯p xáº¿p gáº§n nháº¥t trÆ°á»›c

---

### **BÆ¯á»šC 2: Prompt Engineering (Context Builder)**

**Má»¥c Ä‘Ã­ch:** GhÃ©p chunks vÃ o prompt Ä‘á»ƒ LLM hiá»ƒu context

**Code:**

```python
# src/prompt_builder.py (file má»›i)

def build_rag_prompt(query: str, context_chunks: list[dict], system_prompt: str = None) -> str:
    """
    XÃ¢y dá»±ng prompt Ä‘áº§y Ä‘á»§ cho LLM
    """
    
    if system_prompt is None:
        system_prompt = """Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh.
Sá»­ dá»¥ng thÃ´ng tin cung cáº¥p dÆ°á»›i Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§, hÃ£y nÃ³i rÃµ.
"""
    
    # GhÃ©p context chunks
    context_text = "\n\n".join([
        f"[Trang {chunk['page_number']}, Chunk {chunk['chunk_index']}]\n{chunk['content']}"
        for chunk in context_chunks
    ])
    
    # XÃ¢y dá»±ng prompt
    prompt = f"""{system_prompt}

---CONTEXT---
{context_text}

---QUESTION---
{query}

---ANSWER---
"""
    
    return prompt

# VÃ­ dá»¥ sá»­ dá»¥ng
query = "LangChain lÃ  gÃ¬?"
context_chunks = [
    {"page_number": 1, "chunk_index": 1, "content": "LangChain lÃ  framework..."},
    {"page_number": 1, "chunk_index": 2, "content": "NÃ³ cung cáº¥p cÃ´ng cá»¥..."},
]
prompt = build_rag_prompt(query, context_chunks)
print(prompt)
```

---

### **BÆ¯á»šC 3: LLM Integration (Response Generator)**

**Má»¥c Ä‘Ã­ch:** Gá»i LLM (OpenAI, Ollama, etc.) Ä‘á»ƒ táº¡o response

**Lá»±a Chá»n LLM:**

#### **Option A: OpenAI API (Dá»…, Nhanh)**

```python
# src/llm_client.py

from openai import OpenAI
from .config import settings

client = OpenAI(api_key=settings.openai_api_key)

def generate_answer(prompt: str, model: str = "gpt-3.5-turbo", temperature: float = 0.7) -> str:
    """
    Gá»i OpenAI API Ä‘á»ƒ generate answer
    """
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=temperature,
        max_tokens=500
    )
    
    return response.choices[0].message.content
```

**Setup:**
```bash
pip install openai
# .env thÃªm:
OPENAI_API_KEY=sk-...
```

---

#### **Option B: Ollama (Local, Free, Privacy)**

```python
# src/llm_client.py

import requests
from .config import settings

def generate_answer(prompt: str, model: str = "llama2", temperature: float = 0.7) -> str:
    """
    Gá»i Ollama local API
    """
    response = requests.post(
        f"{settings.ollama_url}/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False,
            "temperature": temperature
        }
    )
    
    result = response.json()
    return result['response']
```

**Setup (macOS/Linux):**
```bash
# CÃ i Ollama
brew install ollama

# Pull model (láº§n Ä‘áº§u máº¥t ~5-10 phÃºt)
ollama pull llama2

# Cháº¡y Ollama server
ollama serve
# Server cháº¡y á»Ÿ http://localhost:11434

# .env thÃªm:
OLLAMA_URL=http://localhost:11434
```

**So sÃ¡nh:**

| TiÃªu ChÃ­ | OpenAI | Ollama |
|----------|--------|--------|
| **Chi phÃ­** | $ (pay per request) | Free |
| **Speed** | 1-2s | 10-20s (tÃ¹y hardware) |
| **Quality** | Tá»‘t nháº¥t | Tá»‘t |
| **Privacy** | Data gá»­i tá»›i OpenAI | Local (private) |
| **Setup** | API key | Docker/Binary |

---

## ğŸ¬ Flow RAG HoÃ n Chá»‰nh

```
User Question
    â†“
[retriever.py]
  1. Embed query
  2. Search Supabase
  3. Get top-k chunks
    â†“
[prompt_builder.py]
  1. Format context
  2. Build prompt
    â†“
[llm_client.py]
  1. Call LLM API
  2. Get response
    â†“
Response to User
```

---

## ğŸ“‹ Todo List: XÃ¢y Dá»±ng RAG

### **Phase 1: Core RAG (1-2 ngÃ y)**

- [ ] **Step 1.1:** Táº¡o `src/retriever.py`
  - HÃ m `retrieve_similar_chunks(query, doc_id, top_k)`
  - Embed query báº±ng SentenceTransformer
  - Query Supabase pgvector similarity

- [ ] **Step 1.2:** Táº¡o SQL function á»Ÿ Supabase
  - `search_embeddings()` stored procedure
  - Test vá»›i query tá»« Python

- [ ] **Step 1.3:** Táº¡o `src/prompt_builder.py`
  - HÃ m `build_rag_prompt(query, chunks)`
  - Format context chunks Ä‘Ãºng

- [ ] **Step 1.4:** Táº¡o `src/llm_client.py`
  - HÃ m `generate_answer(prompt)`
  - Chá»n OpenAI hoáº·c Ollama

- [ ] **Step 1.5:** Test end-to-end
  - Python script: query â†’ retrieve â†’ generate â†’ print

### **Phase 2: Backend API (1-2 ngÃ y)**

- [ ] **Step 2.1:** Táº¡o endpoint `/api/rag/chat`
  - Method: POST
  - Body: `{ query: "...", document_id: "..." }`
  - Response: `{ answer: "...", sources: [...] }`

- [ ] **Step 2.2:** Add tracing/logging
  - Log query â†’ retrieved chunks â†’ generated answer

- [ ] **Step 2.3:** Add error handling
  - No chunks found
  - LLM timeout
  - Invalid document_id

### **Phase 3: Frontend Integration (1 ngÃ y)**

- [ ] **Step 3.1:** Táº¡o UI component `<ChatInterface />`
  - Input field (query)
  - Display answer
  - Show sources (retrieved chunks)

- [ ] **Step 3.2:** Wire up API call
  - Fetch POST `/api/rag/chat`
  - Handle loading/error states

---

## ğŸ”§ Recommended Tech Stack

**For RAG System:**

```
Backend:
â”œâ”€ Python (Flask/FastAPI) - API server
â”œâ”€ SentenceTransformer - Query embedding
â”œâ”€ Supabase pgvector - Vector DB
â”œâ”€ OpenAI / Ollama - LLM
â””â”€ LangChain (optional) - Orchestration

Frontend:
â”œâ”€ React - UI
â”œâ”€ TailwindCSS - Styling
â”œâ”€ React Query - API calls
â””â”€ Markdown renderer - Display formatted answers
```

---

## ğŸ“ File Structure (Sau khi thÃªm RAG)

```
Embedding_langchain/
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ config.py              âœ… CÃ³
â”‚  â”œâ”€ text_extractor.py      âœ… CÃ³
â”‚  â”œâ”€ chunker.py             âœ… CÃ³
â”‚  â”œâ”€ embedder.py            âœ… CÃ³
â”‚  â”œâ”€ supabase_client.py     âœ… CÃ³
â”‚  â”œâ”€ pipeline.py            âœ… CÃ³
â”‚  â”œâ”€ retriever.py           ğŸ†• Táº¡o (retrieve chunks)
â”‚  â”œâ”€ prompt_builder.py      ğŸ†• Táº¡o (build prompt)
â”‚  â”œâ”€ llm_client.py          ğŸ†• Táº¡o (call LLM)
â”‚  â””â”€ rag_pipeline.py        ğŸ†• Táº¡o (end-to-end)
â”œâ”€ scripts/
â”‚  â”œâ”€ ingest_document.py     âœ… CÃ³
â”‚  â””â”€ test_rag.py            ğŸ†• Táº¡o (test RAG)
â””â”€ requirements.txt          ğŸ“ Update
```

---

## ğŸš€ Quick Start (Náº¿u Chá»n Ollama)

```bash
# 1. CÃ i Ollama
brew install ollama  # macOS
# hoáº·c download tá»« ollama.ai

# 2. Pull model
ollama pull llama2

# 3. Cháº¡y Ollama server (background)
ollama serve

# 4. Cáº­p nháº­t requirements.txt
pip install requests  # Cho Ollama API call

# 5. Táº¡o retriever.py
# (giáº£i thÃ­ch á»Ÿ BÆ¯á»šC 1 á»Ÿ trÃªn)

# 6. Test
python -c "
from src.retriever import retrieve_similar_chunks
chunks = retrieve_similar_chunks('LangChain lÃ  gÃ¬?', 'doc123')
print(chunks)
"
```

---

## ğŸ’¡ Workflow Recommend

### **Náº¿u Muá»‘n Nhanh:**
1. âœ… DÃ¹ng Ollama (local, free, privacy)
2. âœ… Implement retriever.py (similarity search)
3. âœ… Implement prompt_builder.py (format context)
4. âœ… Implement llm_client.py (Ollama API call)
5. âœ… Test á»Ÿ Python trÆ°á»›c (khÃ´ng cáº§n frontend)

### **Náº¿u Muá»‘n Production-Ready:**
1. âœ… DÃ¹ng OpenAI (stable, high quality)
2. âœ… Implement retriever.py
3. âœ… Implement prompt_builder.py
4. âœ… Implement llm_client.py
5. âœ… Táº¡o Express API endpoint
6. âœ… Add caching (Redis)
7. âœ… Add monitoring (logs, metrics)
8. âœ… Wire up frontend

---

## â“ CÃ¢u Há»i ThÆ°á»ng Gáº·p

### **Q: Khi nÃ o nÃªn dÃ¹ng OpenAI vs Ollama?**

**DÃ¹ng OpenAI náº¿u:**
- Muá»‘n quality cao nháº¥t
- Dá»± Ã¡n commercial
- CÃ³ budget
- KhÃ´ng quan tÃ¢m data privacy

**DÃ¹ng Ollama náº¿u:**
- Muá»‘n free + private
- Development/learning
- CÃ³ GPU máº¡nh (RTX 4090, etc.)
- Cháº¥p nháº­n quality tháº¥p hÆ¡n

### **Q: Bao lÃ¢u má»›i cÃ³ RAG?**

- **Minimal RAG** (Python only): 2-3 giá»
- **Full RAG with API**: 1 ngÃ y
- **Production-ready**: 2-3 ngÃ y (+ testing, monitoring)

### **Q: Cáº§n GPU khÃ´ng?**

**Cho retrieval:** KhÃ´ng (CPU Ä‘á»§)
**Cho local LLM (Ollama):** CÃ³ GPU tá»‘t hÆ¡n nhiá»u
  - Ollama cÃ³ há»— trá»£ GPU (CUDA, Metal)
  - CPU mode cháº¡y Ä‘Æ°á»£c nhÆ°ng cháº­m

### **Q: Embeddings 768 chiá»u lÃ  bao nhiÃªu "thÃ´ng tin"?**

- 768-dim vector â‰ˆ "semantic fingerprint" cá»§a text
- DÃ¹ng Ä‘á»ƒ Ä‘o Ä‘á»™ tÆ°Æ¡ng tá»± (cosine similarity)
- KhÃ´ng pháº£i "compression" - thÃ´ng tin khÃ´ng máº¥t

---

## ğŸ¯ Next Steps (Cá»¥ Thá»ƒ)

**Báº¡n muá»‘n:**

1. **[A] Minimal RAG demo** (cháº¡y local Python)
   â†’ Cáº§n: retriever.py + prompt_builder.py + Ollama
   â†’ Time: 2-3 giá»
   
2. **[B] Backend API** (Express endpoint)
   â†’ Cáº§n: thÃªm llm_client.py + Express POST endpoint
   â†’ Time: 1 ngÃ y
   
3. **[C] Full UI** (chatbot interface)
   â†’ Cáº§n: React component + styling + error handling
   â†’ Time: 1-2 ngÃ y

**Báº¡n chá»n hÆ°á»›ng nÃ o?** TÃ´i sáº½ implement chi tiáº¿t! ğŸš€

---

## ğŸ“š Resources

**Docs:**
- SentenceTransformer: https://www.sbert.net/
- Supabase pgvector: https://supabase.com/docs/guides/database/extensions/pgvector
- Ollama: https://ollama.ai/
- OpenAI: https://platform.openai.com/docs/

**Examples:**
- LangChain RAG: https://github.com/langchain-ai/langchain/tree/master/templates/rag
- pgvector examples: https://github.com/pgvector/pgvector/tree/master/examples

**Best Practices:**
- Chunk size: 800-1000 tokens (báº¡n Ä‘Ã£ tá»‘i Æ°u 900)
- Similarity threshold: 0.5-0.7 (tuá»³ use case)
- Top-k: 3-5 chunks (balance quality & latency)
- Temperature: 0.5-0.7 (creativity vs consistency)
