# ğŸ“š HÆ¯á»šNG DáºªN Äá»ŒC CODE - RAG PIPELINE

**Má»¥c Ä‘Ã­ch:** Hiá»ƒu rÃµ quy trÃ¬nh RAG tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i  
**Thá»i gian:** 30-45 phÃºt  
**Äá»™ khÃ³:** â­â­â­ (Trung bÃ¬nh)

---

## ğŸ¯ TÃ“M Táº®T QUY TRÃŒNH RAG

```
ğŸ“„ PDF Document
    â†“ [1. TEXT EXTRACTION]
ğŸ“ Raw Text
    â†“ [2. CHUNKING]
ğŸ“¦ Text Chunks (900 chars/chunk)
    â†“ [3. EMBEDDING]
ğŸ”¢ Vector Embeddings (768-dim)
    â†“ [4. STORAGE]
ğŸ’¾ Supabase pgvector
    â†“ [5. RETRIEVAL - User asks question]
â“ Query â†’ Vector â†’ Find Similar Chunks
    â†“ [6. PROMPT BUILDING]
ğŸ“‹ Context + Query â†’ Prompt
    â†“ [7. LLM GENERATION]
ğŸ¤– Ollama llama3 â†’ Answer
    â†“
âœ… Final Answer + Sources
```

---

## ğŸ“– CÃCH Äá»ŒC CODE - THEO THá»¨ Tá»°

### **GIAI ÄOáº N 1: HIá»‚U Cáº¤U HÃŒNH** (5 phÃºt)

#### ğŸ”¹ File 1: `src/config.py`
**Äá»c trÆ°á»›c tiÃªn!** - NÆ¡i cáº¥u hÃ¬nh táº¥t cáº£ settings

**CÃ¡c setting quan trá»ng:**
- `chunk_size`: 900 chars - Ä‘á»™ dÃ i má»—i chunk
- `chunk_overlap`: 200 chars - overlap giá»¯a cÃ¡c chunks
- `hf_model_name`: Model embedding (768-dim vectors)
- `ollama_model`: llama3 - LLM Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i
- `supabase_url/key`: Database connection

**Kiáº¿n thá»©c:**
- `@dataclass(frozen=True)` â†’ Immutable settings
- `load_dotenv()` â†’ Äá»c tá»« `.env` file

---

### **GIAI ÄOáº N 2: INGESTION PIPELINE** (Nhá»“i tÃ i liá»‡u vÃ o há»‡ thá»‘ng)

#### ğŸ”¹ File 2: `src/text_extractor.py`
**Chá»©c nÄƒng:** Äá»c PDF â†’ Raw text

**Äá»c theo thá»© tá»±:**
1. **Class `DocumentChunk`** (dÃ²ng 18-38)
   - `text`: Ná»™i dung vÄƒn báº£n
   - `page_number`: Sá»‘ trang
   - `source_file`: TÃªn file PDF
   - `__slots__` â†’ Tá»‘i Æ°u bá»™ nhá»›

2. **Function `clean_text()`** (dÃ²ng 41-63)
   - Loáº¡i bá» null bytes (`\x00`)
   - Ná»‘i tá»« bá»‹ ngáº¯t dÃ²ng: `"process-\ning"` â†’ `"processing"`
   - Chuáº©n hÃ³a whitespace

3. **Function `extract_pdf_text()`** (dÃ²ng 66-107)
   - DÃ¹ng `PyPDF` Ä‘á»ƒ Ä‘á»c PDF
   - Yield tá»«ng trang (streaming, tiáº¿t kiá»‡m RAM)
   - Error handling: File not found, encrypted PDF

**Kiáº¿n thá»©c:**
- `yield` vs `return` â†’ Generaor pattern
- `__slots__`t â†’ Memory optimization
- Regex `re.sub()` â†’ Text cleaning

---

#### ğŸ”¹ File 3: `src/chunker.py`
**Chá»©c nÄƒng:** Chia text dÃ i â†’ Chunks nhá» (900 chars)

**Äá»c theo thá»© tá»±:**
1. **Class `TextChunk`** (dÃ²ng 14-32)
   - Káº¿ thá»«a `DocumentChunk`
   - ThÃªm `chunk_index`: Sá»‘ thá»© tá»± chunk

2. **`_splitter`** (dÃ²ng 35-41)
   - `RecursiveCharacterTextSplitter` tá»« LangChain
   - `separators`: Æ¯u tiÃªn tÃ¡ch theo `\n\n`, `\n`, space
   - `keep_separator=False` â†’ Bá» kÃ½ tá»± phÃ¢n cÃ¡ch

3. **Function `split_chunks()`** (dÃ²ng 44-67)
   - Loop qua tá»«ng `DocumentChunk`
   - Split thÃ nh nhiá»u pieces
   - Yield tá»«ng `TextChunk` vá»›i `chunk_index` tÄƒng dáº§n

**Kiáº¿n thá»©c:**
- LangChain `RecursiveCharacterTextSplitter`
- Streaming pattern vá»›i `yield`
- `global_chunk_index` â†’ Counter tÄƒng dáº§n

**Táº¡i sao chunk?*
- LLM cÃ³ giá»›i háº¡n context window
- Embedding models hoáº¡t Ä‘á»™ng tá»‘t vá»›i text ngáº¯n
- TÃ¬m kiáº¿m semantic chÃ­nh xÃ¡c hÆ¡n vá»›i chunks nhá»

---

#### ğŸ”¹ File 4: `src/embedder.py`
**Chá»©c nÄƒng:** Text chunks â†’ Vector embeddings (768-dim)

**Äá»c theo thá»© tá»±:**
1. **Class `EmbeddingResult`** (dÃ²ng 14-19)
   - `chunk`: TextChunk gá»‘c
   - `vector`: numpy array (768 chiá»u)

2. **Function `_get_model()`** (dÃ²ng 25-30)
   - Singleton pattern: Chá»‰ load model 1 láº§n
   - `SentenceTransformer` tá»« HuggingFace
   - Model: `paraphrase-multilingual-mpnet-base-v2`

3. **Function `embed_chunks()`** (dÃ²ng 33-42)
   - Input: List of TextChunk
   - `model.encode()` â†’ Batch encoding
   - Output: List of EmbeddingResult (chunk + vector)

**Kiáº¿n thá»©c:**
- Sentence Transformers â†’ Dense vector representations
- 768 dimensions â†’ Semantic meaning
- Batch processing â†’ Efficient GPU usage
- Singleton pattern â†’ Memory optimization

**Táº¡i sao 768 chiá»u?**
- Model architecture (BERT-based)
- Balance giá»¯a accuracy vÃ  performance
- Enough Ä‘á»ƒ capture semantic meaning

---

#### ğŸ”¹ File 5: `src/supabase_client.py`
**Chá»©c nÄƒng:** Giao tiáº¿p vá»›i Supabase (Database + Storage)

**Äá»c theo thá»© tá»±:**
1. **Function `get_supabase_client()`** (dÃ²ng 18-23)
   - Singleton pattern
   - Create Supabase client 1 láº§n

2. **Function `download_file()`** (dÃ²ng 26-32)
   - Download PDF tá»« Supabase Storage
   - LÆ°u vÃ o local `tmp/` directory

3. **Function `fetch_document_metadata()`** (dÃ²ng 35-47)
   - Query báº£ng `documents`
   - Láº¥y metadata: title, file_path, category_id, etc.

4. **Function `insert_embeddings()`** (dÃ²ng 85-95)
   - Insert vectors vÃ o báº£ng `document_embeddings`
   - **NEW:** Batch insert (100 rows/batch) Ä‘á»ƒ trÃ¡nh timeout

**Kiáº¿n thá»©c:**
- Supabase Python SDK
- PostgreSQL with pgvector extension
- Retry logic vá»›i `@retry_with_backoff`
- Error handling vá»›i `APIError`

---

#### ğŸ”¹ File 6: `src/pipeline.py`
**Chá»©c nÄƒng:** ORCHESTRATOR - Káº¿t há»£p táº¥t cáº£ bÆ°á»›c ingestion

**Äá»c theo thá»© tá»±:**
1. **Function `_load_document()`** (dÃ²ng 18-21)
   - Extract PDF â†’ DocumentChunk
   - Split â†’ TextChunk

2. **Function `_prepare_records()`** (dÃ²ng 24-39)
   - TextChunk + Vector â†’ Database records
   - Format: `{document_id, content, page_number, chunk_index, embedding}`

3. **Function `process_document()`** (dÃ²ng 42-75)
   - **MAIN ORCHESTRATOR**
   - Flow:
     ```
     1. Fetch metadata tá»« DB
     2. Set status = "processing"
     3. Download PDF tá»« Storage
     4. Extract text â†’ chunks
     5. Embed chunks â†’ vectors
     6. Prepare records
     7. Delete old embeddings
     8. Insert new embeddings
     9. Set status = "completed"
     10. Cleanup: Delete temp file
     ```

**Kiáº¿n thá»©c:**
- Try-except-finally pattern
- Status tracking: processing â†’ completed/failed
- Resource cleanup trong `finally` block
- Error propagation

---

### **GIAI ÄOáº N 3: RETRIEVAL PIPELINE** (Truy váº¥n)

#### ğŸ”¹ File 7: `src/retriever.py`
**Chá»©c nÄƒng:** TÃ¬m chunks tÆ°Æ¡ng Ä‘á»“ng vá»›i cÃ¢u há»i

**Äá»c theo thá»© tá»±:**
1. **Class `RetrievedChunk`** (dÃ²ng 13-18)
   - `content`: Text cá»§a chunk
   - `similarity`: Äiá»ƒm tÆ°Æ¡ng Ä‘á»“ng (0-1)
   - `chunk_index`, `page_number`: Metadata

2. **Function `_cosine_similarity()`** (dÃ²ng 21-28)
   - TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 vectors
   - Formula: `dot(a,b) / (norm(a) * norm(b))`
   - Output: 0 (khÃ´ng giá»‘ng) â†’ 1 (giá»‘ng há»‡t)

3. **Function `retrieve_similar_chunks()`** (dÃ²ng 31-90)
   - **CÅ¨:** Search trong 1 document cá»¥ thá»ƒ
   - Query DB â†’ Get all chunks of document
   - Encode query â†’ vector
   - TÃ­nh cosine similarity vá»›i tá»«ng chunk
   - Sort theo similarity giáº£m dáº§n
   - Return top_k chunks

4. **Function `retrieve_similar_chunks_by_user()`** (dÃ²ng 93-161)
   - **Má»šI - PHASE C1:** Search trong Táº¤T Cáº¢ documents cá»§a user
   - Encode query â†’ vector
   - Call Supabase RPC `match_embeddings_by_user`
   - RPC function:
     - JOIN `document_embeddings` vá»›i `documents`
     - Filter theo `created_by = user_id`
     - TÃ­nh cosine similarity báº±ng pgvector
     - Sort + Limit top_k

**Kiáº¿n thá»©c:**
- Cosine similarity â†’ Semantic search
- Vector search vá»›i pgvector
- RPC (Remote Procedure Call) trong Supabase
- Numpy operations

**Táº¡i sao cosine similarity?**
- Measure semantic similarity
- KhÃ´ng phá»¥ thuá»™c vÃ o Ä‘á»™ dÃ i vector
- Fast computation

---

#### ğŸ”¹ File 8: `src/prompt_builder.py`
**Chá»©c nÄƒng:** XÃ¢y dá»±ng prompt cho LLM

**Äá»c theo thá»© tá»±:**
1. **`_DEFAULT_SYSTEM_PROMPT`** (dÃ²ng 7-11)
   - HÆ°á»›ng dáº«n LLM cÃ¡ch tráº£ lá»i
   - Chá»‰ dÃ¹ng thÃ´ng tin trong Context
   - Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t

2. **Function `build_rag_prompt()`** (dÃ²ng 14-45)
   - Input: query + list of chunks + (optional) system_prompt
   - Format chunks thÃ nh context:
     ```
     Äoáº¡n 1 | Trang 5 | Score: 0.9500
     [Content cá»§a chunk 1]
     
     Äoáº¡n 2 | Trang 6 | Score: 0.8500
     [Content cá»§a chunk 2]
     ```
   - Final prompt:
     ```
     [System Prompt]
     
     Context:
     [Formatted chunks]
     
     CÃ¢u há»i: [User query]
     HÃ£y cung cáº¥p cÃ¢u tráº£ lá»i ngáº¯n gá»n...
     ```

**Kiáº¿n thá»©c:**
- Prompt engineering
- Context injection
- Template pattern

**Táº¡i sao cáº§n System Prompt?**
- HÆ°á»›ng dáº«n behavior cá»§a LLM
- Giáº£m hallucination
- Äá»‹nh dáº¡ng output

---

#### ğŸ”¹ File 9: `src/llm_client.py`
**Chá»©c nÄƒng:** Gá»i Ollama LLM Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i

**Äá»c theo thá»© tá»±:**
1. **Class `LLMResponse`** (dÃ²ng 14-19)
   - `answer`: CÃ¢u tráº£ lá»i tá»« LLM
   - `model`: TÃªn model (llama3)
   - `raw`: Full response JSON

2. **Function `generate_answer()`** (dÃ²ng 28-60)
   - **NEW:** CÃ³ `@retry_with_backoff` decorator
   - POST request â†’ `http://localhost:11434/api/generate`
   - Payload: `{model, prompt, stream: false}`
   - Parse response â†’ Extract answer
   - Error handling: Connection error, HTTP error, Invalid response

**Kiáº¿n thá»©c:**
- Ollama API
- HTTP requests vá»›i `requests` library
- Retry logic vá»›i exponential backoff
- Error handling vá»›i custom exceptions

**Táº¡i sao Ollama?**
- Local LLM â†’ Privacy
- No API costs
- Offline capable
- llama3 â†’ Good Vietnamese support

---

#### ğŸ”¹ File 10: `src/rag_service.py` â­ **MAIN ORCHESTRATOR**
**Chá»©c nÄƒng:** Káº¿t há»£p retrieval + prompt + LLM

**Äá»c theo thá»© tá»±:**
1. **Function `_serialize_chunk()`** (dÃ²ng 13-20)
   - Convert `RetrievedChunk` â†’ JSON-friendly dict

2. **Function `rag_query()`** (dÃ²ng 23-97) â­â­â­
   - **MAIN RAG WORKFLOW**
   - **NEW:** Input validation vá»›i Pydantic
   
   **Flow:**
   ```python
   # 1. VALIDATION
   validated = RAGQueryRequest(query, user_id, top_k, system_prompt)
   
   # 2. RETRIEVAL
   chunks = retrieve_similar_chunks_by_user(
       query=validated.query,
       user_id=validated.user_id,
       top_k=validated.top_k
   )
   
   # 3. PROMPT BUILDING
   prompt = build_rag_prompt(
       query=validated.query,
       chunks=chunks,
       system_prompt=validated.system_prompt
   )
   
   # 4. LLM GENERATION
   llm_response = generate_answer(prompt)
   
   # 5. RETURN RESPONSE
   return {
       "answer": llm_response.answer,
       "sources": [serialized chunks],
       "metadata": {model, query_time_ms, chunk_count},
       "prompt": full_prompt,
       "raw_llm_response": raw_data
   }
   ```

**Kiáº¿n thá»©c:**
- Orchestration pattern
- Performance tracking vá»›i `perf_counter()`
- Input validation vá»›i Pydantic
- Comprehensive error handling

---

### **GIAI ÄOáº N 4: HELPERS & UTILITIES**

#### ğŸ”¹ File 11: `src/validators.py`
**Chá»©c nÄƒng:** Input validation vá»›i Pydantic

**Äá»c cÃ¡c classes:**
1. **`RAGQueryRequest`** (dÃ²ng 6-37)
   - Validate query (min 1, max 2000 chars)
   - Validate user_id (pháº£i lÃ  UUID)
   - Validate top_k (1-20)
   - Auto strip whitespace

2. **`DocumentIngestRequest`** (dÃ²ng 40-53)
   - Validate document_id (UUID)
   - Force refresh flag

3. **`ChunkConfig`** (dÃ²ng 56-72)
   - Validate chunk_size (100-2000)
   - Validate chunk_overlap < chunk_size

**Kiáº¿n thá»©c:**
- Pydantic v2 validation
- `@field_validator` decorator
- Cross-field validation
- UUID validation

---

#### ğŸ”¹ File 12: `src/retry_utils.py`
**Chá»©c nÄƒng:** Retry logic vá»›i exponential backoff

**Äá»c theo thá»© tá»±:**
1. **Function `retry_with_backoff()`** (dÃ²ng 11-68)
   - Decorator pattern
   - Retry logic: 1s â†’ 2s â†’ 4s â†’ 8s
   - Configurable: max_retries, exceptions

2. **Class `CircuitBreaker`** (dÃ²ng 71-144)
   - States: CLOSED â†’ OPEN â†’ HALF_OPEN
   - Prevent cascading failures
   - Timeout-based recovery

**Kiáº¿n thá»©c:**
- Decorator pattern
- Exponential backoff
- Circuit breaker pattern
- Resilience engineering

---

## ğŸ¯ Äá»ŒC THEO USECASE

### **USECASE 1: User upload PDF â†’ Embedding**
Äá»c theo thá»© tá»±:
1. `scripts/ingest_document.py` (entry point)
2. `src/pipeline.py` â†’ `process_document()`
3. `src/text_extractor.py` â†’ `extract_pdf_text()`
4. `src/chunker.py` â†’ `split_chunks()`
5. `src/embedder.py` â†’ `embed_chunks()`
6. `src/supabase_client.py` â†’ `insert_embeddings()`

---

### **USECASE 2: User há»i cÃ¢u há»i â†’ Nháº­n answer**
Äá»c theo thá»© tá»±:
1. `scripts/rag_runner.py` (entry point tá»« Node.js)
2. `src/rag_service.py` â†’ `rag_query()` â­
3. `src/validators.py` â†’ Validate input
4. `src/retriever.py` â†’ `retrieve_similar_chunks_by_user()`
5. `src/prompt_builder.py` â†’ `build_rag_prompt()`
6. `src/llm_client.py` â†’ `generate_answer()`

---

## ğŸ“Š KIáº¾N THá»¨C Cáº¦N Náº®M

### **Python Concepts**
- âœ… Generators (`yield`)
- âœ… Decorators (`@retry_with_backoff`)
- âœ… Type hints (Python 3.10+)
- âœ… Dataclasses
- âœ… Context managers (try-finally)
- âœ… Singleton pattern

### **ML/AI Concepts**
- âœ… Text embeddings (Dense vectors)
- âœ… Semantic similarity (Cosine)
- âœ… Chunking strategies
- âœ… RAG (Retrieval-Augmented Generation)
- âœ… Prompt engineering

### **Libraries**
- âœ… LangChain (RecursiveCharacterTextSplitter)
- âœ… Sentence Transformers (Embeddings)
- âœ… Supabase Python SDK
- âœ… Pydantic (Validation)
- âœ… NumPy (Vector operations)
- âœ… PyPDF (PDF parsing)

---

## ğŸš€ QUICK START - Äá»ŒC NHANH (15 phÃºt)

Náº¿u chá»‰ cÃ³ 15 phÃºt, Ä‘á»c 5 files nÃ y theo thá»© tá»±:

1. **`src/config.py`** - Settings
2. **`src/pipeline.py`** - Ingestion flow
3. **`src/rag_service.py`** - Query flow â­
4. **`src/retriever.py`** - Semantic search
5. **`src/llm_client.py`** - LLM generation

---

## ğŸ“ TIPS Äá»ŒC CODE HIá»†U QUáº¢

### âœ… **DO:**
- Äá»c theo flow (khÃ´ng nháº£y lung tung)
- ChÃº Ã½ comments tiáº¿ng Viá»‡t
- Váº½ diagram flow trÃªn giáº¥y
- Cháº¡y tests Ä‘á»ƒ hiá»ƒu behavior
- Debug step-by-step

### âŒ **DON'T:**
- Äá»c tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i tá»«ng file
- Bá» qua docstrings
- Äá»c khÃ´ng theo usecase
- Cá»‘ nhá»› táº¥t cáº£ details

---

## ğŸ” DEBUG & EXPERIMENT

### Cháº¡y thá»­ tá»«ng bÆ°á»›c

```python
# 1. Test text extraction
from src.text_extractor import extract_pdf_text
chunks = list(extract_pdf_text(Path("test.pdf")))
print(chunks[0].text)

# 2. Test chunking
from src.chunker import split_chunks
text_chunks = list(split_chunks(chunks))
print(f"Total chunks: {len(text_chunks)}")

# 3. Test embedding
from src.embedder import embed_chunks
embeddings = embed_chunks(text_chunks[:5])  # Embed 5 chunks
print(embeddings[0].vector.shape)  # (768,)

# 4. Test retrieval
from src.retriever import retrieve_similar_chunks_by_user
results = retrieve_similar_chunks_by_user(
    query="What is Python?",
    user_id="user-uuid",
    top_k=3
)
print(results[0].content)

# 5. Test RAG query
from src.rag_service import rag_query
response = rag_query(
    query="Explain OOP",
    user_id="user-uuid",
    top_k=5
)
print(response["answer"])
```

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- **LangChain:** https://python.langchain.com/docs/modules/data_connection/document_transformers/
- **Sentence Transformers:** https://www.sbert.net/
- **Supabase Python:** https://supabase.com/docs/reference/python/
- **Pydantic:** https://docs.pydantic.dev/
- **Ollama:** https://ollama.ai/

---

## âœ¨ TÃ“M Táº®T

### **Ingestion Pipeline:**
```
PDF â†’ Text â†’ Chunks â†’ Embeddings â†’ Database
```

### **Query Pipeline:**
```
Question â†’ Vector â†’ Search Similar â†’ Build Prompt â†’ LLM â†’ Answer
```

### **Core Concepts:**
- ğŸ“¦ Chunking: 900 chars, overlap 200
- ğŸ”¢ Embeddings: 768-dim vectors
- ğŸ” Search: Cosine similarity
- ğŸ¤– LLM: Ollama llama3
- ğŸ’¾ Storage: Supabase pgvector

---

**Happy Learning! ğŸ‰**

Báº¯t Ä‘áº§u tá»« `src/config.py` â†’ `src/pipeline.py` â†’ `src/rag_service.py`
