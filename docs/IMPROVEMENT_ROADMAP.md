# ğŸš€ ROADMAP Cáº¢I THIá»†N Há»† THá»NG RAG - EMBEDDING_LANGCHAIN

**NgÃ y táº¡o:** 2 thÃ¡ng 12, 2025  
**ÄÃ¡nh giÃ¡ hiá»‡n táº¡i:** 7.5-8/10  
**Má»¥c tiÃªu:** 9-9.5/10

---

## ğŸ“Š TÃ“M Táº®T ÄÃNH GIÃ

### âœ… Äiá»ƒm máº¡nh
- Kiáº¿n trÃºc RAG pipeline rÃµ rÃ ng, tÃ¡ch biá»‡t module tá»‘t
- Documentation xuáº¥t sáº¯c (PHASE_A/B_EXPLAINED.md)
- Code clean, cÃ³ comments tiáº¿ng Viá»‡t Ä‘áº§y Ä‘á»§
- Integration vá»›i backend Node.js tá»‘t
- Memory optimization vá»›i `__slots__`, generators

### âŒ Äiá»ƒm yáº¿u chÃ­nh
1. **KhÃ´ng cÃ³ Unit Tests** (nghiÃªm trá»ng nháº¥t)
2. Thiáº¿u Error Recovery & Retry Logic
3. Thiáº¿u Performance Monitoring
4. Thiáº¿u Caching Layer
5. ChÆ°a cÃ³ Batch Processing

---

# ğŸ”´ PHASE 1: CRITICAL FIXES (Báº®T BUá»˜C)

## âœ… BÆ¯á»šC 1: ThÃªm Unit Tests (Æ¯u tiÃªn cao nháº¥t)

### 1.1. Setup Testing Framework

```bash
# CÃ i Ä‘áº·t dependencies
cd Embedding_langchain
pip install pytest pytest-cov pytest-mock pytest-asyncio

# ThÃªm vÃ o requirements.txt
echo "pytest>=7.4.0" >> requirements.txt
echo "pytest-cov>=4.1.0" >> requirements.txt
echo "pytest-mock>=3.12.0" >> requirements.txt
```

### 1.2. Táº¡o cáº¥u trÃºc thÆ° má»¥c tests

```
Embedding_langchain/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py              # Pytest fixtures dÃ¹ng chung
â”‚   â”œâ”€â”€ test_text_extractor.py   # Test PDF extraction
â”‚   â”œâ”€â”€ test_chunker.py          # Test text chunking
â”‚   â”œâ”€â”€ test_embedder.py         # Test embedding generation
â”‚   â”œâ”€â”€ test_retriever.py        # Test similarity search
â”‚   â”œâ”€â”€ test_pipeline.py         # Test orchestration
â”‚   â”œâ”€â”€ test_rag_service.py      # Test RAG workflow
â”‚   â”œâ”€â”€ test_prompt_builder.py   # Test prompt construction
â”‚   â””â”€â”€ test_llm_client.py       # Test Ollama integration
â”œâ”€â”€ pytest.ini                   # Pytest configuration
â””â”€â”€ .coveragerc                  # Coverage configuration
```

### 1.3. File: `tests/conftest.py`

```python
"""Pytest fixtures vÃ  configuration dÃ¹ng chung."""
import pytest
from pathlib import Path
import tempfile
import shutil
from unittest.mock import MagicMock

@pytest.fixture
def temp_dir():
    """Táº¡o thÆ° má»¥c táº¡m cho tests."""
    tmp = tempfile.mkdtemp()
    yield Path(tmp)
    shutil.rmtree(tmp, ignore_errors=True)

@pytest.fixture
def sample_text():
    """Text máº«u cho testing."""
    return """
    Python lÃ  má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh báº­c cao.
    Python Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong AI vÃ  Machine Learning.
    LangChain lÃ  framework Ä‘á»ƒ xÃ¢y dá»±ng LLM applications.
    """

@pytest.fixture
def mock_supabase_client(mocker):
    """Mock Supabase client."""
    mock_client = MagicMock()
    mocker.patch('src.supabase_client.get_supabase_client', return_value=mock_client)
    return mock_client

@pytest.fixture
def mock_sentence_transformer(mocker):
    """Mock SentenceTransformer model."""
    mock_model = MagicMock()
    mock_model.encode.return_value = [[0.1] * 768]  # Fake 768-dim vector
    mocker.patch('sentence_transformers.SentenceTransformer', return_value=mock_model)
    return mock_model
```

### 1.4. File: `tests/test_chunker.py`

```python
"""Tests cho text chunking module."""
import pytest
from src.chunker import split_chunks, TextChunk
from src.text_extractor import DocumentChunk

def test_split_chunks_empty_input():
    """Test vá»›i input rá»—ng."""
    chunks = list(split_chunks([]))
    assert len(chunks) == 0

def test_split_chunks_single_small_chunk():
    """Test vá»›i 1 chunk nhá» hÆ¡n chunk_size."""
    doc_chunk = DocumentChunk(text="Python lÃ  ngÃ´n ngá»¯ láº­p trÃ¬nh.", page_number=1)
    result = list(split_chunks([doc_chunk]))
    
    assert len(result) == 1
    assert result[0].text == "Python lÃ  ngÃ´n ngá»¯ láº­p trÃ¬nh."
    assert result[0].page_number == 1
    assert result[0].chunk_index == 1

def test_split_chunks_preserves_page_number():
    """Test giá»¯ nguyÃªn page_number tá»« DocumentChunk."""
    doc_chunk = DocumentChunk(text="A" * 1000, page_number=5)
    result = list(split_chunks([doc_chunk]))
    
    assert all(chunk.page_number == 5 for chunk in result)

def test_split_chunks_increments_index():
    """Test chunk_index tÄƒng dáº§n."""
    doc_chunks = [
        DocumentChunk(text="A" * 1000, page_number=1),
        DocumentChunk(text="B" * 1000, page_number=2),
    ]
    result = list(split_chunks(doc_chunks))
    
    # Kiá»ƒm tra chunk_index tÄƒng liÃªn tá»¥c
    for i, chunk in enumerate(result, start=1):
        assert chunk.chunk_index == i

def test_split_chunks_removes_empty():
    """Test loáº¡i bá» chunks rá»—ng sau khi strip."""
    doc_chunk = DocumentChunk(text="   \n\n   ", page_number=1)
    result = list(split_chunks([doc_chunk]))
    
    assert len(result) == 0
```

### 1.5. File: `tests/test_embedder.py`

```python
"""Tests cho embedding generation."""
import pytest
import numpy as np
from src.embedder import embed_chunks, EmbeddingResult
from src.chunker import TextChunk

def test_embed_chunks_empty_input():
    """Test vá»›i danh sÃ¡ch rá»—ng."""
    result = embed_chunks([])
    assert result == []

def test_embed_chunks_returns_correct_type(mock_sentence_transformer):
    """Test tráº£ vá» list[EmbeddingResult]."""
    chunks = [TextChunk(text="Python", chunk_index=1)]
    result = embed_chunks(chunks)
    
    assert isinstance(result, list)
    assert len(result) == 1
    assert isinstance(result[0], EmbeddingResult)

def test_embed_chunks_vector_dimension(mock_sentence_transformer):
    """Test vector cÃ³ Ä‘Ãºng 768 chiá»u."""
    chunks = [TextChunk(text="Test", chunk_index=1)]
    result = embed_chunks(chunks)
    
    assert result[0].vector.shape == (768,)
    assert result[0].vector.dtype == np.float32

def test_embed_chunks_preserves_chunk_data(mock_sentence_transformer):
    """Test giá»¯ nguyÃªn thÃ´ng tin chunk."""
    chunk = TextChunk(text="Hello", page_number=3, chunk_index=5)
    result = embed_chunks([chunk])
    
    assert result[0].chunk.text == "Hello"
    assert result[0].chunk.page_number == 3
    assert result[0].chunk.chunk_index == 5
```

### 1.6. File: `tests/test_retriever.py`

```python
"""Tests cho retrieval & similarity search."""
import pytest
import numpy as np
from src.retriever import _cosine_similarity, retrieve_similar_chunks

def test_cosine_similarity_identical_vectors():
    """Test 2 vectors giá»‘ng nhau â†’ similarity = 1.0."""
    a = np.array([1.0, 2.0, 3.0])
    b = np.array([1.0, 2.0, 3.0])
    
    similarity = _cosine_similarity(a, b)
    assert abs(similarity - 1.0) < 0.0001

def test_cosine_similarity_orthogonal_vectors():
    """Test 2 vectors vuÃ´ng gÃ³c â†’ similarity = 0.0."""
    a = np.array([1.0, 0.0])
    b = np.array([0.0, 1.0])
    
    similarity = _cosine_similarity(a, b)
    assert abs(similarity - 0.0) < 0.0001

def test_cosine_similarity_zero_vector():
    """Test vá»›i zero vector â†’ similarity = 0.0."""
    a = np.array([0.0, 0.0, 0.0])
    b = np.array([1.0, 2.0, 3.0])
    
    similarity = _cosine_similarity(a, b)
    assert similarity == 0.0

def test_retrieve_similar_chunks_empty_query():
    """Test query rá»—ng â†’ raise ValueError."""
    with pytest.raises(ValueError, match="Query khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng"):
        retrieve_similar_chunks(query="", document_id="abc")

def test_retrieve_similar_chunks_empty_document_id():
    """Test document_id rá»—ng â†’ raise ValueError."""
    with pytest.raises(ValueError, match="document_id khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng"):
        retrieve_similar_chunks(query="test", document_id="")
```

### 1.7. File: `tests/test_text_extractor.py`

```python
"""Tests cho PDF text extraction."""
import pytest
from pathlib import Path
from src.text_extractor import clean_text, DocumentChunk

def test_clean_text_removes_null_bytes():
    """Test loáº¡i bá» null bytes."""
    text = "Hello\x00World"
    result = clean_text(text)
    assert "\x00" not in result
    assert result == "Hello World"

def test_clean_text_joins_hyphenated_words():
    """Test ná»‘i tá»« bá»‹ ngáº¯t dÃ²ng."""
    text = "process- \ning data"
    result = clean_text(text)
    assert result == "processing data"

def test_clean_text_normalizes_whitespace():
    """Test chuáº©n hÃ³a khoáº£ng tráº¯ng."""
    text = "Hello    \n\n  World  \t  Test"
    result = clean_text(text)
    assert result == "Hello World Test"

def test_clean_text_empty_input():
    """Test vá»›i input rá»—ng."""
    result = clean_text("")
    assert result == ""

def test_document_chunk_to_dict():
    """Test DocumentChunk.to_dict() serialization."""
    chunk = DocumentChunk(text="Test", page_number=5, source_file="test.pdf")
    result = chunk.to_dict()
    
    assert result == {
        "text": "Test",
        "page_number": 5,
        "source_file": "test.pdf"
    }
```

### 1.8. File: `tests/test_prompt_builder.py`

```python
"""Tests cho prompt construction."""
import pytest
from src.prompt_builder import build_rag_prompt
from src.retriever import RetrievedChunk

def test_build_rag_prompt_empty_query():
    """Test query rá»—ng â†’ raise ValueError."""
    with pytest.raises(ValueError, match="Query khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng"):
        build_rag_prompt(query="", chunks=[])

def test_build_rag_prompt_no_chunks():
    """Test khÃ´ng cÃ³ chunks â†’ hiá»ƒn thá»‹ thÃ´ng bÃ¡o khÃ´ng cÃ³ context."""
    prompt = build_rag_prompt(query="Test question", chunks=[])
    
    assert "Test question" in prompt
    assert "KhÃ´ng cÃ³ context phÃ¹ há»£p" in prompt

def test_build_rag_prompt_with_chunks():
    """Test vá»›i chunks há»£p lá»‡."""
    chunks = [
        RetrievedChunk(content="Python is great", chunk_index=1, page_number=5, similarity=0.95),
        RetrievedChunk(content="AI is powerful", chunk_index=2, page_number=6, similarity=0.85),
    ]
    prompt = build_rag_prompt(query="What is Python?", chunks=chunks)
    
    assert "What is Python?" in prompt
    assert "Python is great" in prompt
    assert "AI is powerful" in prompt
    assert "Trang 5" in prompt
    assert "0.9500" in prompt

def test_build_rag_prompt_custom_system_prompt():
    """Test custom system prompt."""
    custom_prompt = "You are a helpful assistant."
    chunks = [RetrievedChunk(content="Test", chunk_index=1, page_number=1, similarity=0.9)]
    
    result = build_rag_prompt(query="Test", chunks=chunks, system_prompt=custom_prompt)
    assert "You are a helpful assistant." in result
```

### 1.9. File: `pytest.ini`

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --cov=src
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=70
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
```

### 1.10. File: `.coveragerc`

```ini
[run]
source = src
omit = 
    */tests/*
    */__pycache__/*
    */venv/*

[report]
precision = 2
show_missing = True
skip_covered = False

[html]
directory = htmlcov
```

### 1.11. Cháº¡y Tests

```bash
# Cháº¡y táº¥t cáº£ tests
pytest

# Cháº¡y vá»›i coverage report
pytest --cov=src --cov-report=html

# Cháº¡y test cá»¥ thá»ƒ
pytest tests/test_chunker.py -v

# Cháº¡y tests theo marker
pytest -m "not slow"

# Xem coverage report
# Má»Ÿ file htmlcov/index.html trong browser
```

---

## âœ… BÆ¯á»šC 2: ThÃªm Error Recovery & Retry Logic

### 2.1. File: `src/retry_utils.py` (Má»šI)

```python
"""Utilities cho retry logic vá»›i exponential backoff."""
import time
import logging
from functools import wraps
from typing import Callable, Type, Tuple

logger = logging.getLogger(__name__)

def retry_with_backoff(
    max_retries: int = 3,
    initial_delay: float = 1.0,
    backoff_factor: float = 2.0,
    exceptions: Tuple[Type[Exception], ...] = (Exception,)
):
    """
    Decorator retry vá»›i exponential backoff.
    
    Args:
        max_retries: Sá»‘ láº§n retry tá»‘i Ä‘a
        initial_delay: Delay ban Ä‘áº§u (giÃ¢y)
        backoff_factor: Há»‡ sá»‘ nhÃ¢n cho má»—i láº§n retry
        exceptions: Tuple cÃ¡c exception cáº§n retry
    
    Example:
        @retry_with_backoff(max_retries=3, exceptions=(requests.RequestException,))
        def call_api():
            return requests.get("https://api.example.com")
    """
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt == max_retries:
                        logger.error(
                            f"{func.__name__} failed after {max_retries} retries: {e}"
                        )
                        raise
                    
                    logger.warning(
                        f"{func.__name__} failed (attempt {attempt + 1}/{max_retries}), "
                        f"retrying in {delay:.1f}s: {e}"
                    )
                    time.sleep(delay)
                    delay *= backoff_factor
            
            raise last_exception
        
        return wrapper
    return decorator
```

### 2.2. Cáº­p nháº­t `src/llm_client.py`

```python
# ThÃªm import
from .retry_utils import retry_with_backoff

# Thay Ä‘á»•i function generate_answer
@retry_with_backoff(
    max_retries=3,
    initial_delay=2.0,
    exceptions=(requests.RequestException, requests.Timeout)
)
def generate_answer(prompt: str, model: str | None = None, timeout: int = 120) -> LLMResponse:
    """Gá»i Ollama generate API vá»›i retry logic."""
    if not prompt.strip():
        raise ValueError("Prompt khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng")

    target_model = model or settings.ollama_model
    if not target_model:
        raise ValueError("ChÆ°a cáº¥u hÃ¬nh OLLAMA_MODEL")

    url = settings.ollama_url.rstrip("/") + "/api/generate"
    payload = {
        "model": target_model,
        "prompt": prompt,
        "stream": False,
    }

    # ThÃªm timeout cho má»—i request
    response = requests.post(url, json=payload, timeout=timeout)

    if response.status_code != 200:
        text = response.text[:500]
        raise LLMClientError(f"Ollama tráº£ vá» mÃ£ lá»—i {response.status_code}: {text}")

    data = response.json()
    answer = data.get("response")
    if not isinstance(answer, str):
        raise LLMClientError("Pháº£n há»“i tá»« Ollama khÃ´ng há»£p lá»‡: thiáº¿u trÆ°á»ng 'response'")

    return LLMResponse(answer=answer.strip(), model=target_model, raw=data)
```

### 2.3. Cáº­p nháº­t `src/supabase_client.py`

```python
# ThÃªm import
from .retry_utils import retry_with_backoff

# ThÃªm retry cho download_file
@retry_with_backoff(max_retries=3, initial_delay=1.0)
def download_file(file_path: str, destination: Path) -> Path:
    """Táº£i tá»‡p tá»« bucket Supabase vá»›i retry logic."""
    client = get_supabase_client()
    response = client.storage.from_(settings.supabase_bucket).download(file_path)
    destination.parent.mkdir(parents=True, exist_ok=True)
    destination.write_bytes(response)
    return destination

# ThÃªm retry cho insert_embeddings
@retry_with_backoff(max_retries=3, initial_delay=1.0, exceptions=(APIError,))
def insert_embeddings(rows: list[dict[str, Any]]) -> None:
    """ChÃ¨n danh sÃ¡ch embedding vá»›i retry logic."""
    if not rows:
        return
    client = get_supabase_client()
    
    # Batch insert náº¿u quÃ¡ nhiá»u rows (trÃ¡nh timeout)
    BATCH_SIZE = 100
    for i in range(0, len(rows), BATCH_SIZE):
        batch = rows[i:i + BATCH_SIZE]
        client.table("document_embeddings").insert(batch).execute()
        logger.info(f"Inserted batch {i//BATCH_SIZE + 1}, {len(batch)} embeddings")
```

---

## âœ… BÆ¯á»šC 3: ThÃªm Input Validation vá»›i Pydantic

### 3.1. CÃ i Ä‘áº·t Pydantic

```bash
pip install pydantic>=2.0.0
echo "pydantic>=2.0.0" >> requirements.txt
```

### 3.2. File: `src/validators.py` (Má»šI)

```python
"""Input validation schemas vá»›i Pydantic."""
from pydantic import BaseModel, Field, validator
from typing import Optional
import uuid

class RAGQueryRequest(BaseModel):
    """Validation cho RAG query request."""
    query: str = Field(..., min_length=1, max_length=2000)
    user_id: str = Field(..., description="UUID cá»§a user")
    top_k: int = Field(default=5, ge=1, le=20)
    system_prompt: Optional[str] = Field(None, max_length=1000)
    
    @validator('query')
    def query_not_empty(cls, v):
        if not v.strip():
            raise ValueError('Query khÃ´ng Ä‘Æ°á»£c chá»‰ chá»©a khoáº£ng tráº¯ng')
        return v.strip()
    
    @validator('user_id')
    def validate_uuid(cls, v):
        try:
            uuid.UUID(v)
        except ValueError:
            raise ValueError('user_id pháº£i lÃ  UUID há»£p lá»‡')
        return v

class DocumentIngestRequest(BaseModel):
    """Validation cho document ingestion."""
    document_id: str = Field(..., description="UUID cá»§a document")
    
    @validator('document_id')
    def validate_uuid(cls, v):
        try:
            uuid.UUID(v)
        except ValueError:
            raise ValueError('document_id pháº£i lÃ  UUID há»£p lá»‡')
        return v

class ChunkConfig(BaseModel):
    """Validation cho chunking configuration."""
    chunk_size: int = Field(default=900, ge=100, le=2000)
    chunk_overlap: int = Field(default=200, ge=0, le=500)
    
    @validator('chunk_overlap')
    def overlap_less_than_size(cls, v, values):
        if 'chunk_size' in values and v >= values['chunk_size']:
            raise ValueError('chunk_overlap pháº£i nhá» hÆ¡n chunk_size')
        return v
```

### 3.3. Cáº­p nháº­t `src/rag_service.py`

```python
# ThÃªm import
from .validators import RAGQueryRequest

# Cáº­p nháº­t function rag_query
def rag_query(
    query: str,
    user_id: str,
    top_k: int = 5,
    system_prompt: str | None = None,
) -> Dict[str, Any]:
    """RAG query vá»›i input validation."""
    
    # Validate input vá»›i Pydantic
    try:
        validated = RAGQueryRequest(
            query=query,
            user_id=user_id,
            top_k=top_k,
            system_prompt=system_prompt
        )
    except Exception as e:
        raise ValueError(f"Invalid input: {e}")
    
    start = perf_counter()
    
    # Sá»­ dá»¥ng validated data
    retrieved_chunks = retrieve_similar_chunks_by_user(
        query=validated.query,
        user_id=validated.user_id,
        top_k=validated.top_k
    )
    
    prompt = build_rag_prompt(
        query=validated.query,
        chunks=retrieved_chunks,
        system_prompt=validated.system_prompt
    )
    
    llm_response: LLMResponse = generate_answer(prompt=prompt)
    elapsed_ms = (perf_counter() - start) * 1000

    return {
        "answer": llm_response.answer,
        "sources": [_serialize_chunk(chunk) for chunk in retrieved_chunks],
        "metadata": {
            "model": llm_response.model,
            "query_time_ms": round(elapsed_ms, 2),
            "chunk_count": len(retrieved_chunks),
        },
        "prompt": prompt,
        "raw_llm_response": llm_response.raw,
    }
```

---

# ğŸŸ¡ PHASE 2: HIGH PRIORITY IMPROVEMENTS

## âœ… BÆ¯á»šC 4: Performance Monitoring

### 4.1. File: `src/performance_monitor.py` (Má»šI)

```python
"""Performance monitoring vÃ  metrics tracking."""
import time
import logging
from functools import wraps
from typing import Callable, Dict, Any
from collections import defaultdict
from dataclasses import dataclass, field
from threading import Lock

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """LÆ°u trá»¯ performance metrics."""
    total_calls: int = 0
    total_time: float = 0.0
    min_time: float = float('inf')
    max_time: float = 0.0
    errors: int = 0
    
    def update(self, duration: float, is_error: bool = False):
        """Cáº­p nháº­t metrics."""
        self.total_calls += 1
        self.total_time += duration
        self.min_time = min(self.min_time, duration)
        self.max_time = max(self.max_time, duration)
        if is_error:
            self.errors += 1
    
    @property
    def avg_time(self) -> float:
        """TÃ­nh thá»i gian trung bÃ¬nh."""
        return self.total_time / self.total_calls if self.total_calls > 0 else 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert sang dictionary."""
        return {
            "total_calls": self.total_calls,
            "total_time_seconds": round(self.total_time, 3),
            "avg_time_seconds": round(self.avg_time, 3),
            "min_time_seconds": round(self.min_time, 3),
            "max_time_seconds": round(self.max_time, 3),
            "errors": self.errors,
            "success_rate": round((self.total_calls - self.errors) / self.total_calls * 100, 2) if self.total_calls > 0 else 0.0
        }

class PerformanceMonitor:
    """Singleton class quáº£n lÃ½ performance metrics."""
    _instance = None
    _lock = Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._metrics = defaultdict(PerformanceMetrics)
        return cls._instance
    
    def record(self, function_name: str, duration: float, is_error: bool = False):
        """Ghi nháº­n metrics cho function."""
        self._metrics[function_name].update(duration, is_error)
    
    def get_metrics(self, function_name: str = None) -> Dict[str, Any]:
        """Láº¥y metrics cá»§a function hoáº·c táº¥t cáº£."""
        if function_name:
            return {function_name: self._metrics[function_name].to_dict()}
        return {name: metrics.to_dict() for name, metrics in self._metrics.items()}
    
    def reset(self):
        """Reset táº¥t cáº£ metrics."""
        self._metrics.clear()

# Global monitor instance
monitor = PerformanceMonitor()

def track_performance(func: Callable):
    """Decorator Ä‘á»ƒ track performance cá»§a function."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        is_error = False
        
        try:
            result = func(*args, **kwargs)
            return result
        except Exception as e:
            is_error = True
            raise
        finally:
            duration = time.perf_counter() - start
            monitor.record(func.__name__, duration, is_error)
            logger.debug(f"{func.__name__} took {duration:.3f}s")
    
    return wrapper
```

### 4.2. Cáº­p nháº­t cÃ¡c modules vá»›i performance tracking

```python
# src/embedder.py
from .performance_monitor import track_performance

@track_performance
def embed_chunks(chunks: Iterable[TextChunk]) -> List[EmbeddingResult]:
    """Sinh embedding vá»›i performance tracking."""
    # ... existing code ...

# src/retriever.py
@track_performance
def retrieve_similar_chunks_by_user(query: str, user_id: str, top_k: int = 5):
    """Retrieve vá»›i performance tracking."""
    # ... existing code ...

# src/llm_client.py
@track_performance
def generate_answer(prompt: str, model: str | None = None, timeout: int = 120):
    """Generate answer vá»›i performance tracking."""
    # ... existing code ...
```

### 4.3. File: `scripts/show_metrics.py` (Má»šI)

```python
#!/usr/bin/env python
"""Script hiá»ƒn thá»‹ performance metrics."""
import json
from pathlib import Path
import sys

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.performance_monitor import monitor

def main():
    """Hiá»ƒn thá»‹ metrics."""
    metrics = monitor.get_metrics()
    
    print("\n" + "="*60)
    print("PERFORMANCE METRICS")
    print("="*60)
    
    for func_name, data in metrics.items():
        print(f"\nğŸ“Š {func_name}")
        print(f"   Total calls: {data['total_calls']}")
        print(f"   Avg time: {data['avg_time_seconds']:.3f}s")
        print(f"   Min time: {data['min_time_seconds']:.3f}s")
        print(f"   Max time: {data['max_time_seconds']:.3f}s")
        print(f"   Errors: {data['errors']}")
        print(f"   Success rate: {data['success_rate']:.2f}%")
    
    # Export to JSON
    output_file = Path("performance_metrics.json")
    output_file.write_text(json.dumps(metrics, indent=2, ensure_ascii=False))
    print(f"\nâœ… Metrics exported to {output_file}")

if __name__ == "__main__":
    main()
```

---

## âœ… BÆ¯á»šC 5: Caching Layer

### 5.1. File: `src/cache_manager.py` (Má»šI)

```python
"""Simple in-memory cache cho embeddings vÃ  queries."""
import hashlib
import logging
from typing import Any, Optional
from functools import lru_cache
import pickle

logger = logging.getLogger(__name__)

class EmbeddingCache:
    """Cache cho embeddings Ä‘á»ƒ trÃ¡nh encode láº¡i text giá»‘ng nhau."""
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self._cache: dict[str, Any] = {}
    
    def _make_key(self, text: str) -> str:
        """Táº¡o cache key tá»« text (hash MD5)."""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def get(self, text: str) -> Optional[Any]:
        """Láº¥y embedding tá»« cache."""
        key = self._make_key(text)
        return self._cache.get(key)
    
    def set(self, text: str, embedding: Any):
        """LÆ°u embedding vÃ o cache."""
        if len(self._cache) >= self.max_size:
            # Simple LRU: xÃ³a item Ä‘áº§u tiÃªn
            self._cache.pop(next(iter(self._cache)))
        
        key = self._make_key(text)
        self._cache[key] = embedding
        logger.debug(f"Cached embedding for text (hash: {key[:8]}...)")
    
    def clear(self):
        """XÃ³a toÃ n bá»™ cache."""
        self._cache.clear()
        logger.info("Embedding cache cleared")

# Global cache instance
embedding_cache = EmbeddingCache(max_size=1000)

@lru_cache(maxsize=100)
def get_query_embedding_cached(query: str):
    """
    Cache cho query embeddings vá»›i LRU.
    Sá»­ dá»¥ng functools.lru_cache cho queries phá»• biáº¿n.
    """
    from .embedder import _get_model
    import numpy as np
    
    model = _get_model()
    vector = model.encode([query])[0]
    return np.asarray(vector, dtype=np.float32)
```

### 5.2. Cáº­p nháº­t `src/embedder.py` Ä‘á»ƒ sá»­ dá»¥ng cache

```python
from .cache_manager import embedding_cache

def embed_chunks(chunks: Iterable[TextChunk]) -> List[EmbeddingResult]:
    """Sinh embedding vá»›i caching."""
    chunk_list = list(chunks)
    if not chunk_list:
        return []
    
    model = _get_model()
    results = []
    texts_to_encode = []
    cache_indices = []
    
    # Check cache trÆ°á»›c
    for idx, chunk in enumerate(chunk_list):
        cached_vector = embedding_cache.get(chunk.text)
        if cached_vector is not None:
            results.append(EmbeddingResult(chunk=chunk, vector=cached_vector))
        else:
            texts_to_encode.append(chunk.text)
            cache_indices.append(idx)
    
    # Encode cÃ¡c text chÆ°a cÃ³ trong cache
    if texts_to_encode:
        embeddings = model.encode(texts_to_encode, show_progress_bar=True)
        for idx, (text, vector) in enumerate(zip(texts_to_encode, embeddings)):
            vector_np = np.array(vector, dtype=np.float32)
            # LÆ°u vÃ o cache
            embedding_cache.set(text, vector_np)
            # ThÃªm vÃ o results
            chunk_idx = cache_indices[idx]
            results.insert(chunk_idx, EmbeddingResult(
                chunk=chunk_list[chunk_idx],
                vector=vector_np
            ))
    
    return results
```

### 5.3. Cáº­p nháº­t `src/retriever.py` Ä‘á»ƒ sá»­ dá»¥ng query cache

```python
from .cache_manager import get_query_embedding_cached

def retrieve_similar_chunks_by_user(query: str, user_id: str, top_k: int = 5):
    """Retrieve vá»›i query embedding cache."""
    if not query.strip():
        raise ValueError("Query khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng")
    if not user_id.strip():
        raise ValueError("user_id khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng")
    
    # Sá»­ dá»¥ng cached query embedding
    query_vector = get_query_embedding_cached(query.strip())
    query_embedding_list = query_vector.tolist()
    
    # ... rest of the code ...
```

---

## âœ… BÆ¯á»šC 6: Batch Processing

### 6.1. File: `src/batch_processor.py` (Má»šI)

```python
"""Batch processing cho multiple documents."""
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
from dataclasses import dataclass

from .pipeline import process_document

logger = logging.getLogger(__name__)

@dataclass
class BatchResult:
    """Káº¿t quáº£ xá»­ lÃ½ batch."""
    document_id: str
    success: bool
    error: Optional[str] = None
    duration: float = 0.0

def process_documents_batch(
    document_ids: List[str],
    max_workers: int = 4
) -> List[BatchResult]:
    """
    Xá»­ lÃ½ batch documents song song vá»›i ThreadPoolExecutor.
    
    Args:
        document_ids: Danh sÃ¡ch document IDs cáº§n xá»­ lÃ½
        max_workers: Sá»‘ worker threads (máº·c Ä‘á»‹nh 4)
    
    Returns:
        List[BatchResult]: Káº¿t quáº£ xá»­ lÃ½ tá»«ng document
    """
    if not document_ids:
        return []
    
    logger.info(f"Starting batch processing for {len(document_ids)} documents")
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit táº¥t cáº£ tasks
        future_to_doc_id = {
            executor.submit(_process_single_document, doc_id): doc_id
            for doc_id in document_ids
        }
        
        # Collect results khi hoÃ n thÃ nh
        for future in as_completed(future_to_doc_id):
            doc_id = future_to_doc_id[future]
            try:
                result = future.result()
                results.append(result)
                logger.info(f"âœ… Completed {doc_id}: {result.success}")
            except Exception as e:
                logger.error(f"âŒ Failed {doc_id}: {e}")
                results.append(BatchResult(
                    document_id=doc_id,
                    success=False,
                    error=str(e)
                ))
    
    # Summary
    success_count = sum(1 for r in results if r.success)
    logger.info(
        f"Batch processing completed: {success_count}/{len(results)} successful"
    )
    
    return results

def _process_single_document(document_id: str) -> BatchResult:
    """Process má»™t document vÃ  tráº£ vá» BatchResult."""
    import time
    start = time.perf_counter()
    
    try:
        process_document(document_id)
        duration = time.perf_counter() - start
        return BatchResult(
            document_id=document_id,
            success=True,
            duration=duration
        )
    except Exception as e:
        duration = time.perf_counter() - start
        return BatchResult(
            document_id=document_id,
            success=False,
            error=str(e),
            duration=duration
        )
```

### 6.2. File: `scripts/batch_ingest.py` (Má»šI)

```python
#!/usr/bin/env python
"""Script cháº¡y batch ingestion cho multiple documents."""
import sys
import json
from pathlib import Path

# Add project root
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.batch_processor import process_documents_batch

def main():
    """Main function."""
    if len(sys.argv) < 2:
        print("Usage: python batch_ingest.py <doc_id1> <doc_id2> ...")
        print("   or: python batch_ingest.py --file document_ids.txt")
        sys.exit(1)
    
    # Parse document IDs
    if sys.argv[1] == "--file":
        # Äá»c tá»« file
        file_path = Path(sys.argv[2])
        document_ids = file_path.read_text().strip().split('\n')
    else:
        # Tá»« command line args
        document_ids = sys.argv[1:]
    
    print(f"ğŸ“¦ Processing {len(document_ids)} documents in batch...")
    
    # Process batch
    results = process_documents_batch(document_ids, max_workers=4)
    
    # Print summary
    print("\n" + "="*60)
    print("BATCH PROCESSING SUMMARY")
    print("="*60)
    
    for result in results:
        status = "âœ… SUCCESS" if result.success else "âŒ FAILED"
        print(f"{status} | {result.document_id} | {result.duration:.2f}s")
        if result.error:
            print(f"  Error: {result.error}")
    
    # Export results
    output = {
        "total": len(results),
        "success": sum(1 for r in results if r.success),
        "failed": sum(1 for r in results if not r.success),
        "results": [
            {
                "document_id": r.document_id,
                "success": r.success,
                "error": r.error,
                "duration": round(r.duration, 2)
            }
            for r in results
        ]
    }
    
    output_file = Path("batch_results.json")
    output_file.write_text(json.dumps(output, indent=2, ensure_ascii=False))
    print(f"\nğŸ“Š Results exported to {output_file}")

if __name__ == "__main__":
    main()
```

---

# ğŸŸ¢ PHASE 3: ADVANCED IMPROVEMENTS

## âœ… BÆ¯á»šC 7: Advanced Text Cleaning

### 7.1. Cáº­p nháº­t `src/text_extractor.py`

```python
import re
from pathlib import Path
from typing import Iterable
from pypdf import PdfReader
import logging

logger = logging.getLogger(__name__)

def clean_text(text: str) -> str:
    """
    HÃ m lÃ m sáº¡ch vÄƒn báº£n nÃ¢ng cao hÆ¡n.
    """
    if not text:
        return ""
    
    # 1. Loáº¡i bá» Null bytes
    text = text.replace("\x00", "")
    
    # 2. Loáº¡i bá» special characters thá»«a
    text = re.sub(r'[Â©Â®â„¢]', '', text)
    
    # 3. Ná»‘i cÃ¡c tá»« bá»‹ ngáº¯t dÃ²ng
    text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
    
    # 4. Loáº¡i bá» URLs
    text = re.sub(r'https?://\S+', '', text)
    
    # 5. Loáº¡i bá» emails
    text = re.sub(r'\S+@\S+', '', text)
    
    # 6. Chuáº©n hÃ³a khoáº£ng tráº¯ng
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 7. Loáº¡i bá» multiple punctuation
    text = re.sub(r'([!?.]){2,}', r'\1', text)
    
    return text
```

---

## âœ… BÆ¯á»šC 8: Semantic Chunking (Advanced)

### 8.1. File: `src/semantic_chunker.py` (Má»šI)

```python
"""Semantic-aware chunking strategy."""
import re
from typing import List, Iterator
from .text_extractor import DocumentChunk
from .chunker import TextChunk

class SemanticChunker:
    """Chunk text dá»±a trÃªn semantic boundaries (paragraphs, sections)."""
    
    def __init__(self, max_chunk_size: int = 900, overlap: int = 100):
        self.max_chunk_size = max_chunk_size
        self.overlap = overlap
    
    def split(self, chunks: List[DocumentChunk]) -> Iterator[TextChunk]:
        """Split documents thÃ nh semantic chunks."""
        global_chunk_index = 0
        
        for doc_chunk in chunks:
            # Split theo paragraphs trÆ°á»›c
            paragraphs = self._split_paragraphs(doc_chunk.text)
            
            current_chunk = []
            current_length = 0
            
            for para in paragraphs:
                para_length = len(para)
                
                if current_length + para_length > self.max_chunk_size:
                    # Flush current chunk
                    if current_chunk:
                        global_chunk_index += 1
                        yield TextChunk(
                            text=" ".join(current_chunk),
                            page_number=doc_chunk.page_number,
                            chunk_index=global_chunk_index,
                            source_file=doc_chunk.source_file
                        )
                    
                    # Start new chunk vá»›i overlap
                    if self.overlap > 0 and current_chunk:
                        overlap_text = current_chunk[-1][:self.overlap]
                        current_chunk = [overlap_text, para]
                        current_length = len(overlap_text) + para_length
                    else:
                        current_chunk = [para]
                        current_length = para_length
                else:
                    current_chunk.append(para)
                    current_length += para_length
            
            # Flush remaining
            if current_chunk:
                global_chunk_index += 1
                yield TextChunk(
                    text=" ".join(current_chunk),
                    page_number=doc_chunk.page_number,
                    chunk_index=global_chunk_index,
                    source_file=doc_chunk.source_file
                )
    
    def _split_paragraphs(self, text: str) -> List[str]:
        """Split text thÃ nh paragraphs."""
        # Split by double newlines hoáº·c section headers
        paragraphs = re.split(r'\n\n+', text)
        return [p.strip() for p in paragraphs if p.strip()]
```

---

## âœ… BÆ¯á»šC 9: Logging Configuration

### 9.1. File: `src/logging_config.py` (Má»šI)

```python
"""Centralized logging configuration."""
import logging
import sys
from pathlib import Path

def setup_logging(level: str = "INFO", log_file: Path = None):
    """
    Setup logging cho toÃ n bá»™ application.
    
    Args:
        level: Log level (DEBUG, INFO, WARNING, ERROR)
        log_file: Optional file path Ä‘á»ƒ ghi logs
    """
    log_level = getattr(logging, level.upper(), logging.INFO)
    
    # Format
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(name)s:%(lineno)d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    
    handlers = [console_handler]
    
    # File handler (optional)
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        handlers.append(file_handler)
    
    # Root logger
    logging.basicConfig(
        level=log_level,
        handlers=handlers
    )
    
    # Suppress noisy libraries
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('sentence_transformers').setLevel(logging.WARNING)
```

### 9.2. Cáº­p nháº­t `src/config.py`

```python
from .logging_config import setup_logging

# Setup logging khi import config
setup_logging(
    level=os.getenv("LOG_LEVEL", "INFO"),
    log_file=Path("logs/app.log") if os.getenv("LOG_TO_FILE") else None
)
```

---

## âœ… BÆ¯á»šC 10: CI/CD vá»›i GitHub Actions

### 10.1. File: `.github/workflows/tests.yml` (Má»šI)

```yaml
name: Tests

on:
  push:
    branches: [ main, devThien ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        cd Embedding_langchain
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock
    
    - name: Run tests
      run: |
        cd Embedding_langchain
        pytest --cov=src --cov-report=xml --cov-report=term
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./Embedding_langchain/coverage.xml
        flags: unittests
        name: codecov-umbrella
```

---

# ğŸ“Š CHECKLIST HOÃ€N THÃ€NH

## ğŸ”´ CRITICAL (Pháº£i lÃ m)
- [ ] **BÆ¯á»šC 1:** ThÃªm Unit Tests (pytest) â­â­â­â­â­
  - [ ] tests/test_chunker.py
  - [ ] tests/test_embedder.py
  - [ ] tests/test_retriever.py
  - [ ] tests/test_text_extractor.py
  - [ ] tests/test_prompt_builder.py
  - [ ] Äáº¡t coverage â‰¥70%

- [ ] **BÆ¯á»šC 2:** Retry Logic vá»›i Exponential Backoff â­â­â­â­
  - [ ] src/retry_utils.py
  - [ ] Cáº­p nháº­t llm_client.py
  - [ ] Cáº­p nháº­t supabase_client.py

- [ ] **BÆ¯á»šC 3:** Input Validation vá»›i Pydantic â­â­â­â­
  - [ ] src/validators.py
  - [ ] Cáº­p nháº­t rag_service.py

## ğŸŸ¡ HIGH PRIORITY (NÃªn lÃ m)
- [ ] **BÆ¯á»šC 4:** Performance Monitoring â­â­â­
  - [ ] src/performance_monitor.py
  - [ ] scripts/show_metrics.py
  - [ ] ThÃªm @track_performance decorators

- [ ] **BÆ¯á»šC 5:** Caching Layer â­â­â­
  - [ ] src/cache_manager.py
  - [ ] Cáº­p nháº­t embedder.py vá»›i cache
  - [ ] Cáº­p nháº­t retriever.py vá»›i query cache

- [ ] **BÆ¯á»šC 6:** Batch Processing â­â­â­
  - [ ] src/batch_processor.py
  - [ ] scripts/batch_ingest.py

## ğŸŸ¢ MEDIUM PRIORITY (Tá»‘t náº¿u cÃ³)
- [ ] **BÆ¯á»šC 7:** Advanced Text Cleaning â­â­
- [ ] **BÆ¯á»šC 8:** Semantic Chunking â­â­
- [ ] **BÆ¯á»šC 9:** Logging Configuration â­â­
- [ ] **BÆ¯á»šC 10:** CI/CD Pipeline â­â­

---

# ğŸ¯ TIMELINE Dá»° KIáº¾N

| Giai Ä‘oáº¡n | Thá»i gian | Má»¥c tiÃªu |
|-----------|-----------|----------|
| **Week 1** | 3-5 ngÃ y | BÆ¯á»šC 1-3 (Tests, Retry, Validation) |
| **Week 2** | 2-3 ngÃ y | BÆ¯á»šC 4-6 (Monitoring, Cache, Batch) |
| **Week 3** | 2-3 ngÃ y | BÆ¯á»šC 7-10 (Advanced features) |
| **Week 4** | 1-2 ngÃ y | Testing, Documentation, Polish |

**Tá»•ng thá»i gian:** 2-4 tuáº§n (tÃ¹y tá»‘c Ä‘á»™)

---

# ğŸ“ˆ EXPECTED IMPROVEMENT

| TiÃªu chÃ­ | TrÆ°á»›c | Sau | Cáº£i thiá»‡n |
|----------|-------|-----|-----------|
| Testing | 1/10 | 9/10 | +8 |
| Error Handling | 6.5/10 | 9/10 | +2.5 |
| Performance | 6/10 | 8.5/10 | +2.5 |
| Code Quality | 7/10 | 9/10 | +2 |
| **Tá»”NG** | **7.5/10** | **9.5/10** | **+2** âœ¨

---

# ğŸš€ GETTING STARTED

```bash
# 1. Clone hoáº·c checkout branch má»›i
git checkout -b feature/improvements

# 2. Báº¯t Ä‘áº§u vá»›i tests (BÆ¯á»šC 1)
cd Embedding_langchain
mkdir tests
pip install pytest pytest-cov pytest-mock

# 3. Táº¡o file tests Ä‘áº§u tiÃªn
# Copy code tá»« BÆ¯á»šC 1.3 â†’ tests/conftest.py
# Copy code tá»« BÆ¯á»šC 1.4 â†’ tests/test_chunker.py

# 4. Cháº¡y tests
pytest -v

# 5. Commit tá»«ng bÆ°á»›c
git add tests/
git commit -m "feat: add unit tests for chunker module"

# 6. Tiáº¿p tá»¥c vá»›i BÆ¯á»šC 2, 3...
```

---

# â“ FAQ

**Q: Pháº£i lÃ m táº¥t cáº£ khÃ´ng?**  
A: KhÃ´ng. Æ¯u tiÃªn PHASE 1 (BÆ¯á»šC 1-3) Ä‘á»ƒ Ä‘áº¡t 8.5/10. PHASE 2-3 lÃ  bonus.

**Q: Máº¥t bao lÃ¢u?**  
A: BÆ¯á»šC 1 (tests) máº¥t 3-5 ngÃ y. ToÃ n bá»™ roadmap 2-4 tuáº§n.

**Q: LÃ m tháº¿ nÃ o Ä‘á»ƒ kiá»ƒm tra?**  
A: Cháº¡y `pytest --cov=src` sau má»—i bÆ°á»›c. Coverage â‰¥70% lÃ  tá»‘t.

**Q: CÃ³ thá»ƒ lÃ m tá»«ng pháº§n khÃ´ng?**  
A: CÃ³! Má»—i BÆ¯á»šC Ä‘á»™c láº­p. Commit riÃªng tá»«ng feature.

---

**ğŸ“ ChÃºc cÃ¡c báº¡n thÃ nh cÃ´ng vá»›i Ä‘á»“ Ã¡n!**
