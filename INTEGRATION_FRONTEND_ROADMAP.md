# ğŸš€ ROADMAP: TÃ­ch Há»£p RAG VÃ o Frontend (mindmap-notion-interface)

## ğŸ“‹ HIá»†N TRáº NG

### âœ… Báº¡n CÃ³
1. **Frontend**: React + TypeScript + Shadcn UI (mindmap-notion-interface)
2. **Backend**: Express.js (hoáº·c Node.js)
3. **Embedding Pipeline**: Python (Embedding_langchain)
4. **Database**: Supabase (pgvector ready)

### âŒ CÃ²n Thiáº¿u
1. **Retriever API**: Backend endpoint Ä‘á»ƒ search embeddings
2. **RAG Service**: Backend service Ä‘á»ƒ call LLM
3. **Chat UI**: Frontend component cho chatbot/RAG
4. **API Integration**: Frontend gá»i backend RAG endpoints

---

## ğŸ¯ 3 GIAI ÄOáº N

### **GIAI ÄOáº N 1: Backend RAG Service (Python)**

**Má»¥c Ä‘Ã­ch**: Táº¡o Python modules Ä‘á»ƒ support RAG

**Tasks**:

```
Embedding_langchain/src/
â”œâ”€ retriever.py          ğŸ†• Search embeddings from DB
â”œâ”€ prompt_builder.py     ğŸ†• Format context + query
â”œâ”€ llm_client.py         ğŸ†• Call OpenAI/Ollama
â””â”€ rag_service.py        ğŸ†• Orchestrate RAG flow
```

**Estimated Time**: 2-3 hours

**File Details**:

```python
# src/retriever.py
def retrieve_similar_chunks(
    query: str, 
    document_id: str, 
    top_k: int = 5
) -> list[dict]:
    """TÃ¬m chunks tÆ°Æ¡ng tá»± tá»« Supabase"""
    pass

# src/prompt_builder.py
def build_rag_prompt(
    query: str, 
    context_chunks: list[dict]
) -> str:
    """XÃ¢y dá»±ng prompt cho LLM"""
    pass

# src/llm_client.py
def generate_answer(
    prompt: str,
    model: str = "gpt-3.5-turbo"
) -> str:
    """Gá»i OpenAI/Ollama Ä‘á»ƒ generate answer"""
    pass

# src/rag_service.py
def rag_query(
    query: str,
    document_id: str,
    top_k: int = 5
) -> dict:
    """End-to-end RAG pipeline"""
    # 1. retrieve_similar_chunks()
    # 2. build_rag_prompt()
    # 3. generate_answer()
    # 4. return formatted response
    pass
```

---

### **GIAI ÄOáº N 2: Backend API Endpoints (Express.js)**

**Má»¥c Ä‘Ã­ch**: Expose RAG functionality thÃ´ng qua API

**Tasks**:

```
mindmap-notion-interface/backend/src/
â”œâ”€ routes/
â”‚  â””â”€ rag.routes.ts      ğŸ†• POST /api/rag/chat
â”œâ”€ controllers/
â”‚  â””â”€ rag.controller.ts  ğŸ†• Call Python rag_service
â””â”€ services/
   â””â”€ ragService.ts      ğŸ†• Python subprocess wrapper
```

**Estimated Time**: 2-3 hours

**Endpoints**:

```typescript
// POST /api/rag/chat
interface RAGRequest {
  query: string;           // "LangChain lÃ  gÃ¬?"
  document_id: string;     // "doc123"
  top_k?: number;          // default 5
}

interface RAGResponse {
  answer: string;          // Generated answer
  sources: Array<{         // Retrieved chunks
    chunk_index: number;
    page_number: number;
    content: string;
    similarity: number;
  }>;
  metadata: {
    query_time_ms: number;
    model: string;
  };
}
```

**Implementation**:

```typescript
// rag.controller.ts
export async function chatWithRAG(req: Request, res: Response) {
  const { query, document_id, top_k } = req.body;
  
  try {
    // Call Python RAG service
    const result = await ragService.queryRAG(query, document_id, top_k);
    
    res.json({
      answer: result.answer,
      sources: result.sources,
      metadata: {
        query_time_ms: Date.now() - startTime,
        model: 'gpt-3.5-turbo'
      }
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
}

// rag.routes.ts
router.post('/chat', chatWithRAG);
```

---

### **GIAI ÄOáº N 3: Frontend Chat UI (React)**

**Má»¥c Ä‘Ã­ch**: Táº¡o chatbot interface cho RAG

**Tasks**:

```
mindmap-notion-interface/src/
â”œâ”€ components/
â”‚  â””â”€ RAGChat/
â”‚     â”œâ”€ ChatInterface.tsx          ğŸ†• Main chat UI
â”‚     â”œâ”€ ChatMessage.tsx            ğŸ†• Message bubble
â”‚     â”œâ”€ ChatInput.tsx              ğŸ†• Query input
â”‚     â”œâ”€ SourcesPanel.tsx           ğŸ†• Retrieved chunks
â”‚     â””â”€ RAGChat.tsx                ğŸ†• Parent component
â”‚
â”œâ”€ hooks/
â”‚  â””â”€ useRAGChat.ts                 ğŸ†• Query hook
â”‚
â””â”€ services/
   â””â”€ ragAPI.ts                     ğŸ†• API client
```

**Estimated Time**: 2-3 hours

**Component Structure**:

```
RAGChat (Parent)
â”œâ”€ ChatInterface (Main)
â”‚  â”œâ”€ ChatMessages
â”‚  â”‚  â”œâ”€ ChatMessage (User)
â”‚  â”‚  â”œâ”€ ChatMessage (Assistant)
â”‚  â”‚  â””â”€ ChatMessage (Loading)
â”‚  â”‚
â”‚  â””â”€ ChatInput
â”‚     â”œâ”€ TextField
â”‚     â””â”€ SendButton
â”‚
â””â”€ SourcesPanel (Sidebar)
   â”œâ”€ SourceCard
   â”‚  â”œâ”€ PageNumber
   â”‚  â”œâ”€ ChunkIndex
   â”‚  â”œâ”€ Content
   â”‚  â””â”€ SimilarityScore
   â””â”€ ShowMore
```

**UI Mockup**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAG Chat Interface                            [Ã—]  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              â”‚                                      â”‚
â”‚ Retrieved    â”‚  Chat Messages                       â”‚
â”‚ Sources      â”‚                                      â”‚
â”‚              â”‚  User: "LangChain lÃ  gÃ¬?"           â”‚
â”‚ â€¢ Chunk 3    â”‚  [â³ Loading...]                     â”‚
â”‚   LangChain  â”‚                                      â”‚
â”‚   is ...     â”‚  Assistant: "LangChain lÃ  framework" â”‚
â”‚   Score: 0.9 â”‚                                      â”‚
â”‚              â”‚                                      â”‚
â”‚ â€¢ Chunk 5    â”‚  Input: [________________] [Send]   â”‚
â”‚   ...        â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š INTEGRATION FLOW

```
Frontend (React)
    â†“ (POST /api/rag/chat)
    â”‚ { query, document_id }
    â–¼
Backend (Express)
    â†“ (spawn Python subprocess)
    â–¼
Python RAG Service
    â”œâ”€ retriever.py: Search embeddings
    â”œâ”€ prompt_builder.py: Format context
    â”œâ”€ llm_client.py: Call OpenAI/Ollama
    â””â”€ return { answer, sources }
    â–¼
Backend (Response)
    â†“ (200 OK with RAG response)
    â–¼
Frontend (Display)
    â”œâ”€ Show answer in chat
    â”œâ”€ Show sources in sidebar
    â””â”€ User can ask follow-up
```

---

## ğŸ”„ STEP-BY-STEP IMPLEMENTATION

### **STEP 1: Create Python RAG Modules (2-3 hours)**

```bash
cd Embedding_langchain

# 1. Create retriever.py
# - Embed query
# - Query Supabase pgvector
# - Return top-k chunks

# 2. Create prompt_builder.py
# - Format context chunks
# - Build system prompt + context + question

# 3. Create llm_client.py
# - Support OpenAI (or Ollama)
# - Call LLM API
# - Return generated answer

# 4. Create rag_service.py
# - Orchestrate all 3 modules above
# - Error handling
# - Return formatted response

# 5. Create requirements.txt updates
pip install openai  # if using OpenAI
# or
pip install requests  # if using Ollama
```

**Test Python RAG**:

```bash
python -c "
from src.rag_service import rag_query
result = rag_query(
    query='LangChain lÃ  gÃ¬?',
    document_id='doc123',
    top_k=5
)
print(result)
"
```

---

### **STEP 2: Create Backend Express Endpoints (2-3 hours)**

```bash
cd mindmap-notion-interface/backend

# 1. Create rag.routes.ts
# - POST /api/rag/chat

# 2. Create rag.controller.ts
# - Parse request
# - Call Python service
# - Return response

# 3. Create ragService.ts
# - Spawn Python subprocess
# - Handle stdin/stdout
# - Error handling

# 4. Update main server file
# - Import rag.routes
# - Register routes
```

**Express Handler Pattern**:

```typescript
// rag.controller.ts
export async function chatWithRAG(req: Request, res: Response) {
  const { query, document_id, top_k } = req.body;
  
  // Validate input
  if (!query || !document_id) {
    return res.status(400).json({ error: 'Missing fields' });
  }
  
  try {
    // Call Python service
    const result = await ragService.queryRAG(query, document_id, top_k);
    
    // Return response
    res.json({
      success: true,
      data: {
        answer: result.answer,
        sources: result.sources,
        metadata: {
          model: 'gpt-3.5-turbo',
          timestamp: new Date().toISOString()
        }
      }
    });
  } catch (error) {
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
}
```

**Test Backend**:

```bash
curl -X POST http://localhost:3000/api/rag/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "LangChain lÃ  gÃ¬?",
    "document_id": "doc123"
  }'
```

---

### **STEP 3: Create React Chat Component (2-3 hours)**

```bash
cd mindmap-notion-interface/src

# 1. Create components/RAGChat/ChatInterface.tsx
# - Main chat UI
# - Message list
# - Input field

# 2. Create components/RAGChat/SourcesPanel.tsx
# - Display retrieved chunks
# - Show similarity scores

# 3. Create hooks/useRAGChat.ts
# - Manage chat state
# - Handle API calls
# - Loading states

# 4. Create services/ragAPI.ts
# - Fetch wrapper
# - Type-safe requests
```

**React Component Pattern**:

```typescript
// components/RAGChat/ChatInterface.tsx
export function ChatInterface({ documentId }: { documentId: string }) {
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const [sources, setSources] = useState<Source[]>([]);
  
  const handleSendMessage = async (query: string) => {
    // Add user message
    setMessages(prev => [...prev, { role: 'user', content: query }]);
    setLoading(true);
    
    try {
      // Call backend RAG
      const response = await ragAPI.chat(query, documentId);
      
      // Add assistant message
      setMessages(prev => [...prev, { 
        role: 'assistant', 
        content: response.answer 
      }]);
      
      // Set sources
      setSources(response.sources);
    } catch (error) {
      // Show error
      setMessages(prev => [...prev, { 
        role: 'error', 
        content: 'Failed to get answer' 
      }]);
    } finally {
      setLoading(false);
      setInput('');
    }
  };
  
  return (
    <div className="flex gap-4 h-full">
      {/* Chat Area */}
      <div className="flex-1 flex flex-col">
        <div className="flex-1 overflow-y-auto space-y-4 p-4">
          {messages.map((msg, idx) => (
            <ChatMessage key={idx} message={msg} />
          ))}
          {loading && <ChatMessage role="assistant" loading />}
        </div>
        
        {/* Input Area */}
        <ChatInput 
          value={input}
          onChange={setInput}
          onSend={handleSendMessage}
          disabled={loading}
        />
      </div>
      
      {/* Sources Sidebar */}
      <SourcesPanel sources={sources} />
    </div>
  );
}

// hooks/useRAGChat.ts
export function useRAGChat(documentId: string) {
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [loading, setLoading] = useState(false);
  
  const sendMessage = async (query: string) => {
    setLoading(true);
    try {
      const response = await ragAPI.chat(query, documentId);
      return response;
    } finally {
      setLoading(false);
    }
  };
  
  return { messages, loading, sendMessage };
}

// services/ragAPI.ts
export const ragAPI = {
  async chat(query: string, documentId: string, topK = 5) {
    const response = await fetch('/api/rag/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ query, document_id: documentId, top_k: topK })
    });
    
    if (!response.ok) throw new Error('RAG chat failed');
    
    return response.json();
  }
};
```

**Test Frontend**:

```bash
npm run dev
# Navigate to RAG Chat page
# Send a query
# See answer + sources
```

---

## ğŸ“ FINAL PROJECT STRUCTURE

```
DACN_MindMapNote/
â”œâ”€ Embedding_langchain/                âœ… Embedding pipeline
â”‚  â”œâ”€ src/
â”‚  â”‚  â”œâ”€ config.py
â”‚  â”‚  â”œâ”€ text_extractor.py
â”‚  â”‚  â”œâ”€ chunker.py
â”‚  â”‚  â”œâ”€ embedder.py
â”‚  â”‚  â”œâ”€ supabase_client.py
â”‚  â”‚  â”œâ”€ pipeline.py
â”‚  â”‚  â”œâ”€ retriever.py              ğŸ†• RAG
â”‚  â”‚  â”œâ”€ prompt_builder.py          ğŸ†• RAG
â”‚  â”‚  â”œâ”€ llm_client.py              ğŸ†• RAG
â”‚  â”‚  â””â”€ rag_service.py             ğŸ†• RAG
â”‚  â”œâ”€ scripts/
â”‚  â”‚  â””â”€ ingest_document.py
â”‚  â””â”€ requirements.txt               ğŸ“ Update
â”‚
â””â”€ mindmap-notion-interface/           âœ… Frontend
   â”œâ”€ backend/
   â”‚  â”œâ”€ src/
   â”‚  â”‚  â”œâ”€ routes/
   â”‚  â”‚  â”‚  â”œâ”€ documents.routes.ts
   â”‚  â”‚  â”‚  â”œâ”€ groups.routes.ts
   â”‚  â”‚  â”‚  â””â”€ rag.routes.ts        ğŸ†• RAG
   â”‚  â”‚  â”œâ”€ controllers/
   â”‚  â”‚  â”‚  â”œâ”€ documents.controller.ts
   â”‚  â”‚  â”‚  â”œâ”€ groups.controller.ts
   â”‚  â”‚  â”‚  â””â”€ rag.controller.ts     ğŸ†• RAG
   â”‚  â”‚  â”œâ”€ services/
   â”‚  â”‚  â”‚  â”œâ”€ authService.ts
   â”‚  â”‚  â”‚  â””â”€ ragService.ts         ğŸ†• RAG
   â”‚  â”‚  â””â”€ index.ts                 ğŸ“ Update
   â”‚  â””â”€ package.json                ğŸ“ Update
   â”‚
   â””â”€ src/
      â”œâ”€ components/
      â”‚  â”œâ”€ UploadDocument.tsx
      â”‚  â”œâ”€ documents/
      â”‚  â”œâ”€ groups/
      â”‚  â”œâ”€ layout/
      â”‚  â”œâ”€ notifications/
      â”‚  â”œâ”€ ui/
      â”‚  â””â”€ RAGChat/                 ğŸ†• RAG
      â”‚     â”œâ”€ ChatInterface.tsx
      â”‚     â”œâ”€ ChatMessage.tsx
      â”‚     â”œâ”€ ChatInput.tsx
      â”‚     â”œâ”€ SourcesPanel.tsx
      â”‚     â””â”€ RAGChat.tsx
      â”‚
      â”œâ”€ hooks/
      â”‚  â”œâ”€ use-mobile.tsx
      â”‚  â”œâ”€ use-toast.ts
      â”‚  â”œâ”€ useAuth.tsx
      â”‚  â””â”€ useRAGChat.ts            ğŸ†• RAG
      â”‚
      â”œâ”€ pages/
      â”‚  â”œâ”€ Auth.tsx
      â”‚  â”œâ”€ Categories.tsx
      â”‚  â”œâ”€ Chatbot.tsx              âœ… CÃ³ rá»“i?
      â”‚  â”œâ”€ Documents.tsx
      â”‚  â”œâ”€ Groups.tsx
      â”‚  â”œâ”€ Home.tsx
      â”‚  â”œâ”€ Search.tsx
      â”‚  â”œâ”€ Settings.tsx
      â”‚  â”œâ”€ Statistics.tsx
      â”‚  â”œâ”€ NotFound.tsx
      â”‚  â””â”€ RAGPage.tsx              ğŸ†• RAG
      â”‚
      â””â”€ services/
         â”œâ”€ api.ts                    ğŸ“ Update
         â””â”€ ragAPI.ts                 ğŸ†• RAG
```

---

## ğŸ¯ TOTAL TIMELINE

| Phase | Task | Time | Status |
|-------|------|------|--------|
| 1 | Python RAG modules | 2-3h | ğŸ†• TODO |
| 2 | Backend API endpoints | 2-3h | ğŸ†• TODO |
| 3 | React Chat UI | 2-3h | ğŸ†• TODO |
| 4 | Integration testing | 1-2h | ğŸ†• TODO |
| 5 | Styling & Polish | 1h | ğŸ†• TODO |
| **TOTAL** | | **8-12h** | **~1-2 days** |

---

## âœ… CHECKLIST: IMPLEMENTATION

### **Phase 1: Python (Embedding_langchain/)**
- [ ] `src/retriever.py` - Query Supabase pgvector
- [ ] `src/prompt_builder.py` - Format context
- [ ] `src/llm_client.py` - Call OpenAI/Ollama
- [ ] `src/rag_service.py` - Orchestrate RAG
- [ ] `requirements.txt` - Add openai/requests
- [ ] Test: `python -c "from src.rag_service import rag_query; rag_query(...)"`

### **Phase 2: Backend (mindmap-notion-interface/backend/)**
- [ ] `src/routes/rag.routes.ts` - POST /api/rag/chat
- [ ] `src/controllers/rag.controller.ts` - Handle requests
- [ ] `src/services/ragService.ts` - Call Python subprocess
- [ ] `src/index.ts` - Register RAG routes
- [ ] `package.json` - Add dependencies (if any)
- [ ] Test: `curl -X POST http://localhost:3000/api/rag/chat ...`

### **Phase 3: Frontend (mindmap-notion-interface/src/)**
- [ ] `components/RAGChat/ChatInterface.tsx` - Main UI
- [ ] `components/RAGChat/ChatMessage.tsx` - Message bubbles
- [ ] `components/RAGChat/ChatInput.tsx` - Input field
- [ ] `components/RAGChat/SourcesPanel.tsx` - Sources display
- [ ] `components/RAGChat/RAGChat.tsx` - Parent component
- [ ] `hooks/useRAGChat.ts` - State management
- [ ] `services/ragAPI.ts` - API client
- [ ] `pages/RAGPage.tsx` - Page component
- [ ] Test: `npm run dev` â†’ navigate to RAG page

### **Phase 4: Integration**
- [ ] E2E testing (UI â†’ Backend â†’ Python â†’ DB)
- [ ] Error handling
- [ ] Loading states
- [ ] Response formatting

---

## ğŸ”§ TECH STACK REQUIREMENTS

**Backend**:
- Node.js + Express
- child_process (spawn Python)

**Frontend**:
- React + TypeScript
- Tanstack Query (optional, for caching)
- Shadcn UI (already have)

**Python**:
- sentence-transformers
- supabase-py
- openai (or requests for Ollama)

---

## ğŸš€ PRIORITY ORDER

### **Recommended**: 
1. âœ… **FIRST**: Implement Python RAG modules
   - Lowest risk
   - Can test independently
   - Foundation for everything else

2. âœ… **SECOND**: Implement Backend endpoints
   - Moderate risk
   - Can mock Python if needed
   - Test with Postman/curl

3. âœ… **THIRD**: Implement Frontend UI
   - Highest risk
   - But can iterate quickly
   - Most visible to user

---

## ğŸ’¡ TIPS

1. **Start with CLI Test**: Before frontend, test Python RAG locally
   ```bash
   python -c "from src.rag_service import rag_query; print(rag_query('LangChain lÃ  gÃ¬?', 'doc123'))"
   ```

2. **Use Postman**: Test backend endpoints before wiring frontend
   ```
   POST http://localhost:3000/api/rag/chat
   Body: { "query": "...", "document_id": "doc123" }
   ```

3. **Mock API First**: Build React UI with mock data, then wire real API
   ```typescript
   const mockResponse = {
     answer: "LangChain lÃ ...",
     sources: [...]
   };
   ```

4. **Deploy Python as Service**: Don't spawn subprocess for prod
   - Better: Run Python as separate service
   - Call via HTTP (FastAPI/Flask wrapper)

---

## ğŸ“š NEXT STEPS

Báº¡n muá»‘n tÃ´i giÃºp cÃ¡i gÃ¬ trÆ°á»›c?

**A) Implement Python RAG Modules**
â†’ TÃ´i táº¡o files + test scripts

**B) Implement Backend API** 
â†’ TÃ´i táº¡o Express routes + controllers

**C) Implement React Chat UI**
â†’ TÃ´i táº¡o components + hooks

**D) All 3 at once (comprehensive)**
â†’ TÃ´i táº¡o táº¥t cáº£ files + integration guide

Chá»n hÆ°á»›ng nÃ o? ğŸš€
