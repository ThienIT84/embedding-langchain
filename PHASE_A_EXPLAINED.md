# Phase A - Giáº£i ThÃ­ch Chi Tiáº¿t (Python RAG Core)

Phase A lÃ  lÃµi chÃ­nh cá»§a há»‡ thá»‘ng RAG (Retrieval-Augmented Generation). ÄÃ¢y lÃ  pháº§n xá»­ lÃ½ dá»¯ liá»‡u vÃ  tráº£ lá»i cÃ¢u há»i báº±ng LLM cÃ³ tham kháº£o tÃ i liá»‡u.

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Phase A

```
Embedding_langchain/
â”œâ”€â”€ src/                    # Code lÃµi
â”‚   â”œâ”€â”€ config.py          # Äá»c biáº¿n mÃ´i trÆ°á»ng
â”‚   â”œâ”€â”€ supabase_client.py # Káº¿t ná»‘i Supabase
â”‚   â”œâ”€â”€ text_extractor.py  # TrÃ­ch xuáº¥t text tá»« PDF
â”‚   â”œâ”€â”€ chunker.py         # Chia nhá» text thÃ nh chunks
â”‚   â”œâ”€â”€ embedder.py        # Chuyá»ƒn chunks â†’ vector (embedding)
â”‚   â”œâ”€â”€ pipeline.py        # Äiá»u phá»‘i: extract â†’ chunk â†’ embed â†’ lÆ°u DB
â”‚   â”œâ”€â”€ retriever.py       # TÃ¬m chunks liÃªn quan tá»›i cÃ¢u há»i
â”‚   â”œâ”€â”€ prompt_builder.py  # XÃ¢y dá»±ng prompt cho LLM
â”‚   â”œâ”€â”€ llm_client.py      # Gá»i Ollama Ä‘á»ƒ tráº£ lá»i
â”‚   â””â”€â”€ rag_service.py     # Äiá»u phá»‘i toÃ n bá»™: retrieval â†’ prompt â†’ LLM
â””â”€â”€ scripts/
    â”œâ”€â”€ ingest_document.py # CLI: upload PDF â†’ embedding â†’ DB
    â”œâ”€â”€ rag_query.py       # CLI: test RAG (há»i cÃ¢u há»i)
    â””â”€â”€ rag_runner.py      # Wrapper cho Node backend (nháº­n JSON stdin â†’ tráº£ JSON stdout)
```

---

## ğŸ“‹ Quy TrÃ¬nh 2 BÆ°á»›c

### **BÆ°á»›c 1: INGEST (Nháº­p tÃ i liá»‡u)**

Cháº¡y láº§n **1 láº§n duy nháº¥t** cho má»—i PDF:

```bash
python scripts/ingest_document.py "path/to/file.pdf" "TiÃªu Ä‘á» tÃ i liá»‡u"
```

**Khi cháº¡y lá»‡nh nÃ y xáº£y ra gÃ¬?**

1. **Extract** (text_extractor.py)
   - Äá»c PDF, trÃ­ch xuáº¥t text tá»« tá»«ng trang
   - Káº¿t quáº£: danh sÃ¡ch text

2. **Chunk** (chunker.py)
   - Chia text thÃ nh cÃ¡c Ä‘oáº¡n nhá» (~900 kÃ½ tá»±, overlap 200 kÃ½ tá»±)
   - LÃ½ do: model embedding cÃ³ giá»›i háº¡n Ä‘á»™ dÃ i (512 tokens)
   - Káº¿t quáº£: danh sÃ¡ch chunk

3. **Embed** (embedder.py)
   - DÃ¹ng mÃ´ hÃ¬nh `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
   - Chuyá»ƒn má»—i chunk â†’ vector 768 chiá»u (danh sÃ¡ch sá»‘ thá»±c)
   - Káº¿t quáº£: danh sÃ¡ch vector

4. **LÆ°u DB** (supabase_client.py)
   - LÆ°u vÃ o báº£ng `document_embeddings` trong Supabase
   - Cá»™t `embedding` dÃ¹ng kiá»ƒu **pgvector** (vector database)
   - Táº¡o index IVFFlat Ä‘á»ƒ tÃ¬m kiáº¿m nhanh (similarity search)

**VÃ­ dá»¥:**
```
PDF: "KNN_Algorithm.pdf"
  â†“ Extract
Text: "K-Nearest Neighbors (KNN) lÃ  má»™t thuáº­t toÃ¡n..."
  â†“ Chunk (chia 900 kÃ½ tá»±)
Chunks: [
  "K-Nearest Neighbors (KNN) lÃ  má»™t thuáº­t toÃ¡n...",
  "Æ¯u Ä‘iá»ƒm cá»§a KNN: 1. ÄÆ¡n giáº£n 2. KhÃ´ng yÃªu cáº§u training...",
  ...
]
  â†“ Embed (mÃ´ hÃ¬nh multilingual)
Vectors: [
  [0.123, -0.456, 0.789, ..., 0.234],  # chunk 0 â†’ 768 sá»‘
  [0.111, -0.222, 0.333, ..., 0.444],  # chunk 1 â†’ 768 sá»‘
  ...
]
  â†“ LÆ°u Supabase
document_embeddings table:
  id  | chunk_index | chunk_text                    | embedding (768 dims)
-----|-------------|-------------------------------|---------------------
  1  |      0      | "K-Nearest Neighbors..."     | [0.123, -0.456, ...]
  2  |      1      | "Æ¯u Ä‘iá»ƒm cá»§a KNN: ..."       | [0.111, -0.222, ...]
```

---

### **BÆ°á»›c 2: RAG QUERY (Tráº£ lá»i cÃ¢u há»i)**

DÃ¹ng khi muá»‘n há»i cÃ¢u há»i vá» tÃ i liá»‡u Ä‘Ã£ ingest:

```bash
python scripts/rag_query.py --query "NÃªu Æ°u Ä‘iá»ƒm cá»§a KNN" --document-id "01287d1b..." --top-k 5
```

**Khi cháº¡y lá»‡nh nÃ y xáº£y ra gÃ¬?**

#### **A. Retrieval (TÃ¬m kiáº¿m)**

1. Láº¥y cÃ¢u há»i: `"NÃªu Æ°u Ä‘iá»ƒm cá»§a KNN"`
2. DÃ¹ng **cÃ¹ng mÃ´ hÃ¬nh embedding** â†’ chuyá»ƒn cÃ¢u há»i thÃ nh vector 768 chiá»u
3. TÃ¬m trong database nhá»¯ng chunk cÃ³ **cosine similarity cao nháº¥t** (gáº§n nháº¥t) vá»›i vector cÃ¢u há»i
   - Cosine similarity: sá»‘ tá»« 0 Ä‘áº¿n 1 (1 = giá»‘ng há»‡t, 0 = khÃ¡c háº³n)
   - Láº¥y top-5 chunk cÃ³ similarity cao nháº¥t
4. Káº¿t quáº£: 5 chunk liÃªn quan nháº¥t kÃ¨m similarity score

VÃ­ dá»¥:
```
Query: "NÃªu Æ°u Ä‘iá»ƒm cá»§a KNN"
Query vector: [0.111, -0.222, 0.333, ..., 0.444]

So vá»›i DB:
  Chunk 0: similarity = 0.45 ("K-Nearest Neighbors...")
  Chunk 1: similarity = 0.92 âœ“ ("Æ¯u Ä‘iá»ƒm cá»§a KNN: ...")
  Chunk 2: similarity = 0.88 âœ“ ("KNN khÃ´ng yÃªu cáº§u training...")
  Chunk 3: similarity = 0.85 âœ“ ("Æ¯u Ä‘iá»ƒm 1: ÄÆ¡n giáº£n...")
  ...
â†’ Top-5: [Chunk1 (0.92), Chunk2 (0.88), Chunk3 (0.85), Chunk4 (0.83), Chunk5 (0.81)]
```

#### **B. Prompt Building (XÃ¢y dá»±ng cÃ¢u há»i cho LLM)**

GhÃ©p láº¡i:
```
System: "Báº¡n lÃ  trá»£ lÃ½ thÃ´ng minh. DÃ¹ng thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i."

Context (tá»« retrieval):
- Chunk 1 (similarity: 0.92): "Æ¯u Ä‘iá»ƒm cá»§a KNN: 1. ÄÆ¡n giáº£n 2. KhÃ´ng yÃªu cáº§u training..."
- Chunk 2 (similarity: 0.88): "KNN khÃ´ng cáº§n há»c tá»« dá»¯ liá»‡u huáº¥n luyá»‡n..."
- ... (chunk 3, 4, 5)

User query: "NÃªu Æ°u Ä‘iá»ƒm cá»§a KNN"
```

#### **C. LLM Inference (Gá»i Ollama)**

Gá»­i prompt trÃªn Ä‘áº¿n Ollama (cháº¡y locally táº¡i `http://localhost:11434`):
- Model: `llama3` (cháº¡y trÃªn mÃ¡y báº¡n, khÃ´ng cáº§n API key, hoÃ n toÃ n offline)
- Ollama Ä‘á»c context + query â†’ tráº£ lá»i

Káº¿t quáº£:
```
"Theo tÃ i liá»‡u, KNN cÃ³ cÃ¡c Æ°u Ä‘iá»ƒm sau:

1. ÄÆ¡n giáº£n: KNN lÃ  thuáº­t toÃ¡n dá»… hiá»ƒu vÃ  dá»… implement.

2. KhÃ´ng yÃªu cáº§u training: KNN lÃ  thuáº­t toÃ¡n lazy learning, 
   khÃ´ng cáº§n huáº¥n luyá»‡n model trÆ°á»›c.

3. [Æ°u Ä‘iá»ƒm khÃ¡c náº¿u cÃ³ trong context]"
```

---

## ğŸ’¾ CÆ¡ Sá»Ÿ Dá»¯ Liá»‡u (Supabase Schema)

### Báº£ng: `documents`
```sql
CREATE TABLE documents (
  id UUID PRIMARY KEY,
  filename TEXT,
  title TEXT,
  source TEXT,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);
```

### Báº£ng: `document_embeddings` (pgvector)
```sql
CREATE TABLE document_embeddings (
  id UUID PRIMARY KEY,
  document_id UUID REFERENCES documents(id),
  chunk_index INTEGER,
  chunk_text TEXT,
  embedding vector(768),    -- 768-dimensional vector
  created_at TIMESTAMP
);

-- Index Ä‘á»ƒ tÃ¬m kiáº¿m nhanh
CREATE INDEX ON document_embeddings 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);
```

**IVFFlat Index**: 
- Giáº£m Ä‘á»™ phá»©c táº¡p tá»« O(n) â†’ O(log n)
- Chia 100 cluster, tÃ¬m kiáº¿m chá»‰ trong cluster gáº§n nháº¥t

---

## ğŸ”§ Tá»«ng File Chi Tiáº¿t

### **1. config.py** - Äá»c Cáº¥u HÃ¬nh
```python
@dataclass
class Settings:
    supabase_url: str           # Supabase URL
    supabase_service_key: str   # Service key (admin key)
    hf_model_name: str          # Embedding model
    chunk_size: int = 900       # Chiá»u dÃ i chunk
    chunk_overlap: int = 200    # Overlap giá»¯a chunks
    ollama_url: str             # Ollama endpoint
    ollama_model: str           # LLM model (llama3)
```

**Tá»« Ä‘Ã¢u?** File `.env` hoáº·c biáº¿n mÃ´i trÆ°á»ng

### **2. supabase_client.py** - Káº¿t Ná»‘i DB
```python
class SupabaseClient:
    # LÆ°u embeddings vÃ o DB
    def insert_embeddings(embeddings):
        # INSERT INTO document_embeddings(document_id, chunk_index, embedding, ...)
        
    # Táº£i metadata tÃ i liá»‡u
    def fetch_document_metadata(doc_id):
        # SELECT * FROM documents WHERE id = doc_id
        
    # XÃ³a embeddings cÅ© trÆ°á»›c khi ingest láº¡i
    def delete_existing_embeddings(doc_id):
        # DELETE FROM document_embeddings WHERE document_id = doc_id
```

### **3. text_extractor.py** - TrÃ­ch Text PDF
```python
def extract_pdf_text(pdf_path):
    """
    Äá»c PDF tá»«ng trang, trÃ­ch text
    Yield: (page_num, text) tá»«ng trang
    """
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            yield (i, text)
```

### **4. chunker.py** - Chia Chunk
```python
def split_chunks(text, chunk_size=900, chunk_overlap=200):
    """
    DÃ¹ng langchain RecursiveCharacterTextSplitter
    Chia text thÃ nh chunks ~900 kÃ½ tá»±
    Chunks gá»‘i nhau 200 kÃ½ tá»± (Ä‘á»ƒ khÃ´ng máº¥t thÃ´ng tin á»Ÿ biÃªn)
    
    Return: [TextChunk(text, chunk_index)]
    """
```

**Táº¡i sao chia chunk?**
- Model embedding cÃ³ limit token (~384 tokens = ~1500 kÃ½ tá»±)
- Chunk nhá» hÆ¡n â†’ dá»… match vá»›i query
- Overlap giá»¯ thÃ´ng tin liÃªn tá»¥c

### **5. embedder.py** - Táº¡o Vector
```python
class EmbeddingResult:
    vector: list[float]  # 768 sá»‘ (multilingual model)
    model: str

def embed_chunks(chunks):
    """
    Load mÃ´ hÃ¬nh sentence-transformers
    Chuyá»ƒn chunk text â†’ vector 768 chiá»u
    Return: [EmbeddingResult(vector=[...], model="multilingual")]
    """
    model = SentenceTransformer("paraphrase-multilingual-mpnet-base-v2")
    embeddings = model.encode(chunk_texts, convert_to_numpy=True)
    # embeddings shape: (n_chunks, 768)
```

**MÃ´ hÃ¬nh Ä‘áº·c biá»‡t:**
- `paraphrase-multilingual-mpnet-base-v2`
- Hiá»ƒu 50+ ngÃ´n ngá»¯ (bao gá»“m tiáº¿ng Viá»‡t)
- Äáº§u ra: vector 768 chiá»u

### **6. retriever.py** - TÃ¬m Kiáº¿m TÆ°Æ¡ng Tá»±
```python
def retrieve_similar_chunks(query: str, document_id: str, top_k: int = 5):
    """
    1. Embed query báº±ng cÃ¹ng mÃ´ hÃ¬nh â†’ vector 768 chiá»u
    2. SELECT embeddings FROM document_embeddings WHERE document_id = doc_id
    3. TÃ­nh cosine_similarity(query_vector, má»—i chunk_vector)
    4. Sort theo similarity giáº£m dáº§n
    5. Return top-k chunks + similarity score
    
    Return: [
        RetrievedChunk(
            chunk_id="...", 
            text="...", 
            similarity=0.92,
            source={"document_id": "...", ...}
        ),
        ...
    ]
    """
```

**Cosine Similarity:**
```
similarity = dot_product(vec1, vec2) / (norm(vec1) * norm(vec2))
Range: [0, 1]
  1.0 = identical
  0.5 = partially similar
  0.0 = completely different
```

### **7. prompt_builder.py** - XÃ¢y Prompt
```python
def build_rag_prompt(query: str, context_chunks: List[RetrievedChunk], system_prompt=None):
    """
    GhÃ©p:
    1. System instruction (tiáº¿ng Viá»‡t thÃ¢n thiá»‡n)
    2. Context tá»« top-5 chunks (má»—i chunk ghi similarity)
    3. User query
    
    Return: prompt string Ä‘á»ƒ gá»­i LLM
    """
    
    prompt = f"""
    {system_prompt or "Báº¡n lÃ  trá»£ lÃ½ thÃ´ng minh..."}
    
    ThÃ´ng tin tham chiáº¿u:
    {"\n".join([
        f"- Chunk {i+1} (similarity: {c.similarity:.2%}): {c.text}"
        for i, c in enumerate(context_chunks)
    ])}
    
    CÃ¢u há»i: {query}
    """
    return prompt
```

### **8. llm_client.py** - Gá»i Ollama
```python
async def generate_answer(prompt: str, model: str = "llama3", timeout=30):
    """
    Gá»i POST http://localhost:11434/api/generate
    Body: {
        "model": "llama3",
        "prompt": prompt,
        "stream": false
    }
    
    Return: LLMResponse(answer="...", model="llama3", raw={"...": "..."})
    """
```

**Ollama:**
- Cháº¡y LLM locally (offline)
- KhÃ´ng cáº§n API key
- Model Ä‘Ã£ download trÆ°á»›c: `ollama pull llama3`

### **9. rag_service.py** - Äiá»u Phá»‘i RAG
```python
def rag_query(query: str, document_id: str, top_k: int = 5, system_prompt=None):
    """
    BÆ°á»›c 1: Retrieval
        chunks = retriever.retrieve_similar_chunks(query, document_id, top_k)
    
    BÆ°á»›c 2: Prompt Building
        prompt = prompt_builder.build_rag_prompt(query, chunks, system_prompt)
    
    BÆ°á»›c 3: LLM Inference
        answer = llm_client.generate_answer(prompt)
    
    Return: {
        "answer": "...",
        "sources": [...],
        "metadata": {"elapsed_ms": 1234, "model": "llama3"},
        "prompt": "...",
        "raw_llm_response": {...}
    }
    """
```

---

## ğŸ”„ Luá»“ng Dá»¯ Liá»‡u (Data Flow)

### **INGEST Flow**
```
PDF File
  â†“ extract_pdf_text()
Text (nhiá»u trang)
  â†“ chunker.split_chunks()
Chunks: ["K-Nearest...", "Æ¯u Ä‘iá»ƒm...", ...]
  â†“ embedder.embed_chunks()
Embeddings: [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]
  â†“ supabase_client.insert_embeddings()
Supabase DB (document_embeddings table)
```

### **QUERY Flow**
```
User Query: "NÃªu Æ°u Ä‘iá»ƒm KNN"
  â†“ embedder.embed_query()
Query Vector: [0.1, -0.2, 0.3, ...]
  â†“ retriever.retrieve_similar_chunks()
Top-5 Chunks (tá»« DB, sorted by similarity)
  â†“ prompt_builder.build_rag_prompt()
Prompt (system + context + query)
  â†“ llm_client.generate_answer()
Ollama (locally) â†’ Answer
  â†“
Return to User: {"answer": "...", "sources": [...], "metadata": {...}}
```

---

## ğŸ“ VÃ­ Dá»¥ Thá»±c Táº¿

**TÃ i liá»‡u PDF:** "Machine Learning Algorithms.pdf"

### Ingest:
```bash
python scripts/ingest_document.py "Machine Learning Algorithms.pdf" "ML Algorithms"
```

**Káº¿t quáº£:** 
- TrÃ­ch Ä‘Æ°á»£c 50 trang, chia thÃ nh 200 chunks
- Táº¡o 200 vector 768 chiá»u
- LÆ°u vÃ o Supabase

### Query:
```bash
python scripts/rag_query.py \
  --query "HÃ£y giáº£i thÃ­ch cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a Support Vector Machine (SVM)" \
  --document-id "01287d1b-ca04-4c8e-9ec7-5126a606cc37" \
  --top-k 5
```

**Káº¿t quáº£:**
1. Embed query "HÃ£y giáº£i thÃ­ch..." â†’ vector
2. TÃ¬m 5 chunk gáº§n nháº¥t:
   - Chunk 45 (sim: 0.95) "SVM lÃ  má»™t thuáº­t toÃ¡n phÃ¢n loáº¡i..."
   - Chunk 47 (sim: 0.92) "NguyÃªn táº¯c SVM: maximize margin..."
   - Chunk 50 (sim: 0.89) "VÃ­ dá»¥ SVM trong binary classification..."
   - ...
3. GhÃ©p prompt gá»­i Ollama
4. Llama3 tráº£ lá»i dá»±a vÃ o 5 chunk:
   ```
   "Support Vector Machine (SVM) lÃ  má»™t thuáº­t toÃ¡n phÃ¢n loáº¡i...
   
   CÃ¡ch hoáº¡t Ä‘á»™ng:
   1. TÃ¬m hyperplane tá»‘i Æ°u Ä‘á»ƒ phÃ¢n tÃ¡ch hai lá»›p
   2. Maximize margin (khoáº£ng cÃ¡ch tá»« hyperplane Ä‘áº¿n cÃ¡c Ä‘iá»ƒm gáº§n nháº¥t)
   3. Sá»­ dá»¥ng kernel trick Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u phi tuyáº¿n
   ..."
   ```

---

## ğŸ¯ TÃ³m Táº¯t Phase A

| BÆ°á»›c | LÃ m gÃ¬ | Tool/Library |
|------|--------|--------------|
| **Extract** | Äá»c PDF | pdfplumber |
| **Chunk** | Chia text | langchain RecursiveCharacterTextSplitter |
| **Embed** | Text â†’ vector | sentence-transformers (multilingual) |
| **Store** | LÆ°u DB | Supabase pgvector |
| **Retrieve** | TÃ¬m chunks gáº§n nháº¥t | Cosine similarity |
| **Prompt** | GhÃ©p context + query | String formatting |
| **Generate** | Gá»i LLM | Ollama (llama3) |

**Äá»™c láº­p & Offline:** Phase A cháº¡y hoÃ n toÃ n trÃªn mÃ¡y báº¡n, khÃ´ng cáº§n API bÃªn ngoÃ i.

---

## âœ… Phase A Ä‘Ã£ hoÃ n thÃ nh:
- âœ“ LÃµi RAG (retrieval + generation)
- âœ“ Database + Indexing
- âœ“ CLI scripts Ä‘á»ƒ test
- âœ“ TÃ­ch há»£p sáºµn cho Phase B (backend API)

**Phase B** sáº¯p tá»›i: GÃ³i Phase A thÃ nh API qua Node backend, Ä‘á»ƒ frontend gá»i.

