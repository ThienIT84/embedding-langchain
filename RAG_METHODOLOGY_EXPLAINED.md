# RAG Methodology - HÆ°á»›ng Dáº«n Äá»c Code & Luá»“ng Xá»­ LÃ½

## ğŸ“– Thá»© Tá»± Äá»c Code (Tá»« Dá»… â†’ KhÃ³)

Äá»ƒ hiá»ƒu toÃ n bá»™ phÆ°Æ¡ng thá»©c RAG, báº¡n nÃªn Ä‘á»c theo thá»© tá»± sau:

1. **`src/config.py`** â€” 2â€“3 phÃºt (Ä‘Ã£ biáº¿t rá»“i)
   - Láº¥y cáº¥u hÃ¬nh SUPABASE_URL, OLLAMA_URL, OLLAMA_MODEL, v.v.
   - Äiá»ƒm máº¥u chá»‘t: `settings` lÃ  singleton, load tá»« `.env`.

2. **`src/retriever.py`** â€” 10 phÃºt
   - Láº¥y context tá»« Supabase dá»±a trÃªn cÃ¢u há»i.
   - ÄÃ¢y lÃ  bÆ°á»›c "R" (Retrieval) trong RAG.

3. **`src/prompt_builder.py`** â€” 5 phÃºt
   - GhÃ©p context + cÃ¢u há»i thÃ nh prompt.
   - Äiá»ƒm quan trá»ng: format sao cho LLM dá»… Ä‘á»c vÃ  hiá»ƒu Ä‘Æ°á»£c giá»›i háº¡n (chá»‰ dá»±a trÃªn context).

4. **`src/llm_client.py`** â€” 5 phÃºt
   - Gá»i Ollama HTTP API.
   - Tráº£ vá» cÃ¢u tráº£ lá»i tá»« LLM.
   - Xá»­ lÃ½ lá»—i khi Ollama khÃ´ng cháº¡y.

5. **`src/rag_service.py`** â€” 5 phÃºt
   - **Orchestrator**: ná»‘i 3 bÆ°á»›c trÃªn thÃ nh má»™t hÃ m chÃ­nh.
   - ÄÃ¢y lÃ  giao diá»‡n duy nháº¥t backend/frontend cáº§n gá»i.

6. **`scripts/rag_query.py`** â€” 3 phÃºt
   - CLI wrapper, test tool.

---

## ğŸ¯ Tá»•ng Quan PhÆ°Æ¡ng Thá»©c RAG

### KhÃ¡i Niá»‡m
**RAG = Retrieval-Augmented Generation**
- **Retrieval**: TÃ¬m kiáº¿m (láº¥y tÃ i liá»‡u/chunk gáº§n nháº¥t).
- **Augmented**: TÄƒng cÆ°á»ng dá»¯ liá»‡u (Ä‘áº©y context vÃ o prompt).
- **Generation**: Sinh (LLM sinh cÃ¢u tráº£ lá»i dá»±a trÃªn context).

### Váº¥n Äá» mÃ  RAG giáº£i quyáº¿t
- **LLM thuáº§n** (vÃ­ dá»¥ llama3 local) cÃ³ kiáº¿n thá»©c cáº¯t tá»« lÃºc train â†’ khÃ´ng biáº¿t tÃ i liá»‡u má»›i.
- **RAG**: Äáº©y tÃ i liá»‡u cá»§a báº¡n vÃ o prompt â†’ LLM sinh cÃ¢u tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u thá»±c.

### Quy TrÃ¬nh Tá»•ng QuÃ¡t (Level 0)

```
NgÆ°á»i dÃ¹ng há»i: "Introduce about the article"
        â†“
[Retriever] TÃ¬m top-k chunks gáº§n nháº¥t
        â†“
[Prompt Builder] GhÃ©p chunks + cÃ¢u há»i thÃ nh prompt
        â†“
[LLM Client] Gá»­i prompt lÃªn Ollama llama3
        â†“
[Ollama] Sinh cÃ¢u tráº£ lá»i
        â†“
Tráº£ vá»: { answer, sources, metadata }
```

---

## ğŸ” Chi Tiáº¿t Tá»«ng BÆ°á»›c

### BÆ¯á»šC 1: Retrieval (`src/retriever.py`)

**Má»¥c tiÃªu**
- TÃ¬m nhá»¯ng Ä‘oáº¡n vÄƒn báº£n (chunks) trong Supabase cÃ³ **Ã½ nghÄ©a tÆ°Æ¡ng tá»±** nháº¥t vá»›i cÃ¢u há»i.
- VÃ­ dá»¥: cÃ¢u há»i "Ollama lÃ  gÃ¬?" â†’ tÃ¬m chunks nÃ³i vá» Ollama.

**Há»£p Äá»“ng HÃ m ChÃ­nh**

```python
def retrieve_similar_chunks(
    query: str,                      # CÃ¢u há»i ngÆ°á»i dÃ¹ng
    document_id: str,                # ID tÃ i liá»‡u trong Supabase
    top_k: int = 5                   # Sá»‘ chunks muá»‘n láº¥y
) -> List[RetrievedChunk]:           # Danh sÃ¡ch chunks sáº¯p xáº¿p theo Ä‘á»™ tÆ°Æ¡ng tá»±
    """
    Input:
      - query: "Introduce about the article"
      - document_id: "01287d1b-ca04-4c8e-9ec7-5126a606cc37"
      - top_k: 5
    
    Output:
      [
        RetrievedChunk(
          content="Article lÃ  má»™t bÃ i viáº¿t...",
          chunk_index=3,
          page_number=1,
          similarity=0.87  # CÃ ng cao (â†’1) cÃ ng giá»‘ng
        ),
        RetrievedChunk(
          content="BÃ i viáº¿t nÃ y nÃ³i vá»...",
          chunk_index=5,
          page_number=2,
          similarity=0.82
        ),
        ...
      ]
    
    Raise:
      - ValueError: náº¿u query hoáº·c document_id rá»—ng
      - Náº¿u khÃ´ng cÃ³ chunk nÃ o â†’ tráº£ vá» []
    """
```

**Luá»“ng Xá»­ LÃ½ BÃªn Trong**

```
1. Query embedding
   - DÃ¹ng SentenceTransformer (cÃ¹ng model lÃºc ingest)
   - Chuyá»ƒn "Introduce about the article" â†’ vector 768 chiá»u
   
2. Fetch chunks tá»« Supabase
   SELECT content, chunk_index, page_number, embedding
   FROM document_embeddings
   WHERE document_id = ?
   
3. TÃ­nh similarity
   FOR each chunk in chunks:
     cos_score = cosine_similarity(query_vector, chunk_embedding)
     # Formula: (aÂ·b) / (||a|| Ã— ||b||)
   
4. Sort & slice
   sorted_by_score DESC â†’ láº¥y top_k chunks Ä‘áº§u
```

**VÃ­ Dá»¥ Cosine Similarity**

Náº¿u:
- Query vector: [0.1, 0.2, 0.3, ..., 0.4]  (768 sá»‘)
- Chunk 1 vector: [0.15, 0.18, 0.35, ..., 0.42]  â†’ similarity = 0.95 (ráº¥t giá»‘ng)
- Chunk 2 vector: [0.8, 0.9, -0.5, ..., 0.1]  â†’ similarity = 0.45 (khÃ´ng giá»‘ng)

â†’ Chunk 1 Ä‘Æ°á»£c chá»n trÆ°á»›c.

**Xá»­ LÃ½ Lá»—i ThÆ°á»ng Gáº·p**

| Lá»—i | NguyÃªn nhÃ¢n | Xá»­ lÃ½ |
|-----|-------------|-------|
| `ValueError: Query khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng` | query = "" | Check empty string |
| `ValueError: document_id khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng` | document_id = "" | Check empty string |
| Embedding lÃ  chuá»—i JSON | Supabase tráº£ `embedding` dáº¡ng `"[0.1, 0.2, ...]"` | Parse JSON + convert to numpy array |
| Vector rá»—ng/Ä‘á»™ dÃ i 0 | Dá»¯ liá»‡u há»ng | Skip vector Ä‘Ã³ |
| KhÃ´ng káº¿t ná»‘i Supabase | Thiáº¿u .env | Xem lá»—i tá»« supabase_client |

---

### BÆ¯á»šC 2: Prompt Building (`src/prompt_builder.py`)

**Má»¥c tiÃªu**
- Táº¡o má»™t prompt rÃµ rÃ ng cho LLM.
- Prompt = (system instruction) + (context) + (cÃ¢u há»i) + (yÃªu cáº§u).

**Há»£p Äá»“ng HÃ m ChÃ­nh**

```python
def build_rag_prompt(
    query: str,                           # CÃ¢u há»i
    chunks: Sequence[RetrievedChunk],     # Danh sÃ¡ch chunks tá»« retriever
    system_prompt: str | None = None      # Tuá»³ chá»n: hÆ°á»›ng dáº«n há»‡ thá»‘ng
) -> str:                                 # Prompt hoÃ n chá»‰nh
    """
    Input:
      - query: "Introduce about the article"
      - chunks: [
          RetrievedChunk(content="Article lÃ ...", chunk_index=3, page_number=1, similarity=0.87),
          RetrievedChunk(content="BÃ i viáº¿t nÃ y...", chunk_index=5, page_number=2, similarity=0.82),
          ...
        ]
      - system_prompt: None (dÃ¹ng máº·c Ä‘á»‹nh)
    
    Output (chuá»—i):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Báº¡n lÃ  trá»£ lÃ½ AI há»— trá»£ tráº£ lá»i cÃ¢u há»i â”‚
    â”‚ dá»±a trÃªn cÃ¡c Ä‘oáº¡n vÄƒn báº£n cung cáº¥p...    â”‚
    â”‚                                         â”‚
    â”‚ Context:                                â”‚
    â”‚ Äoáº¡n 1 | Trang 1 | Score: 0.8700       â”‚
    â”‚ Article lÃ  má»™t bÃ i viáº¿t...             â”‚
    â”‚                                         â”‚
    â”‚ Äoáº¡n 2 | Trang 2 | Score: 0.8200       â”‚
    â”‚ BÃ i viáº¿t nÃ y nÃ³i vá»...                 â”‚
    â”‚                                         â”‚
    â”‚ CÃ¢u há»i: Introduce about the article   â”‚
    â”‚ HÃ£y cung cáº¥p cÃ¢u tráº£ lá»i ngáº¯n gá»n...   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Raise:
      - ValueError: náº¿u query rá»—ng
    """
```

**Cáº¥u TrÃºc Prompt Chi Tiáº¿t**

```
<SYSTEM PROMPT>
Báº¡n lÃ  trá»£ lÃ½ AI há»— trá»£ tráº£ lá»i cÃ¢u há»i dá»±a trÃªn cÃ¡c Ä‘oáº¡n vÄƒn báº£n cung cáº¥p.
Chá»‰ sá»­ dá»¥ng thÃ´ng tin trong pháº§n Context.
Náº¿u Context khÃ´ng Ä‘á»§, hÃ£y nÃ³i rÃµ báº¡n khÃ´ng cháº¯c cháº¯n.
Tráº£ lá»i ngáº¯n gá»n báº±ng tiáº¿ng Viá»‡t, Æ°u tiÃªn liá»‡t kÃª bullet khi phÃ¹ há»£p.

<CONTEXT>
Äoáº¡n 1 | Trang 1 | Score: 0.8700
<ná»™i dung chunk 1>

Äoáº¡n 2 | Trang 2 | Score: 0.8200
<ná»™i dung chunk 2>

(... thÃªm cÃ¡c Ä‘oáº¡n khÃ¡c ...)

<QUESTION>
CÃ¢u há»i: Introduce about the article
HÃ£y cung cáº¥p cÃ¢u tráº£ lá»i ngáº¯n gá»n vÃ  chá»‰ dá»±a trÃªn context á»Ÿ trÃªn.
```

**Táº¡i Sao Cáº§n System Prompt?**
- Giá»›i háº¡n LLM: "Chá»‰ tráº£ lá»i dá»±a trÃªn context" â†’ trÃ¡nh LLM dá»±a vÃ o kiáº¿n thá»©c huáº¥n luyá»‡n (cÃ³ thá»ƒ sai).
- HÆ°á»›ng dáº«n format: "Tráº£ lá»i ngáº¯n gá»n, liá»‡t kÃª bullet" â†’ output dá»… Ä‘á»c.
- XÃ¡c Ä‘á»‹nh ngÃ´n ngá»¯: "báº±ng tiáº¿ng Viá»‡t" â†’ output Ä‘Ãºng ngÃ´n ngá»¯.

---

### BÆ¯á»šC 3: LLM Call (`src/llm_client.py`)

**Má»¥c TiÃªu**
- Gá»­i prompt tá»›i Ollama (llama3 local).
- Nháº­n cÃ¢u tráº£ lá»i Ä‘Ã£ sinh.

**Há»£p Äá»“ng HÃ m ChÃ­nh**

```python
def generate_answer(
    prompt: str,                    # Prompt Ä‘áº§y Ä‘á»§ tá»« prompt_builder
    model: str | None = None,       # Model name (máº·c Ä‘á»‹nh: settings.ollama_model)
    timeout: int = 120              # Timeout (giÃ¢y)
) -> LLMResponse:                   # Káº¿t quáº£ tá»« LLM
    """
    Input:
      - prompt: "<SYSTEM>....<CONTEXT>....<QUESTION>..."
      - model: None â†’ dÃ¹ng "llama3"
      - timeout: 120
    
    Output:
      LLMResponse(
        answer="Article lÃ  má»™t loáº¡i tÃ i liá»‡u...",
        model="llama3",
        raw={...} # JSON gá»‘c tá»« Ollama
      )
    
    Raise:
      - ValueError: prompt rá»—ng
      - LLMClientError: KhÃ´ng káº¿t ná»‘i Ollama
      - LLMClientError: Response khÃ´ng há»£p lá»‡
    """
```

**HTTP Request Chi Tiáº¿t**

```
POST http://localhost:11434/api/generate

Body:
{
  "model": "llama3",
  "prompt": "<SYSTEM>....<CONTEXT>....",
  "stream": false
}

Response (200):
{
  "response": "Article lÃ  má»™t bÃ i viáº¿t chá»©a thÃ´ng tin...",
  "model": "llama3",
  "created_at": "2024-10-29T10:30:00Z",
  "done": true,
  "context": [123, 456, 789, ...],  # Token IDs
  "total_duration": 4669359048600,  # ns
  "load_duration": 832090710,
  ...
}
```

**Xá»­ LÃ½ Lá»—i**

| Lá»—i | NguyÃªn nhÃ¢n | Fix |
|-----|-------------|-----|
| `Connection refused` | Ollama chÆ°a cháº¡y | `ollama serve` |
| `500 Internal Server Error` | Ollama crash hoáº·c model lá»—i | Check logs, reload model |
| Timeout (>120s) | Prompt quÃ¡ dÃ i hoáº·c mÃ¡y cháº­m | Giáº£m `top_k`, dÃ¹ng model nháº¹ |
| Missing "response" field | Response JSON sai Ä‘á»‹nh dáº¡ng | Kiá»ƒm tra version Ollama |

---

### BÆ¯á»šC 4: Orchestration (`src/rag_service.py`)

**Má»¥c TiÃªu**
- Ná»‘i 3 bÆ°á»›c trÃªn thÃ nh 1 hÃ m duy nháº¥t.
- ÄÃ¢y lÃ  **giao diá»‡n cÃ´ng cá»™ng** (public API) cho backend/frontend gá»i.

**Há»£p Äá»“ng HÃ m ChÃ­nh**

```python
def rag_query(
    query: str,
    document_id: str,
    top_k: int = 5,
    system_prompt: str | None = None
) -> Dict[str, Any]:
    """
    Input:
      - query: "Introduce about the article"
      - document_id: "01287d1b-ca04-4c8e-9ec7-5126a606cc37"
      - top_k: 5
      - system_prompt: None
    
    Output:
      {
        "answer": "Article lÃ  má»™t bÃ i viáº¿t chá»©a...",
        "sources": [
          {
            "content": "Article lÃ ...",
            "chunk_index": 3,
            "page_number": 1,
            "similarity": 0.87
          },
          {
            "content": "BÃ i viáº¿t nÃ y...",
            "chunk_index": 5,
            "page_number": 2,
            "similarity": 0.82
          },
          ...
        ],
        "metadata": {
          "model": "llama3",
          "query_time_ms": 12345.67,
          "chunk_count": 5
        },
        "prompt": "<SYSTEM>....<CONTEXT>....<QUESTION>...",  # Debug
        "raw_llm_response": {...}  # Debug
      }
    
    Raise:
      - Báº¥t ká»³ lá»—i tá»« retriever, prompt_builder, llm_client
      - (KhÃ´ng nuá»‘t lá»—i, Ä‘á»ƒ caller xá»­ lÃ½)
    """
```

**Luá»“ng Tá»•ng Há»£p**

```
START
  â†“
1. retrieve_similar_chunks(query, document_id, top_k)
   â†’ chunks: List[RetrievedChunk]
  â†“
2. build_rag_prompt(query, chunks, system_prompt)
   â†’ prompt: str
  â†“
3. generate_answer(prompt)
   â†’ llm_response: LLMResponse
  â†“
4. Format output
   - answer: llm_response.answer
   - sources: convert chunks â†’ dict list
   - metadata: {model, query_time_ms, chunk_count}
   - prompt: Ä‘á»ƒ debug
  â†“
5. Return dict
END
```

**TÃ­nh ToÃ¡n Thá»i Gian**

```python
from time import perf_counter

start = perf_counter()
# ... táº¥t cáº£ 3 hÃ m gá»i ...
elapsed_ms = (perf_counter() - start) * 1000
# elapsed_ms = ~12500 ms = 12.5 giÃ¢y (tuá»³ Ä‘á»™ phá»©c táº¡p)
```

---

## ğŸ”„ Luá»“ng End-to-End (Level 1: Tá»•ng QuÃ¡t)

```
USER INPUT
"Introduce about the article"
        â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   rag_service.rag_query()             â•‘
â•‘   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•             â•‘
â•‘  1. retriever.retrieve_similar_chunks â•‘
â•‘  2. prompt_builder.build_rag_prompt   â•‘
â•‘  3. llm_client.generate_answer        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        â†“
RESPONSE {
  answer: "...",
  sources: [...],
  metadata: {...}
}
```

---

## ğŸ”„ Luá»“ng End-to-End (Level 2: Chi Tiáº¿t)

```
1. RETRIEVAL
   query_vector = embed("Introduce about the article")
   chunks = Supabase.select("document_embeddings")
   scores = [cosine_sim(query_vector, chunk.embedding) for chunk in chunks]
   top_5_chunks = sort(chunks by scores)[0:5]
   
2. PROMPT BUILDING
   prompt = f"""
   {SYSTEM_PROMPT}
   
   Context:
   {Äoáº¡n 1 | Trang 1 | Score: 0.87}
   {chunk_1_content}
   ...
   
   Question: Introduce about the article
   """
   
3. LLM CALL
   response = ollama.post("/api/generate", {
     model: "llama3",
     prompt: prompt,
     stream: false
   })
   answer = response["response"]
   
4. FORMAT OUTPUT
   return {
     answer: answer,
     sources: [
       {content, chunk_index, page_number, similarity},
       ...
     ],
     metadata: {model, query_time_ms, chunk_count}
   }
```

---

## ğŸ“Š Data Flow Diagram (Level 3: Transformer)

```
Input Query String
"Introduce about the article"
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SentenceTransformer     â”‚
â”‚ encode()                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Query Vector (768-dim)
[0.1, 0.2, 0.3, ..., 0.4]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Supabase pgvector       â”‚
â”‚ document_embeddings     â”‚
â”‚ (fetch by document_id)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
All Chunk Vectors
[
  {content: "...", embedding: [...]},
  {content: "...", embedding: [...]},
  ...
]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cosine Similarity       â”‚
â”‚ Compute & Sort          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Top-K Chunks (Ranked by Score)
[
  RetrievedChunk(content="...", similarity=0.87),
  RetrievedChunk(content="...", similarity=0.82),
  ...
]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Builder          â”‚
â”‚ Format + Concatenate    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Prompt String (1000+ chars)
"<SYSTEM>...<CONTEXT>...<QUESTION>..."
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama HTTP POST        â”‚
â”‚ /api/generate           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
LLM Response JSON
{
  response: "Article lÃ  má»™t bÃ i viáº¿t...",
  model: "llama3",
  ...
}
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Format Output           â”‚
â”‚ Extract + Structure     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Final Dict Output
{
  answer: "Article lÃ ...",
  sources: [...],
  metadata: {...}
}
```

---

## ğŸ—ï¸ Kiáº¿n TrÃºc File (Level 4: Components)

```
config.py
â”œâ”€ Settings dataclass
â”‚  â”œâ”€ supabase_url, supabase_service_key
â”‚  â”œâ”€ ollama_url, ollama_model
â”‚  â””â”€ chunk_size, chunk_overlap, hf_model_name
â””â”€ settings: Settings (singleton)

supabase_client.py
â”œâ”€ get_supabase_client() â†’ Client
â”œâ”€ fetch_document_metadata()
â”œâ”€ download_file()
â””â”€ insert_embeddings()

embedder.py
â”œâ”€ _get_model() â†’ SentenceTransformer (singleton)
â””â”€ embed_chunks() â†’ List[EmbeddingResult]

retriever.py
â”œâ”€ retrieve_similar_chunks()
â”‚  â”œâ”€ Embed query (SentenceTransformer)
â”‚  â”œâ”€ Fetch chunks from Supabase
â”‚  â”œâ”€ Compute cosine similarity
â”‚  â””â”€ Return top_k
â”œâ”€ RetrievedChunk dataclass
â”‚  â”œâ”€ content: str
â”‚  â”œâ”€ chunk_index: int
â”‚  â”œâ”€ page_number: int | None
â”‚  â””â”€ similarity: float
â””â”€ _cosine_similarity() helper

prompt_builder.py
â”œâ”€ build_rag_prompt()
â”‚  â”œâ”€ Format system prompt
â”‚  â”œâ”€ Format context (chunks)
â”‚  â””â”€ Append question
â””â”€ _DEFAULT_SYSTEM_PROMPT constant

llm_client.py
â”œâ”€ generate_answer()
â”‚  â”œâ”€ POST to Ollama /api/generate
â”‚  â”œâ”€ Handle errors
â”‚  â””â”€ Parse response
â”œâ”€ LLMResponse dataclass
â”‚  â”œâ”€ answer: str
â”‚  â”œâ”€ model: str
â”‚  â””â”€ raw: dict
â””â”€ LLMClientError exception

rag_service.py
â”œâ”€ rag_query() â† **MAIN PUBLIC API**
â”‚  â”œâ”€ Call retriever
â”‚  â”œâ”€ Call prompt_builder
â”‚  â”œâ”€ Call llm_client
â”‚  â”œâ”€ Measure time
â”‚  â””â”€ Format & return
â””â”€ _serialize_chunk() helper

scripts/rag_query.py
â”œâ”€ parse_args()
â”‚  â”œâ”€ --query (required)
â”‚  â”œâ”€ --document-id (required)
â”‚  â”œâ”€ --top-k (default 5)
â”‚  â”œâ”€ --show-prompt
â”‚  â””â”€ --pretty
â””â”€ main()
   â”œâ”€ Call rag_service.rag_query()
   â””â”€ Pretty print result
```

---

## ğŸ¯ Báº£ng TÃ³m Táº¯t: Há»£p Äá»“ng I/O

| Module | Input | Output | Lá»—i |
|--------|-------|--------|-----|
| **retriever.py** | query: str, document_id: str, top_k: int | List[RetrievedChunk] | ValueError (empty), None (no data) |
| **prompt_builder.py** | query: str, chunks: Sequence[RetrievedChunk], system_prompt?: str | str (prompt) | ValueError (empty query) |
| **llm_client.py** | prompt: str, model?: str, timeout: int | LLMResponse | LLMClientError (connection, response) |
| **rag_service.py** | query: str, document_id: str, top_k: int, system_prompt?: str | Dict (answer, sources, metadata) | Any error from 3 modules |

---

## ğŸ’¡ VÃ­ Dá»¥ Cháº¡y Thá»­

### Scenario: Há»i vá» má»™t tÃ i liá»‡u
```bash
python scripts/rag_query.py \
  --query "Introduce about the article" \
  --document-id 01287d1b-ca04-4c8e-9ec7-5126a606cc37 \
  --top-k 5 \
  --show-prompt
```

### Output mong Ä‘á»£i
```
=== ANSWER ===
Article lÃ  má»™t bÃ i viáº¿t chá»©a thÃ´ng tin vá»...

=== PROMPT Gá»¬I LÃŠN LLM ===
Báº¡n lÃ  trá»£ lÃ½ AI há»— trá»£ tráº£ lá»i cÃ¢u há»i dá»±a trÃªn cÃ¡c Ä‘oáº¡n vÄƒn báº£n cung cáº¥p...
[... prompt dÃ i ...]

=== CONTEXT Sá»¬ Dá»¤NG ===
[1] Chunk 3 | Trang 1 | Score 0.8700
Article lÃ  má»™t dáº¡ng tÃ i liá»‡u...
-
[2] Chunk 5 | Trang 2 | Score 0.8200
BÃ i viáº¿t nÃ y nÃ³i vá»...
-
...

=== METADATA ===
model: llama3
query_time_ms: 12345.67
chunk_count: 5
```

---

## ğŸ” Gá»¡ Lá»—i Nhanh

| Váº¥n Äá» | NguyÃªn nhÃ¢n | CÃ¡ch Fix |
|--------|-------------|---------|
| `ModuleNotFoundError: No module named 'src'` | Script cháº¡y tá»« sai thÆ° má»¥c | ÄÃ£ sá»­a á»Ÿ rag_query.py: thÃªm sys.path |
| `No similar chunks found` (sources = []) | Document khÃ´ng cÃ³ embeddings | Kiá»ƒm tra document Ä‘Ã£ ingest chÆ°a |
| LLM tráº£ lá»i khÃ´ng liÃªn quan | top_k quÃ¡ tháº¥p hoáº·c prompt khÃ´ng rÃµ | TÄƒng top_k hoáº·c sá»­a system_prompt |
| Ollama timeout | Server cháº­m hoáº·c model náº·ng | DÃ¹ng model nháº¹ hÆ¡n (mistral, phi) |
| Embedding mismatch | Model embedding khÃ¡c lÃºc ingest | Báº£o Ä‘áº£m config HF_MODEL_NAME giá»‘ng |

---

## ğŸ“ Key Takeaways

1. **RAG = 3 bÆ°á»›c tuáº§n tá»±**: Retrieval â†’ Prompt Building â†’ LLM Generation.
2. **Retriever dÃ¹ng embedding similarity** (cosine): vector space matching.
3. **Prompt Builder Ä‘á»‹nh hÃ¬nh output**: system instruction giá»›i háº¡n LLM, context cung cáº¥p kiáº¿n thá»©c.
4. **LLM Client lÃ  wrapper HTTP**: gá»i Ollama local `/api/generate`.
5. **RAG Service lÃ  cÃ´ng cá»™ng API**: backend/frontend chá»‰ cáº§n gá»i hÃ m nÃ y.
6. **Thá»i gian xá»­ lÃ½**: pháº§n lá»›n á»Ÿ step 1 (embedding) + step 3 (LLM generation).

---

## ğŸ“š Äá»c ThÃªm
- Cosine Similarity: https://en.wikipedia.org/wiki/Cosine_similarity
- Ollama API: https://github.com/ollama/ollama/blob/main/docs/api.md
- Sentence Transformers: https://www.sbert.net/

