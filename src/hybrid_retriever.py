from __future__ import annotations

import logging
import os
import re
from dataclasses import dataclass
from typing import List, Optional, Dict, Any, Literal
from concurrent.futures import ThreadPoolExecutor

from langchain_community.retrievers import TavilySearchAPIRetriever
from .retriever import RetrievedChunk, retrieve_similar_chunks_by_user

# Thi·∫øt l·∫≠p logging
logger = logging.getLogger(__name__)

@dataclass
class HybridRetrievalResult:
    """K·∫øt qu·∫£ retrieval t·ª´ nhi·ªÅu ngu·ªìn."""
    sources: List[RetrievedChunk]
    metadata: Dict[str, Any]

WebSearchMode = Literal["auto", "force-on", "force-off"]

class HybridRetriever:
    """
    Hybrid Retriever k·∫øt h·ª£p:
    1. Internal Knowledge Base (Supabase vector search)
    2. External Web Search (Tavily AI)
    
    NEW: Smart web search mode v·ªõi auto-detection
    """
    
    # Keywords indicating document-specific questions
    DOC_SPECIFIC_KEYWORDS = [
    # --- Ti·∫øng Vi·ªát ---
    r"b√†i (b√°o|vi·∫øt|nghi√™n c·ª©u|survey)",
    r"theo (b√†i|b√†i b√°o|t√†i li·ªáu)",
    r"trong (b√†i|b√†i b√°o|t√†i li·ªáu)",
    r"t√°c gi·∫£ (cho r·∫±ng|n√™u|tr√¨nh b√†y|ƒë·ªÅ xu·∫•t)",
    r"m·ª•c \d+",
    r"h√¨nh \d+",
    r"b·∫£ng \d+",
    r"ph·∫ßn \d+",
    r"ch∆∞∆°ng \d+",

    # --- Ti·∫øng Anh ---
    r"this (paper|document|article|study|research|survey)",
    r"the (paper|document|article|study|survey)",
    r"according to (this|the)",
    r"author(s)? (state|claim|argue|propose|describe)",
    r"in (section|chapter|page)",
    r"figure \d+",
    r"table \d+",
]

    
    def __init__(self):
        self.tavily_api_key = os.getenv("TAVILY_API_KEY")
        if not self.tavily_api_key:
            logger.warning("‚ö†Ô∏è TAVILY_API_KEY not found. Web search will be disabled.")
            self.tavily_retriever = None
        else:
            self.tavily_retriever = TavilySearchAPIRetriever(
                k=3,  # Default web results
                api_key=self.tavily_api_key,
                search_depth="advanced"
            )

    def _should_enable_web_search(
        self,
        mode: WebSearchMode,
        query: str,
        document_id: Optional[str]
    ) -> bool:
        """
        Smart resolution: Quy·∫øt ƒë·ªãnh c√≥ n√™n b·∫≠t t√¨m ki·∫øm web kh√¥ng.
        
        Logic:
        - force-on: Lu√¥n b·∫≠t t√¨m ki·∫øm web
        - force-off: Lu√¥n t·∫Øt t√¨m ki·∫øm web
        - auto: Ph√°t hi·ªán th√¥ng minh d·ª±a tr√™n document_id v√† t·ª´ kh√≥a trong query
        """
        if mode == "force-on":
            logger.info("üåê T√¨m ki·∫øm web: B·∫¨T (ng∆∞·ªùi d√πng ch·ªçn)")
            return True
        
        if mode == "force-off":
            logger.info("üìö T√¨m ki·∫øm web: T·∫ÆT (ng∆∞·ªùi d√πng ch·ªçn)")
            return False
        
        # Auto mode: Ph√°t hi·ªán th√¥ng minh
        # Rule 1: Document lock - N·∫øu ƒëang xem t√†i li·ªáu c·ª• th·ªÉ ‚Üí t·∫Øt web
        if document_id:
            logger.info(f"üìÑ T√¨m ki·∫øm web: T·∫ÆT (kh√≥a t√†i li·ªáu cho {document_id})")
            return False
        
        # Rule 2: Query intent detection - Ki·ªÉm tra t·ª´ kh√≥a
        query_lower = query.lower()
        for pattern in self.DOC_SPECIFIC_KEYWORDS:
            if re.search(pattern, query_lower):
                logger.info(f"üìÑ T√¨m ki·∫øm web: T·∫ÆT (ph√°t hi·ªán t·ª´ kh√≥a ƒë·∫∑c th√π t√†i li·ªáu: '{pattern}')")
                return False
        
        # Default: B·∫≠t t√¨m ki·∫øm web
        logger.info("üåê T√¨m ki·∫øm web: B·∫¨T (m·∫∑c ƒë·ªãnh ch·∫ø ƒë·ªô t·ª± ƒë·ªông)")
        return True
    
    def retrieve(
        self, 
        query: str, 
        user_id: str, 
        document_id: Optional[str] = None,
        web_search_mode: WebSearchMode = "auto",  # ‚≠ê NEW: Smart mode control
        top_k: int = 5,
        web_max_results: int = 3,
        internal_max_results: int = 5
    ) -> HybridRetrievalResult:
        """
        HYBRID RETRIEVAL v·ªõi Ki·ªÉm So√°t T√¨m Ki·∫øm Web Th√¥ng Minh.
        
        Args:
            query: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
            user_id: ID ng∆∞·ªùi d√πng (cho t√¨m ki·∫øm n·ªôi b·ªô)
            document_id: N·∫øu c√≥, CH·ªà t√¨m trong t√†i li·ªáu n√†y (kh√≥a t√†i li·ªáu)
            web_search_mode: ‚≠ê M·ªöI - "auto" (th√¥ng minh), "force-on", "force-off"
            top_k: T·ªïng s·ªë k·∫øt qu·∫£ mong mu·ªën (sau khi g·ªôp)
            web_max_results: S·ªë k·∫øt qu·∫£ web t·ªëi ƒëa
            internal_max_results: S·ªë k·∫øt qu·∫£ n·ªôi b·ªô t·ªëi ƒëa
            
        Returns:
            HybridRetrievalResult ch·ª©a sources v√† metadata
        """
        # Resolve web search based on mode
        enable_web = self._should_enable_web_search(web_search_mode, query, document_id)
        
        all_chunks: List[RetrievedChunk] = []
        metadata = {
            "internal_results": 0,
            "web_results": 0,
            "total_results": 0,
            "web_search_mode": web_search_mode,
            "web_enabled": enable_web,
        }

        # --- H√ÄM H·ªñ TR·ª¢ cho th·ª±c thi song song ---
        def _retrieve_internal():
            """H√†m t√¨m ki·∫øm n·ªôi b·ªô"""
            try:
                if document_id:
                    from .retriever import retrieve_similar_chunks_by_document
                    chunks = retrieve_similar_chunks_by_document(
                        query=query,
                        document_id=document_id,
                        top_k=internal_max_results
                    )
                    logger.info(f"üìÑ T√¨m ki·∫øm trong t√†i li·ªáu c·ª• th·ªÉ: t√¨m th·∫•y {len(chunks)} ƒëo·∫°n t·ª´ t√†i li·ªáu {document_id}")
                else:
                    chunks = retrieve_similar_chunks_by_user(
                        query=query,
                        user_id=user_id,
                        top_k=internal_max_results
                    )
                    logger.info(f"üìö T√¨m ki·∫øm n·ªôi b·ªô: t√¨m th·∫•y {len(chunks)} ƒëo·∫°n")
                return chunks
            except Exception as e:
                logger.error(f"‚ùå T√¨m ki·∫øm n·ªôi b·ªô th·∫•t b·∫°i: {e}")
                return []

        def _retrieve_web():
            """H√†m t√¨m ki·∫øm web"""
            if not enable_web or not self.tavily_retriever:
                return []
            
            try:
                self.tavily_retriever.k = web_max_results
                web_docs = self.tavily_retriever.invoke(query)
                
                web_chunks = []
                for i, doc in enumerate(web_docs):
                    sim_score = 0.45 - (i * 0.05)  # Lower than internal to prioritize docs
                    
                    source_url = doc.metadata.get('source', 'Unknown URL')
                    title = doc.metadata.get('title', 'Web Result')
                    
                    chunk = RetrievedChunk(
                        content=doc.page_content,
                        chunk_index=i,
                        page_number=None,
                        similarity=sim_score,
                        metadata={
                            'source': 'web',
                            'url': source_url,
                            'title': title
                        }
                    )
                    web_chunks.append(chunk)
                
                logger.info(f"üåê T√¨m ki·∫øm web: t√¨m th·∫•y {len(web_chunks)} k·∫øt qu·∫£")
                return web_chunks
            except Exception as e:
                logger.error(f"‚ùå T√¨m ki·∫øm web th·∫•t b·∫°i: {e}")
                return []

        # --- ‚ö° TH·ª∞C THI SONG SONG: N·ªôi b·ªô + Web ƒë·ªìng th·ªùi ---
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_internal = executor.submit(_retrieve_internal)
            future_web = executor.submit(_retrieve_web)
            
            internal_chunks = future_internal.result()
            web_chunks = future_web.result()
            
            all_chunks.extend(internal_chunks)
            all_chunks.extend(web_chunks)
            
            metadata["internal_results"] = len(internal_chunks)
            metadata["web_results"] = len(web_chunks)

        # 3. Re-ranking / Sorting (Simple merge based on similarity)
        # Internal chunks c√≥ cosine similarity th·ª±c (0-1).
        # Web chunks c√≥ gi·∫£ l·∫≠p similarity (0.85 xu·ªëng).
        # Sort l·∫°i to√†n b·ªô list
        all_chunks.sort(key=lambda x: x.similarity, reverse=True)
        
        # C·∫Øt top_k
        final_chunks = all_chunks[:top_k]
        metadata["total_results"] = len(final_chunks)

        return HybridRetrievalResult(
            sources=final_chunks,
            metadata=metadata
        )

# Singleton instance
hybrid_retriever = HybridRetriever()
