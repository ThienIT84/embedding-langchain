# RAG System - Document Embedding & Semantic Search

Há»‡ thá»‘ng RAG (Retrieval-Augmented Generation) vá»›i document embedding, vector search vÃ  LLM generation.

## âœ¨ TÃ­nh nÄƒng

- ğŸ“„ Process PDF/text documents â†’ chunking â†’ embedding â†’ vector storage
- ğŸ” Semantic search vá»›i cosine similarity
- ğŸ¤– LLM generation qua Ollama (local) hoáº·c Gemini (cloud)
- ğŸš€ FastAPI server vá»›i RAG endpoints
- ğŸ” User-scoped search (JWT authentication)
- ğŸŒ Multilingual support (Vietnamese, English, 50+ languages)

## ğŸš€ Quick Start

### 1. Install
```bash
pip install -r requirements.txt
```

### 2. Config
```bash
cp .env.example .env
# Sá»­a SUPABASE_URL, SUPABASE_SERVICE_KEY
```

### 3. Run
```bash
python scripts/start_rag_server.py
```

Server: http://localhost:8001  
Docs: http://localhost:8001/docs

## ğŸ“ Cáº¥u trÃºc

```
Embedding_langchain/
â”œâ”€â”€ api/                   # FastAPI application
â”œâ”€â”€ src/                   # Core modules
â”‚   â”œâ”€â”€ embedder.py       # Text â†’ vectors
â”‚   â”œâ”€â”€ retriever.py      # Semantic search
â”‚   â”œâ”€â”€ llm_client.py     # Ollama LLM
â”‚   â”œâ”€â”€ rag_service.py    # Main RAG workflow
â”‚   â””â”€â”€ pipeline.py       # Document processing
â”œâ”€â”€ scripts/              # CLI tools
â”œâ”€â”€ tests/                # Unit tests
â””â”€â”€ docs/                 # Documentation
```

## ğŸ”Œ API Endpoints

### Query RAG (Full)
```bash
POST /api/rag/query
{
  "query": "CÃ¢u há»i?",
  "user_id": "uuid",
  "top_k": 5
}

Response:
{
  "answer": "...",
  "sources": [...],
  "metadata": {...}
}
```

### Retrieve Only (Fast)
```bash
POST /api/rag/retrieve
{
  "query": "CÃ¢u há»i?",
  "user_id": "uuid",
  "top_k": 5
}

Response:
{
  "sources": [...]
}
```

## ğŸ› ï¸ Scripts

### Process document
```bash
python scripts/ingest_document.py --document-id <uuid>
```

### Run server
```bash
# Production
python scripts/start_rag_server.py

# Development
python scripts/start_rag_server.py --reload
```

## ğŸ“Š Performance

| Operation | Time |
|-----------|------|
| Retrieve chunks | ~0.5-2s |
| LLM (Ollama) | ~5-30s |
| LLM (Gemini) | ~2-8s |
| Total RAG | ~6-35s |

## ğŸ§ª Testing

```bash
pytest                      # Run tests
pytest --cov=src           # With coverage
```

## ğŸ“– Docs

- [Code Reading Guide](docs/CODE_READING_GUIDE.md)
- [RAG Server Guide](RAG_SERVER_GUIDE.md)
- [Phase A Explained](docs/PHASE_A_EXPLAINED.md)
- [Testing Guide](docs/TESTING_GUIDE.md)

## ğŸ”§ Config

Key environment variables:

```bash
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_KEY=xxx
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3
CHUNK_SIZE=900
CHUNK_OVERLAP=200
```

## ğŸ› Troubleshooting

**NumPy error:**
```bash
pip install "numpy<2.0.0" --force-reinstall
```

**Ollama not running:**
```bash
ollama serve
ollama pull llama3
```

## ğŸ“ Notes

- Embedding model auto-downloads (~500MB) láº§n Ä‘áº§u
- Ollama pháº£i cháº¡y local cho LLM generation
- Supabase cáº§n pgvector extension
