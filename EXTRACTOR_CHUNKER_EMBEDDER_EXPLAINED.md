# üî¨ GI·∫¢I TH√çCH 3 FILE TR·ªåNG T√ÇM: TEXT_EXTRACTOR, CHUNKER, EMBEDDER

---

# üìÑ FILE 1: `src/text_extractor.py` - ƒê·ªåC FILE PDF

## üìå M·ª•c ƒê√≠ch File

File n√†y ch·ªâ l√†m **m·ªôt vi·ªác duy nh·∫•t**: ƒê·ªçc file PDF, tr√≠ch text t·ª´ t·ª´ng trang, v√† g·ª≠i cho file ti·∫øp theo.

**Analogy:** Gi·ªëng nh∆∞ b·∫°n m·ªü m·ªôt cu·ªën s√°ch PDF, ƒë·ªçc t·ª´ng trang, vi·∫øt l·∫°i n·ªôi dung trang ƒë√≥ v√†o notebook.

---

## üîç PH·∫¶N 1: IMPORT & KH√ÅI NI·ªÜM

```python
from __future__ import annotations
from pathlib import Path
from typing import Iterable

from pypdf import PdfReader
```

| Import | T·ª´ ƒê√¢u | T√°c D·ª•ng |
|--------|--------|---------|
| `Path` | `pathlib` | L√†m vi·ªác v·ªõi ƒë∆∞·ªùng d·∫´n file |
| `Iterable` | `typing` | Type hint cho "chu·ªói d·ªØ li·ªáu c√≥ th·ªÉ l·∫∑p" |
| `PdfReader` | `pypdf` | Th∆∞ vi·ªán ƒë·ªçc file PDF |

**`PdfReader` l√† g√¨?**
- M·ªôt l·ªõp (class) t·ª´ th∆∞ vi·ªán `pypdf`
- D√πng ƒë·ªÉ m·ªü file PDF v√† ƒë·ªçc t·ª´ng trang
- Gi·ªëng nh∆∞ m·ªôt "b·ªô ƒë·ªçc PDF" chuy√™n nghi·ªáp

---

## üîç PH·∫¶N 2: L·ªöP `DocumentChunk`

```python
class DocumentChunk:
    """C·∫•u tr√∫c nh·∫π ch·ª©a ƒëo·∫°n vƒÉn b·∫£n v√† (tu·ª≥ ch·ªçn) s·ªë trang."""

    __slots__ = ("text", "page_number")

    def __init__(self, text: str, page_number: int | None = None) -> None:
        self.text = text
        self.page_number = page_number
```

### **C√∫ Ph√°p Gi·∫£i Th√≠ch:**

#### **`class DocumentChunk:`**
- **`class`**: T·ª´ kh√≥a ƒë·ªãnh nghƒ©a m·ªôt l·ªõp (class)
- **`DocumentChunk`**: T√™n l·ªõp
- L·ªõp n√†y l√† m·ªôt "m·∫´u" (template) ƒë·ªÉ t·∫°o object ch·ª©a d·ªØ li·ªáu

#### **`__slots__ = ("text", "page_number")`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `__slots__` | M·ªôt attribute ƒë·∫∑c bi·ªát c·ªßa Python |
| `= (...)` | Danh s√°ch c√°c thu·ªôc t√≠nh object n√†y c√≥ th·ªÉ ch·ª©a |

**`__slots__` l√† g√¨?**
- N√≥i v·ªõi Python: "Object n√†y ch·ªâ ch·ª©a 2 attribute: `text` v√† `page_number`"
- Kh√¥ng th·ªÉ th√™m attribute kh√°c
- **L·ª£i √≠ch:** Ti·∫øt ki·ªám memory (quan tr·ªçng khi c√≥ nhi·ªÅu objects)

**V√≠ d·ª•:**
```python
# V·ªõi __slots__, ch·ªâ ƒë∆∞·ª£c
chunk.text = "n·ªôi dung"
chunk.page_number = 1

# Nh∆∞ng kh√¥ng ƒë∆∞·ª£c
chunk.author = "John"  # ‚ùå Error: 'DocumentChunk' object has no attribute 'author'
```

#### **`def __init__(self, text: str, page_number: int | None = None) -> None:`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `def __init__(...)` | H√†m kh·ªüi t·∫°o (constructor) |
| `self` | Object hi·ªán t·∫°i |
| `text: str` | Tham s·ªë, ph·∫£i l√† chu·ªói |
| `page_number: int \| None = None` | Tham s·ªë t√πy ch·ªçn (m·∫∑c ƒë·ªãnh `None`) |
| `-> None` | Kh√¥ng return g√¨ |

**`__init__` l√† g√¨?**
- H√†m ƒë·∫∑c bi·ªát ƒë∆∞·ª£c g·ªçi t·ª± ƒë·ªông khi t·∫°o object m·ªõi
- D√πng ƒë·ªÉ kh·ªüi t·∫°o c√°c thu·ªôc t√≠nh

**`page_number: int | None = None` l√† g√¨?**
- **`int | None`**: C√≥ th·ªÉ l√† s·ªë nguy√™n ho·∫∑c `None`
- **`= None`**: N·∫øu kh√¥ng truy·ªÅn v√†o, m·∫∑c ƒë·ªãnh l√† `None`

**V√≠ d·ª• s·ª≠ d·ª•ng:**
```python
# C√°ch 1: Truy·ªÅn c·∫£ 2 tham s·ªë
chunk1 = DocumentChunk(text="N·ªôi dung trang 1", page_number=1)

# C√°ch 2: Ch·ªâ truy·ªÅn text (page_number m·∫∑c ƒë·ªãnh None)
chunk2 = DocumentChunk(text="N·ªôi dung")

# Object ƒë∆∞·ª£c t·∫°o:
# chunk1.text = "N·ªôi dung trang 1"
# chunk1.page_number = 1
# chunk2.text = "N·ªôi dung"
# chunk2.page_number = None
```

#### **`self.text = text` v√† `self.page_number = page_number`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `self` | Object hi·ªán t·∫°i |
| `.text` | Attribute `text` c·ªßa object |
| `= text` | G√°n gi√° tr·ªã tham s·ªë `text` v√†o attribute |

**ƒêi·ªÅu n√†y l√†m g√¨?**
- L∆∞u tham s·ªë ƒë·∫ßu v√†o v√†o c√°c attribute c·ªßa object
- Sau n√†y c√≥ th·ªÉ truy c·∫≠p: `chunk.text`, `chunk.page_number`

---

## üîç PH·∫¶N 3: H√ÄM `extract_pdf_text()`

```python
def extract_pdf_text(file_path: Path) -> Iterable[DocumentChunk]:
    """ƒê·ªçc PDF v√† yield c√°c kh·ªëi n·ªôi dung theo t·ª´ng trang."""
    reader = PdfReader(str(file_path))
    for idx, page in enumerate(reader.pages, start=1):
        text = page.extract_text() or ""
        text = text.replace("\x00", "").strip()
        if not text:
            continue
        yield DocumentChunk(text=text, page_number=idx)
```

### **C√∫ Ph√°p Gi·∫£i Th√≠ch:**

#### **`def extract_pdf_text(file_path: Path) -> Iterable[DocumentChunk]:`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `file_path: Path` | Tham s·ªë: ƒë∆∞·ªùng d·∫´n file PDF |
| `-> Iterable[DocumentChunk]` | Tr·∫£ v·ªÅ m·ªôt chu·ªói `DocumentChunk` |

**`Iterable[DocumentChunk]` l√† g√¨?**
- M·ªôt "chu·ªói" c√°c `DocumentChunk` c√≥ th·ªÉ l·∫∑p qua
- Kh√¥ng ph·∫£i danh s√°ch c·ªë ƒë·ªãnh, m√† l√† "lu·ªìng d·ªØ li·ªáu" (stream)
- D√πng `yield` ƒë·ªÉ tr·∫£ v·ªÅ t·ª´ng ph·∫ßn t·ª≠ m·ªôt

#### **D√≤ng 1: `reader = PdfReader(str(file_path))`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `PdfReader(...)` | T·∫°o m·ªôt object ƒë·ªçc PDF |
| `str(file_path)` | Chuy·ªÉn `Path` object th√†nh string |
| `= reader` | L∆∞u v√†o bi·∫øn `reader` |

**T·∫°i sao `str(file_path)`?**
- `PdfReader` c√≥ th·ªÉ nh·∫≠n string ho·∫∑c Path
- Chuy·ªÉn th√†nh string ƒë·ªÉ ch·∫Øc ch·∫Øn t∆∞∆°ng th√≠ch
- `str(Path("/tmp/file.pdf"))` ‚Üí `"/tmp/file.pdf"`

**`reader` ƒë∆∞·ª£c t·∫°o l√† g√¨?**
- M·ªôt object ƒë√£ m·ªü file PDF
- B√¢y gi·ªù c√≥ th·ªÉ ƒë·ªçc t·ª´ng trang qua `reader.pages`

#### **D√≤ng 2: `for idx, page in enumerate(reader.pages, start=1):`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `for ... in` | L·∫∑p qua t·ª´ng ph·∫ßn t·ª≠ |
| `enumerate(reader.pages, start=1)` | L·∫∑p qua t·ª´ng trang, b·∫Øt ƒë·∫ßu t·ª´ 1 |
| `idx, page` | `idx` l√† s·ªë th·ª© t·ª± (1, 2, 3, ...), `page` l√† object trang |

**`enumerate(...)` l√† g√¨?**
- H√†m Python ƒë·ªÉ l·∫∑p v√† l·∫•y ch·ªâ s·ªë (index)
- **`start=1`**: B·∫Øt ƒë·∫ßu t·ª´ 1 (thay v√¨ 0)

**V√≠ d·ª•:**
```python
pages = [page1, page2, page3]
for idx, page in enumerate(pages, start=1):
    # L·∫ßn 1: idx=1, page=page1
    # L·∫ßn 2: idx=2, page=page2
    # L·∫ßn 3: idx=3, page=page3
```

#### **D√≤ng 3: `text = page.extract_text() or ""`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `page.extract_text()` | G·ªçi ph∆∞∆°ng th·ª©c ƒë·ªÉ tr√≠ch text t·ª´ trang |
| `or ""` | N·∫øu k·∫øt qu·∫£ `None`, d√πng `""` (chu·ªói r·ªóng) |

**`or ""` l√† g√¨?**
- **`or`**: To√°n t·ª≠ logic "ho·∫∑c"
- N·∫øu `page.extract_text()` tr·∫£ v·ªÅ `None` (kh√¥ng c√≥ text), d√πng `""`
- Tr√°nh l·ªói khi g·ªçi ph∆∞∆°ng th·ª©c tr√™n `None`

**V√≠ d·ª•:**
```python
text = None or ""          # text = ""
text = "Hello" or ""       # text = "Hello"
text = "" or ""            # text = ""
```

#### **D√≤ng 4: `text = text.replace("\x00", "").strip()`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `.replace("\x00", "")` | Xo√° k√Ω t·ª± null (`\x00`) t·ª´ text |
| `.strip()` | Xo√° kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi |

**`\x00` l√† k√Ω t·ª± g√¨?**
- L√† k√Ω t·ª± null (ASCII 0)
- ƒê√¥i khi PDF ch·ª©a k√Ω t·ª± n√†y, c·∫ßn xo√°
- `replace(old, new)`: Thay th·∫ø `old` b·∫±ng `new`

**V√≠ d·ª•:**
```python
text = "Hello\x00World"
text = text.replace("\x00", "")
# text = "HelloWorld"

text = "  Hello World  "
text = text.strip()
# text = "Hello World"
```

#### **D√≤ng 5-6: `if not text: continue`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `if not text:` | N·∫øu text r·ªóng |
| `continue` | B·ªè qua l·∫ßn l·∫∑p n√†y, sang trang ti·∫øp theo |

**T·∫°i sao?**
- N·∫øu trang PDF kh√¥ng c√≥ text (v√≠ d·ª• trang tr·∫Øng), b·ªè qua
- Tr√°nh l∆∞u c√°c chunk r·ªóng

#### **D√≤ng 7: `yield DocumentChunk(text=text, page_number=idx)`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `yield` | Tr·∫£ v·ªÅ m·ªôt ph·∫ßn t·ª≠ (nh∆∞ng kh√¥ng k·∫øt th√∫c h√†m) |
| `DocumentChunk(...)` | T·∫°o m·ªôt object DocumentChunk |
| `text=text, page_number=idx` | V·ªõi text ƒë√£ tr√≠ch, trang th·ª© `idx` |

**`yield` l√† g√¨?**
- Kh√°c `return`: `return` k·∫øt th√∫c h√†m, `yield` t·∫°m d·ª´ng
- L·∫ßn g·ªçi h√†m ti·∫øp theo, h√†m s·∫Ω ti·∫øp t·ª•c t·ª´ sau `yield`
- D√πng ƒë·ªÉ t·∫°o "generator" (lu·ªìng d·ªØ li·ªáu)

**V√≠ d·ª•:**
```python
def gen():
    print("1")
    yield 10
    print("2")
    yield 20

for x in gen():
    print(x)

# Output:
# 1
# 10
# 2
# 20
```

### **T√≥m T·∫Øt H√†m `extract_pdf_text()`**

```
INPUT: /tmp/document.pdf
  ‚Üì
M·ªü file PDF b·∫±ng PdfReader
  ‚Üì
L·∫∑p qua t·ª´ng trang (idx, page):
  1. Tr√≠ch text t·ª´ trang
  2. Xo√° k√Ω t·ª± null
  3. Xo√° kho·∫£ng tr·∫Øng
  4. N·∫øu text r·ªóng, b·ªè qua
  5. N·∫øu c√≥ text, yield DocumentChunk
  ‚Üì
OUTPUT: Chu·ªói DocumentChunk (m·ªói ph·∫ßn t·ª≠ l√† 1 trang)
```

---

---

# üìÑ FILE 2: `src/chunker.py` - CHIA TEXT TH√ÄNH CHUNKS

## üìå M·ª•c ƒê√≠ch File

File n√†y l·∫•y c√°c `DocumentChunk` l·ªõn (m·ªói trang PDF = 1 chunk l·ªõn), chia ch√∫ng th√†nh nh·ªØng `TextChunk` nh·ªè h∆°n.

**Analogy:** Gi·ªëng nh∆∞ b·∫°n c√≥ m·ªôt ch∆∞∆°ng s√°ch (trang), v√† b·∫°n chia n√≥ th√†nh c√°c ƒëo·∫°n nh·ªè (chunks) ƒë·ªÉ d·ªÖ hi·ªÉu h∆°n.

---

## üîç PH·∫¶N 1: IMPORT

```python
from typing import Iterable, List

from langchain_text_splitters import RecursiveCharacterTextSplitter

from .config import settings
from .text_extractor import DocumentChunk
```

| Import | T√°c D·ª•ng |
|--------|---------|
| `Iterable, List` | Type hints |
| `RecursiveCharacterTextSplitter` | C√¥ng c·ª• chia text c·ªßa LangChain |
| `settings` | C·∫•u h√¨nh (chunk_size, chunk_overlap) |
| `DocumentChunk` | L·ªõp t·ª´ file `text_extractor.py` |

---

## üîç PH·∫¶N 2: L·ªöP `TextChunk` (M·ªü R·ªông)

```python
class TextChunk(DocumentChunk):
    """M·ªü r·ªông DocumentChunk ƒë·ªÉ l∆∞u th√™m th·ª© t·ª± v√† ch·ªâ s·ªë chunk."""

    __slots__ = ("text", "page_number", "chunk_index")

    def __init__(self, text: str, page_number: int | None = None, chunk_index: int | None = None) -> None:
        super().__init__(text=text, page_number=page_number)
        self.chunk_index = chunk_index
```

### **C√∫ Ph√°p Gi·∫£i Th√≠ch:**

#### **`class TextChunk(DocumentChunk):`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `class TextChunk(...)` | ƒê·ªãnh nghƒ©a l·ªõp `TextChunk` |
| `(DocumentChunk)` | K·∫ø th·ª´a t·ª´ l·ªõp `DocumentChunk` |

**K·∫ø th·ª´a (Inheritance) l√† g√¨?**
- `TextChunk` "extends" `DocumentChunk`
- K·∫ø th·ª´a t·∫•t c·∫£ thu·ªôc t√≠nh, ph∆∞∆°ng th·ª©c t·ª´ l·ªõp cha
- C√≥ th·ªÉ th√™m attributes m·ªõi

**V√≠ d·ª•:**
```python
class Animal:
    def __init__(self, name):
        self.name = name

class Dog(Animal):  # Dog k·∫ø th·ª´a t·ª´ Animal
    def __init__(self, name, breed):
        super().__init__(name)  # G·ªçi __init__ c·ªßa Animal
        self.breed = breed

dog = Dog("Buddy", "Golden")
# dog.name = "Buddy"  (t·ª´ Animal)
# dog.breed = "Golden"  (t·ª´ Dog)
```

#### **`super().__init__(text=text, page_number=page_number)`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `super()` | Tham chi·∫øu ƒë·∫øn l·ªõp cha |
| `.`__init__`(...)` | G·ªçi h√†m `__init__` c·ªßa l·ªõp cha |

**`super()` l√† g√¨?**
- Cho ph√©p g·ªçi ph∆∞∆°ng th·ª©c c·ªßa l·ªõp cha t·ª´ l·ªõp con
- Tr√°nh ph·∫£i vi·∫øt l·∫°i code c·ªßa l·ªõp cha

#### **`self.chunk_index = chunk_index`**

- L∆∞u attribute m·ªõi `chunk_index` (kh√¥ng c√≥ ·ªü l·ªõp cha)
- ƒê√¢y l√† s·ªë th·ª© t·ª± c·ªßa chunk

**T√≥m T·∫Øt:**
```
DocumentChunk: text, page_number
       ‚Üë (k·∫ø th·ª´a)
TextChunk: text, page_number, chunk_index (th√™m m·ªõi)
```

---

## üîç PH·∫¶N 3: GLOBAL SPLITTER

```python
_splitter = RecursiveCharacterTextSplitter(
    chunk_size=settings.chunk_size,
    chunk_overlap=settings.chunk_overlap,
    separators=["\n\n", "\n", " ", ""],
)
```

### **C√∫ Ph√°p Gi·∫£i Th√≠ch:**

#### **`_splitter = RecursiveCharacterTextSplitter(...)`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `_splitter` | T√™n bi·∫øn global (d·∫•u `_` = bi·∫øn private) |
| `RecursiveCharacterTextSplitter(...)` | C√¥ng c·ª• chia text t·ª´ LangChain |

**`RecursiveCharacterTextSplitter` l√† g√¨?**
- L·ªõp t·ª´ th∆∞ vi·ªán LangChain
- D√πng ƒë·ªÉ chia text d√†i th√†nh chunks nh·ªè
- "Recursive" = chia l·∫ßn l∆∞·ª£t theo c√°c separator

#### **`chunk_size=settings.chunk_size`**

- K√≠ch th∆∞·ªõc m·ªói chunk (bao nhi√™u k√Ω t·ª±)
- VD: 900 k√Ω t·ª±

#### **`chunk_overlap=settings.chunk_overlap`**

- ƒê·ªô ch·ªìng l·∫•p gi·ªØa chunks li√™n ti·∫øp
- VD: 200 k√Ω t·ª±
- **T·∫°i sao?** ƒê·ªÉ kh√¥ng m·∫•t th√¥ng tin ·ªü bi√™n gi·ªØa chunks

**V√≠ d·ª•:**
```
Text: "ABCDEFGHIJ..." (1000 k√Ω t·ª±)
chunk_size=3, chunk_overlap=1

Chunk 1: "ABC"
Chunk 2: "BCD"  (ch·ªìng 1 k√Ω t·ª± "B", "C")
Chunk 3: "DEF"
...
```

#### **`separators=["\n\n", "\n", " ", ""]`**

- Danh s√°ch k√Ω t·ª±/chu·ªói d√πng ƒë·ªÉ chia
- Th·ª© t·ª±: n·∫øu kh√¥ng chia ƒë∆∞·ª£c b·∫±ng `"\n\n"`, th·ª≠ `"\n"`, r·ªìi `" "`, cu·ªëi c√πng `""`

**√ù Nghƒ©a:**
1. **`"\n\n"`**: Chia theo ƒëo·∫°n (double newline) - ∆∞u ti√™n nh·∫•t
2. **`"\n"`**: Chia theo d√≤ng
3. **`" "`**: Chia theo kho·∫£ng tr·∫Øng
4. **`""`**: Chia theo t·ª´ng k√Ω t·ª± (cu·ªëi c√πng)

**V√≠ d·ª•:**
```
Text: "ƒêo·∫°n 1\n\nƒêo·∫°n 2\n\nƒêo·∫°n 3"

Chia b·∫±ng "\n\n":
- Chunk 1: "ƒêo·∫°n 1"
- Chunk 2: "ƒêo·∫°n 2"
- Chunk 3: "ƒêo·∫°n 3"
```

### **T·∫°i sao `_splitter` l√† bi·∫øn global?**

- D√πng chung cho t·∫•t c·∫£ h√†m trong file
- T·∫°o m·ªôt l·∫ßn (kh√¥ng l√£ng ph√≠ memory)
- T·∫•t c·∫£ l·ªánh g·ªçi `_splitter.split_text()` d√πng c√πng object

---

## üîç PH·∫¶N 4: H√ÄM `split_chunks()`

```python
def split_chunks(chunks: Iterable[DocumentChunk]) -> List[TextChunk]:
    """T√°ch l·∫ßn l∆∞·ª£t t·ª´ng DocumentChunk th√†nh c√°c TextChunk nh·ªè h∆°n."""
    output: List[TextChunk] = []
    for source_idx, chunk in enumerate(chunks):
        pieces = _splitter.split_text(chunk.text)
        for piece_idx, piece in enumerate(pieces):
            text = piece.strip()
            if not text:
                continue
            output.append(
                TextChunk(
                    text=text,
                    page_number=chunk.page_number,
                    chunk_index=len(output) + 1,
                )
            )
    return output
```

### **C√∫ Ph√°p Gi·∫£i Th√≠ch:**

#### **`def split_chunks(chunks: Iterable[DocumentChunk]) -> List[TextChunk]:`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `chunks: Iterable[DocumentChunk]` | Input: chu·ªói DocumentChunk (t·ª´ `extract_pdf_text`) |
| `-> List[TextChunk]` | Output: danh s√°ch TextChunk |

#### **D√≤ng 1: `output: List[TextChunk] = []`**

- T·∫°o danh s√°ch r·ªóng ƒë·ªÉ l∆∞u k·∫øt qu·∫£

#### **D√≤ng 2: `for source_idx, chunk in enumerate(chunks):`**

- L·∫∑p qua t·ª´ng `DocumentChunk` trong input
- `source_idx`: ch·ªâ s·ªë (0, 1, 2, ...)
- `chunk`: object DocumentChunk

#### **D√≤ng 3: `pieces = _splitter.split_text(chunk.text)`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `_splitter.split_text(...)` | G·ªçi c√¥ng c·ª• chia text |
| `chunk.text` | L·∫•y text t·ª´ chunk |
| `= pieces` | L∆∞u danh s√°ch pieces (ƒëo·∫°n nh·ªè) |

**H√†m n√†y tr·∫£ v·ªÅ g√¨?**
- M·ªôt danh s√°ch string (m·ªói ph·∫ßn t·ª≠ l√† m·ªôt ƒëo·∫°n nh·ªè)
- VD: `["ƒêo·∫°n 1", "ƒêo·∫°n 2", "ƒêo·∫°n 3"]`

#### **D√≤ng 4-5: `for piece_idx, piece in enumerate(pieces):`**

- L·∫∑p qua t·ª´ng `piece` (ƒëo·∫°n nh·ªè)

#### **D√≤ng 6-7: `text = piece.strip()` v√† `if not text: continue`**

- Xo√° kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi
- B·ªè qua n·∫øu r·ªóng

#### **D√≤ng 8-14: `output.append(TextChunk(...))`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `output.append(...)` | Th√™m ph·∫ßn t·ª≠ v√†o danh s√°ch |
| `TextChunk(...)` | T·∫°o object TextChunk |
| `text=text` | N·ªôi dung text |
| `page_number=chunk.page_number` | Trang t·ª´ chunk g·ªëc |
| `chunk_index=len(output) + 1` | S·ªë th·ª© t·ª± (1, 2, 3, ...) |

**`len(output) + 1` l√† g√¨?**
- `len(output)`: S·ªë ph·∫ßn t·ª≠ trong `output` hi·ªán t·∫°i
- `+ 1`: Th√™m 1 ƒë·ªÉ b·∫Øt ƒë·∫ßu t·ª´ 1 (thay v√¨ 0)

#### **D√≤ng 15: `return output`**

- Tr·∫£ v·ªÅ danh s√°ch `TextChunk` ƒë·∫ßy ƒë·ªß

### **V√≠ D·ª• C·ª• Th·ªÉ:**

**Input:**
```python
chunks = [
    DocumentChunk(text="Trang 1: ABC DEF GHI", page_number=1),
    DocumentChunk(text="Trang 2: JKL MNO", page_number=2),
]
# chunk_size=5, chunk_overlap=1, separator=[" ", ""]
```

**X·ª≠ l√Ω:**
```
Chunk 1 (Trang 1):
  Text: "Trang 1: ABC DEF GHI"
  Split by spaces: ["Trang", "1:", "ABC", "DEF", "GHI"]
  Combine to chunk_size=5: 
    - Piece 1: "Trang 1: ABC"  (text="Trang 1: ABC", page=1, index=1)
    - Piece 2: "ABC DEF"       (text="ABC DEF", page=1, index=2)
    - Piece 3: "DEF GHI"       (text="DEF GHI", page=1, index=3)

Chunk 2 (Trang 2):
  ...
```

**Output:**
```python
[
    TextChunk(text="Trang 1: ABC", page_number=1, chunk_index=1),
    TextChunk(text="ABC DEF", page_number=1, chunk_index=2),
    TextChunk(text="DEF GHI", page_number=1, chunk_index=3),
    TextChunk(text="Trang 2: JKL", page_number=2, chunk_index=4),
    ...
]
```

### **T√≥m T·∫Øt H√†m `split_chunks()`**

```
INPUT: Chu·ªói DocumentChunk
  ‚Üì
L·∫∑p qua t·ª´ng chunk:
  1. L·∫•y text t·ª´ chunk
  2. Chia th√†nh pieces nh·ªè
  3. L·∫∑p qua t·ª´ng piece:
     - Xo√° kho·∫£ng tr·∫Øng
     - N·∫øu kh√¥ng r·ªóng, t·∫°o TextChunk m·ªõi
  ‚Üì
OUTPUT: Danh s√°ch TextChunk
```

---

---

# üìÑ FILE 3: `src/embedder.py` - SINH EMBEDDING VECTOR

## üìå M·ª•c ƒê√≠ch File

File n√†y l·∫•y c√°c `TextChunk` (ƒëo·∫°n text nh·ªè), sinh embedding vector (m·∫£ng s·ªë ƒë·∫°i di·ªán cho √Ω nghƒ©a) cho m·ªói chunk.

**Analogy:** Gi·ªëng nh∆∞ b·∫°n c√≥ m·ªôt cu·ªën s√°ch, v√† b·∫°n t·∫°o m·ªôt "b·∫£n ƒë·ªì t∆∞ duy" (embedding vector) cho m·ªói ƒëo·∫°n ƒë·ªÉ m√°y t√≠nh hi·ªÉu √Ω nghƒ©a.

---

## üîç PH·∫¶N 1: IMPORT

```python
import numpy as np
from sentence_transformers import SentenceTransformer

from .config import settings
from .chunker import TextChunk
```

| Import | T√°c D·ª•ng |
|--------|---------|
| `numpy as np` | Th∆∞ vi·ªán l√†m vi·ªác v·ªõi m·∫£ng s·ªë |
| `SentenceTransformer` | Model AI ƒë·ªÉ sinh embedding |
| `settings` | C·∫•u h√¨nh (t√™n model) |
| `TextChunk` | L·ªõp t·ª´ file `chunker.py` |

**`numpy` l√† g√¨?**
- Th∆∞ vi·ªán Python m·∫°nh m·∫Ω cho t√≠nh to√°n khoa h·ªçc
- L√†m vi·ªác v·ªõi m·∫£ng (arrays) v√† ma tr·∫≠n

**`SentenceTransformer` l√† g√¨?**
- M·ªôt model AI ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn
- Chuy√™n d√πng ƒë·ªÉ sinh embedding (vector ƒë·∫°i di·ªán √Ω nghƒ©a) cho text
- L√† part c·ªßa th∆∞ vi·ªán `sentence-transformers`

---

## üîç PH·∫¶N 2: L·ªöP `EmbeddingResult`

```python
class EmbeddingResult:
    """ƒê√≥ng g√≥i TextChunk c√πng vector embedding t∆∞∆°ng ·ª©ng."""
    __slots__ = ("chunk", "vector")

    def __init__(self, chunk: TextChunk, vector: np.ndarray) -> None:
        self.chunk = chunk
        self.vector = vector
```

### **C√∫ Ph√°p Gi·∫£i Th√≠ch:**

#### **`__slots__ = ("chunk", "vector")`**

- Object n√†y ch·ªâ ch·ª©a 2 attribute:
  - `chunk`: M·ªôt object `TextChunk` (ƒëo·∫°n text)
  - `vector`: M·ªôt numpy array (embedding vector)

#### **`vector: np.ndarray`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `np.ndarray` | Type hint: numpy array (m·∫£ng) |

**`np.ndarray` l√† g√¨?**
- `np`: L√† alias c·ªßa `numpy`
- `ndarray`: "n-dimensional array" (m·∫£ng nhi·ªÅu chi·ªÅu)
- ·ªû ƒë√¢y l√† 1D array (m·∫£ng 1 chi·ªÅu) ch·ª©a s·ªë th·∫≠p ph√¢n

**V√≠ d·ª•:**
```python
import numpy as np

vector = np.array([0.1, 0.2, 0.3, 0.4, ...])  # 768 s·ªë
print(type(vector))  # <class 'numpy.ndarray'>
```

### **T√≥m T·∫Øt L·ªõp `EmbeddingResult`**

```
EmbeddingResult ch·ª©a:
  - chunk: TextChunk (text + metadata)
  - vector: np.ndarray (embedding vector)
```

---

## üîç PH·∫¶N 3: GLOBAL MODEL

```python
_model: SentenceTransformer | None = None


def _get_model() -> SentenceTransformer:
    """Kh·ªüi t·∫°o (ho·∫∑c t√°i s·ª≠ d·ª•ng) model SentenceTransformer d√πng chung."""
    global _model
    if _model is None:
        _model = SentenceTransformer(settings.hf_model_name)
    return _model
```

### **C√∫ Ph√°p Gi·∫£i Th√≠ch:**

#### **`_model: SentenceTransformer | None = None`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `_model` | Bi·∫øn global (d·∫•u `_` = private) |
| `: SentenceTransformer \| None` | C√≥ th·ªÉ l√† model ho·∫∑c `None` |
| `= None` | Kh·ªüi t·∫°o th√†nh `None` (ch∆∞a load model) |

#### **`global _model`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `global` | T·ª´ kh√≥a: cho ph√©p s·ª≠a bi·∫øn global |
| `_model` | T√™n bi·∫øn global c·∫ßn s·ª≠a |

**`global` l√† g√¨?**
- M·∫∑c ƒë·ªãnh, b√™n trong h√†m kh√¥ng th·ªÉ s·ª≠a bi·∫øn global
- `global` cho ph√©p s·ª≠a

**V√≠ d·ª•:**
```python
count = 0

def increment():
    global count
    count += 1  # S·ª≠a bi·∫øn global

increment()
print(count)  # 1
```

#### **`if _model is None: _model = SentenceTransformer(...)`**

- N·∫øu model ch∆∞a ƒë∆∞·ª£c load (`None`), load n√≥
- **Lazy loading:** Ch·ªâ load khi c·∫ßn (ti·∫øt ki·ªám memory)

#### **`settings.hf_model_name`**

- T√™n model t·ª´ c·∫•u h√¨nh
- VD: `"sentence-transformers/paraphrase-multilingual-mpnet-base-v2"`

**T·∫°i sao `global _model`?**
- L·∫ßn ƒë·∫ßu g·ªçi h√†m: load model (m·∫•t th·ªùi gian)
- L·∫ßn ti·∫øp theo: t√°i s·ª≠ d·ª•ng model c≈© (nhanh h∆°n)
- Ch·ªâ load 1 l·∫ßn duy nh·∫•t

### **T√≥m T·∫Øt H√†m `_get_model()`**

```
L·∫ßn 1 g·ªçi _get_model():
  - _model = None
  - Load model t·ª´ HuggingFace
  - L∆∞u v√†o _model
  - Return model

L·∫ßn 2+ g·ªçi _get_model():
  - _model ƒë√£ ƒë∆∞·ª£c load
  - Ch·ªâ return _model (kh√¥ng load l·∫°i)
```

---

## üîç PH·∫¶N 4: H√ÄM `embed_chunks()`

```python
def embed_chunks(chunks: Iterable[TextChunk]) -> List[EmbeddingResult]:
    """Sinh embedding cho danh s√°ch TextChunk v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng list."""
    chunk_list = list(chunks)
    if not chunk_list:
        return []
    model = _get_model()
    embeddings = model.encode([chunk.text for chunk in chunk_list], show_progress_bar=True)
    return [EmbeddingResult(chunk=chunk, vector=np.array(vector, dtype=np.float32)) for chunk, vector in zip(chunk_list, embeddings)]
```

### **C√∫ Ph√°p Gi·∫£i Th√≠ch:**

#### **D√≤ng 1: `chunk_list = list(chunks)`**

- Chuy·ªÉn t·ª´ `Iterable` th√†nh `List`
- **T·∫°i sao?** ƒê·ªÉ d√πng ƒë∆∞·ª£c ch·ªâ s·ªë (index) sau n√†y

**V√≠ d·ª•:**
```python
# Iterable: kh√¥ng th·ªÉ truy c·∫≠p theo index
iterable = (x for x in range(10))
# iterable[0]  # ‚ùå Error

# List: c√≥ th·ªÉ truy c·∫≠p theo index
chunk_list = list(iterable)
# chunk_list[0]  # ‚úÖ OK
```

#### **D√≤ng 2-3: `if not chunk_list: return []`**

- N·∫øu danh s√°ch r·ªóng (kh√¥ng c√≥ chunks), return danh s√°ch r·ªóng
- Tr√°nh l·ªói khi encoding

#### **D√≤ng 4: `model = _get_model()`**

- L·∫•y model SentenceTransformer
- L·∫ßn ƒë·∫ßu: load t·ª´ HuggingFace
- L·∫ßn ti·∫øp theo: t√°i s·ª≠ d·ª•ng

#### **D√≤ng 5: `embeddings = model.encode([chunk.text for chunk in chunk_list], show_progress_bar=True)`**

**Chia nh·ªè:**

##### **`[chunk.text for chunk in chunk_list]`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `[... for ... in ...]` | List comprehension (v√≤ng l·∫∑p trong list) |
| `chunk.text` | L·∫•y text t·ª´ m·ªói chunk |
| `for chunk in chunk_list` | L·∫∑p qua t·∫•t c·∫£ chunks |

**V√≠ d·ª•:**
```python
chunks = [
    TextChunk(text="N·ªôi dung 1", ...),
    TextChunk(text="N·ªôi dung 2", ...),
    TextChunk(text="N·ªôi dung 3", ...),
]

texts = [chunk.text for chunk in chunks]
# K·∫øt qu·∫£: ["N·ªôi dung 1", "N·ªôi dung 2", "N·ªôi dung 3"]
```

##### **`model.encode(..., show_progress_bar=True)`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `model.encode(...)` | G·ªçi ph∆∞∆°ng th·ª©c sinh embedding |
| `show_progress_bar=True` | Hi·ªÉn th·ªã thanh ti·∫øn ƒë·ªô (progress bar) |

**H√†m n√†y tr·∫£ v·ªÅ g√¨?**
- M·ªôt numpy array 2D (ma tr·∫≠n)
- M·ªói h√†ng l√† embedding vector (768 s·ªë) cho m·ªôt chunk

**V√≠ d·ª•:**
```python
texts = ["Hello world", "Goodbye world"]

embeddings = model.encode(texts)
# embeddings.shape = (2, 768)
# embeddings[0] = [0.1, 0.2, 0.3, ...]  (768 s·ªë cho "Hello world")
# embeddings[1] = [0.4, 0.5, 0.6, ...]  (768 s·ªë cho "Goodbye world")
```

#### **D√≤ng 6: `return [... for chunk, vector in zip(chunk_list, embeddings)]`**

**Chia nh·ªè:**

##### **`zip(chunk_list, embeddings)`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `zip(list1, list2)` | Gh√©p 2 danh s√°ch l·∫°i (t·ª´ng c·∫∑p) |

**V√≠ d·ª•:**
```python
chunks = [chunk1, chunk2, chunk3]
vectors = [vec1, vec2, vec3]

for chunk, vector in zip(chunks, vectors):
    # L·∫ßn 1: chunk=chunk1, vector=vec1
    # L·∫ßn 2: chunk=chunk2, vector=vec2
    # L·∫ßn 3: chunk=chunk3, vector=vec3
```

##### **`EmbeddingResult(chunk=chunk, vector=np.array(vector, dtype=np.float32))`**

| Ph·∫ßn | √ù Nghƒ©a |
|-----|--------|
| `np.array(vector, ...)` | Chuy·ªÉn vector th√†nh numpy array |
| `dtype=np.float32` | Ki·ªÉu d·ªØ li·ªáu: s·ªë th·∫≠p ph√¢n 32-bit |

**`dtype=np.float32` l√† g√¨?**
- `dtype`: "data type"
- `float32`: S·ªë th·∫≠p ph√¢n 32-bit (ti·∫øt ki·ªám memory h∆°n `float64`)

### **V√≠ D·ª• C·ª• Th·ªÉ:**

**Input:**
```python
chunks = [
    TextChunk(text="N·ªôi dung 1", page_number=1, chunk_index=1),
    TextChunk(text="N·ªôi dung 2", page_number=1, chunk_index=2),
]
```

**X·ª≠ l√Ω:**
```
1. chunk_list = list(chunks)
   ‚Üí [TextChunk1, TextChunk2]

2. texts = ["N·ªôi dung 1", "N·ªôi dung 2"]

3. embeddings = model.encode(texts)
   ‚Üí [
       [0.1, 0.2, 0.3, ..., 0.768],  (768 s·ªë cho chunk 1)
       [0.4, 0.5, 0.6, ..., 0.768],  (768 s·ªë cho chunk 2)
     ]

4. zip(chunks, embeddings)
   ‚Üí [(chunk1, vec1), (chunk2, vec2)]

5. T·∫°o EmbeddingResult cho m·ªói c·∫∑p
```

**Output:**
```python
[
    EmbeddingResult(
        chunk=TextChunk(text="N·ªôi dung 1", page_number=1, chunk_index=1),
        vector=np.array([0.1, 0.2, 0.3, ..., 0.768])
    ),
    EmbeddingResult(
        chunk=TextChunk(text="N·ªôi dung 2", page_number=1, chunk_index=2),
        vector=np.array([0.4, 0.5, 0.6, ..., 0.768])
    ),
]
```

### **T√≥m T·∫Øt H√†m `embed_chunks()`**

```
INPUT: Chu·ªói TextChunk
  ‚Üì
1. Chuy·ªÉn th√†nh List
2. Ki·ªÉm tra kh√¥ng r·ªóng
3. Load model
4. Tr√≠ch t·∫•t c·∫£ texts: ["text1", "text2", ...]
5. Sinh embedding b·∫±ng model.encode()
6. Gh√©p chunks + vectors b·∫±ng zip()
7. T·∫°o EmbeddingResult cho m·ªói c·∫∑p
  ‚Üì
OUTPUT: Danh s√°ch EmbeddingResult
```

---

## üìä B·∫¢NG T√ìML·ªñI 3 FILE

| File | M·ª•c ƒê√≠ch | Input | Output |
|-----|---------|-------|--------|
| **`text_extractor.py`** | ƒê·ªçc PDF | File path | Chu·ªói DocumentChunk (m·ªói trang) |
| **`chunker.py`** | Chia chunks | Chu·ªói DocumentChunk | Danh s√°ch TextChunk (nh·ªè h∆°n) |
| **`embedder.py`** | Sinh embedding | Chu·ªói TextChunk | Danh s√°ch EmbeddingResult (text + vector) |

---

## üí° C√ÅC C√ö PH√ÅP PYTHON C·∫¶N BI·∫æT

| C√∫ Ph√°p | √ù Nghƒ©a | V√≠ D·ª• |
|--------|--------|-------|
| `class X(Y):` | K·∫ø th·ª´a | `class TextChunk(DocumentChunk):` |
| `super().__init__()` | G·ªçi __init__ l·ªõp cha | `super().__init__(text=text)` |
| `yield` | Tr·∫£ v·ªÅ t·ª´ng ph·∫ßn t·ª≠ (generator) | `yield DocumentChunk(...)` |
| `enumerate(iterable, start=N)` | L·∫∑p v·ªõi ch·ªâ s·ªë t·ª´ N | `for idx, item in enumerate(..., start=1):` |
| `str.replace(old, new)` | Thay th·∫ø chu·ªói | `text.replace("\x00", "")` |
| `str.strip()` | Xo√° kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi | `text.strip()` |
| `if not x:` | N·∫øu x l√† False/None/r·ªóng | `if not text: continue` |
| `or default` | Ho·∫∑c gi√° tr·ªã m·∫∑c ƒë·ªãnh | `text = None or ""` |
| `for x in y: ...` | V√≤ng l·∫∑p | `for chunk in chunks:` |
| `[expr for x in y]` | List comprehension | `[chunk.text for chunk in chunks]` |
| `global var` | Cho ph√©p s·ª≠a bi·∫øn global | `global _model` |
| `zip(list1, list2)` | Gh√©p 2 danh s√°ch | `for x, y in zip(a, b):` |
| `np.ndarray` | Numpy array | `vector: np.ndarray` |
| `np.array(list, dtype=...)` | T·∫°o numpy array | `np.array([0.1, 0.2], dtype=np.float32)` |

---

B·∫°n ƒë√£ hi·ªÉu r√µ 3 file n√†y ch∆∞a? C√≥ ph·∫ßn n√†o c·∫ßn gi·∫£i th√≠ch th√™m kh√¥ng?
